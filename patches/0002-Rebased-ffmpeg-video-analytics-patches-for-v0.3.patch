From f211968648f5dc49213ed69eef1e47f2b3c6fd7c Mon Sep 17 00:00:00 2001
From: Lin Xie <lin.xie@intel.com>
Date: Thu, 11 Jul 2019 10:38:12 +0800
Subject: [PATCH] Rebased ffmpeg video analytics patches for v0.3

Introduce image inference backend

Change-Id: I35a3fd0da2e81204ac823b7770ca98be5a562993

IE filters based on image inference backend.

* new filters are named "ie_detect" and "ie_classify"
  w/ functionality similar to previous ones
* add RGBP format into ffmpeg swscale including list
* rewrite some shell scripts as new/modified parameters

Change-Id: If4bffa7b36f523a366b8a465701a7259890cd902

Fix the inference configuration problem

Change-Id: I5e4568a3697f3f794ecb26876497098ebf565216

Enable classify post process and fix batch mode

* Porting all classify metadata converter, moved
  more data structure to image backend.
* Fixed batch mode not working issue:
  - batch size was not saved to context
  - data address offset got from blob was incorrect
  - request was not pushed to correct queue
* Updated the shell script

Change-Id: Id2c94218268fcbfce11a5990d9a85c245f39d7d8

Add YOLOv3 output handling

* Fixed a memory double free issue caused by error index
* Add script to test YOLOv3 network

Change-Id: Ib53c7be1054a241025c22887dfc67b6fa7b76d97

Move model proc Json parsing into inference backend

Change-Id: Ieacbc921223228f0de2b83b6cdae384a23fba624

Refine inference skipping

Change-Id: I3783b4e9d27c2b07ac43548000a2bd3d31a4156f

vaapi VPP pre proc for inference backend
enabled it in both detect and classify filters

Change-Id: I7133951f0f56c8e1cbf702c63c6569bf22734081

Add lock when get processed frame

Change-Id: I5c1a04943564caca24fdbe4457ac53026a7a2797

Add iss test scripts.

* Copy iss scripts to docker image for test

Change-Id: I2719e96f226ed418fec0e37f1fbccc64a227e02c

fix pre_proc_vaapi color space and surface allocate
to deliver what dst output needed for inference

Change-Id: Ic97eb7e0af3553e66181dbc665c2c82e0cdbde52

Clean pre proc related code

Change-Id: Ic51646ee44ab4582dfc9767bc68ca8c81a8960eb

using fast bilinear sws to speed up

Change-Id: I78340c2d0915415d343fce4c7d5d9bb63f61cb69

Add async preprocess in inference backend

* Support VAAPI Preproc firstly, will support swscale

Change-Id: I564232804ff895a14cfd9d7d4cd25155b752dfed

Add a mocker preproc to achieve SW async preproc

* Add a new option in filter "async_preproc"

Change-Id: I2666f6dc6236023550619299991c62ba91d55c4b

Fix classify filter EOS handling failure

Change-Id: I9c1e40c56da4edb291aba86a3f9134a5550ae7f4

Push request back to free queue if it is not used

Change-Id: Ibd294473b8ca517fda37acf916439a20c85b9f10

Fix incorrect age value

Change-Id: Ie0098bebd0c5ff86be4dc98590ca8061d797fcd6

speed up the sws vpp in inference pre proc to about 2 times
of before

Change-Id: I176ded6a99c5ab3bfa57eefd5fcfc3a07fb08b25

[Detect] Drain all possible input and output frames

Change-Id: Id02af0218f396f219a10bc38db9b2b6eb4f32caf

Add trace function for inference backend

Change-Id: I8ae5fd6873e965638e3e8575848e9f7b8a57b496

Fix the decode get buffer failed issue on HKH with va decoder,
which caused by classify holding too much frames without
release at once

Change-Id: I9bd2c954705a385a122cdc46081ade379331f3cf

[profiling] fixed init excluded fps issue and total frames
issue on some clips, remove the dec and enc profiling data
show temporary due to some statistic issue on multithread
and HW cases which users not concern by now.

Change-Id: Iaa0b5d148af9b49fc057fb3fbc6c72d46f9eb369

[Preproc] Refine the image memory allocation

* Use av_image_alloc to alloc proper memory size for temp images

Change-Id: I73a3c829842997ad22f8eb0f58c9eda993872c8b

[Preproc] Sync surface before mapping

* Expect to fix #FF-81

Change-Id: I0293ca7bf1817424a3e7e8400f2d82138d7b2055

Radical fix hw decode get buffer failed block issue by holding
the decoder when buffer is empty and continue the filter
schedule

Co-authored-by: Lin Xie <lin.xie@intel.com>

Change-Id: Ieec9d789d31c2ec9f26a2bbdff55fe2e66cc04ce

[IE filter] Release frames sent to backend at exit

* Some frames are still in inference backend processing
  queue when ask for stopping the application.
* Drain and release those frames at that time to assure
  no memory is lost.

Change-Id: Ia0f2321d0699a67378797b99796884cf78d150d6

Release preproc when close async preproc inference backend

* To fix mem leak issue FF-85

Change-Id: I6b6da29049896862b38bd7c82a9e77d3e64068dc

Support crop for YoloV3

Change-Id: Ie996dab1e2d2c6437bd6a277ef778c37546e8d5c
Signed-off-by: Yuan Hu <yuan1.hu@intel.com>
Signed-off-by: Lin Xie <lin.xie@intel.com>

Fix two mem leak issues

* Release json objesct when finish the json file reading (FF-83)
* Free labels in buffer

Change-Id: Ia6e56054c4422d0348fd3cf3ee0bc84e10185975

Implement a generic converter

* The output json file can read by python directly
* Support the scripts in samples/shell-new

Change-Id: I7c5ecc19a6cccbe9af543b77b4d5c9e4200771c5

Align Yolov3 post process with OpenVino R2 (FF-89)

Change-Id: If5470f8ef1e3145b1a7586fb441b40f99b82d119

Add a shell script example for converter filter

Change-Id: Idb2f748fd8b0e7f298710f688c40b392e74c8a27

[perf] load balance of the pipeline and infer filters

using by option "-load_balance <number>",the number
can be tuned, and it can be set to be a number
larger than the "nireq" of infer filter

1. will improve ~10% perf on CLX and HKH by some
   single density cases
2. improve the real time latency of end to end
3. help to avoid the decoder get too much buffer,
   to save memory and extra computation

Change-Id: Idff14274ae5381a2d94c6a539ae28ba827a1096e

[backend] Use core api to replace plugin api

Change-Id: I9b5de379659a4fa20b627355f6224bfa07304674

Check multi-device config to add cpu extension

Change-Id: Iee457815ad0a376c33059ae31e1189a1eae4f345

Update test shell scripts

* Add cpu extension if hetero devices contain cpu

Change-Id: I2a9ec14d97b0b44a88c3cd824fd4758078dc952b

Add "timestamp" item in json output structure

Change-Id: I76ae4c8e61234b96e7b58863b8607ae9b877a1b9

Fix cannot exit when working in batch mode

* Reproducing step:
  "HW decode" + "inference" + "batch_size > 1",
  then pressing 'q' during ffmpeg running

Change-Id: I1a9d86a4a6269ab0c44012d36fee1ebda78454a9

Add reshape API for batch mode

Change-Id: I7d8b8345d330249969cd210768ea4f4cf42aae27

Enable doing pre-process in IE

* see samples for actual use cases

Change-Id: Idcc78ce0e8f5e7da1923219ce2ea1164306408ba

Enable yolov3 in converter filter

Change-Id: I722995e2ed8ec405a4cd53f68908451ebbb6fdea

Update post process for yolov3

* Use anchor offset to control size of bounding box for objects
* Fix FF-89

Change-Id: If4232a84d98bb53a422e8616d3787f6680ccca44

Do code clean

* Delete inference_classify and inference_detect filters and clean related code.
* Use trace function in inference backend.

Change-Id: Ie430ecac11e544e11b4014602ffbd843f6da7ade

use verbose instead of warning to show not enough
surface, in order to avoid frequent
screen printing

Change-Id: I7418c40230d33e330e6ba1259eefbc1229edcc9e

Rename detect and classify filters

* Align filter names with ffmpeg analytics V0.1

Change-Id: I8506f53398e58cc2e24e0510df35767f481bf681

Update shell scripts

Change-Id: Ia67cf20aec64f7d7a49f186c91e16b854bd7eac8

Change license headers in backend

Change-Id: I1f5637e48cf41b9a26cafd6ee2d418dfdd6ee154

fix several klocwork reported issues.

Change-Id: Ibaf5c9fdaff36e5f133bba329eea8e6937e789ea

Add model proc check for convert filter

Change-Id: I5a443a3572138644464658f38898d616cfc022af

Klocwork fix

Change-Id: I1bac217dd2a63bc3bf595cb91194d623d2a1897d

Make string explicit null terminated after strncpy.

Change-Id: Ia92ca7f43e4a0adacec301553ef1ac524ddd8430

Profile fix for initial time

Change-Id: Ie9e01ee23dd1597c2d884caecb57e8f7823cf2a3

Fixed wrong frames and fps printing with some streams

Change-Id: I4397d04c4fed018ce9b5590032c649b80f26cda0
---
 configure                                          |   29 +-
 fftools/ffmpeg.c                                   |   47 +-
 fftools/ffmpeg.h                                   |    1 +
 fftools/ffmpeg_opt.c                               |    7 +
 libavfilter/Makefile                               |   23 +-
 libavfilter/allfilters.c                           |    5 +-
 libavfilter/avfilter.c                             |   12 +-
 libavfilter/avfilter.h                             |   12 +
 libavfilter/avfiltergraph.c                        |   37 +
 libavfilter/dnn_backend_intel_ie.c                 |  419 --------
 libavfilter/dnn_backend_intel_ie.h                 |   40 -
 libavfilter/dnn_interface.c                        |   11 -
 libavfilter/dnn_interface.h                        |    2 +-
 libavfilter/inference.c                            | 1042 --------------------
 libavfilter/inference.h                            |  266 -----
 libavfilter/inference_backend/.clang-format        |   10 +
 libavfilter/inference_backend/ff_base_inference.c  |  130 +++
 libavfilter/inference_backend/ff_base_inference.h  |  238 +++++
 libavfilter/inference_backend/ff_inference_impl.c  |  559 +++++++++++
 libavfilter/inference_backend/ff_inference_impl.h  |   39 +
 libavfilter/inference_backend/ff_list.c            |   93 ++
 libavfilter/inference_backend/ff_list.h            |   55 ++
 libavfilter/inference_backend/ff_proc_factory.c    |  686 +++++++++++++
 libavfilter/inference_backend/ff_proc_factory.h    |   27 +
 libavfilter/inference_backend/image.c              |   92 ++
 libavfilter/inference_backend/image.h              |   90 ++
 libavfilter/inference_backend/image_inference.c    |  199 ++++
 libavfilter/inference_backend/image_inference.h    |  163 +++
 .../image_inference_async_preproc.c                |  245 +++++
 .../image_inference_async_preproc.h                |   46 +
 libavfilter/inference_backend/logger.c             |   57 ++
 libavfilter/inference_backend/logger.h             |   88 ++
 libavfilter/inference_backend/metaconverter.c      |  209 ++++
 libavfilter/inference_backend/metaconverter.h      |   56 ++
 libavfilter/inference_backend/model_proc.c         |  275 ++++++
 libavfilter/inference_backend/model_proc.h         |   39 +
 .../inference_backend/openvino_image_inference.c   |  644 ++++++++++++
 .../inference_backend/openvino_image_inference.h   |   71 ++
 libavfilter/inference_backend/pre_proc.c           |  137 +++
 libavfilter/inference_backend/pre_proc.h           |   72 ++
 libavfilter/inference_backend/pre_proc_mocker.c    |   61 ++
 libavfilter/inference_backend/pre_proc_swscale.c   |  252 +++++
 libavfilter/inference_backend/pre_proc_vaapi.c     |  267 +++++
 libavfilter/inference_backend/queue.c              |  171 ++++
 libavfilter/inference_backend/safe_queue.c         |  167 ++++
 libavfilter/inference_backend/safe_queue.h         |   45 +
 libavfilter/vf_inference_classify.c                |  806 +++++----------
 libavfilter/vf_inference_detect.c                  |  561 ++++-------
 libavfilter/vf_inference_identify.c                |   82 +-
 libavfilter/vf_inference_metaconvert.c             |  141 ++-
 libavfilter/vf_ocv_overlay.c                       |  215 ++++
 libavformat/iemetadataenc.c                        |    2 +-
 libavutil/buffer.c                                 |   11 +
 libavutil/buffer.h                                 |    6 +
 libavutil/log.c                                    |   11 +
 libavutil/log.h                                    |    3 +
 libswscale/utils.c                                 |    1 +
 57 files changed, 6230 insertions(+), 2845 deletions(-)
 delete mode 100644 libavfilter/dnn_backend_intel_ie.c
 delete mode 100644 libavfilter/dnn_backend_intel_ie.h
 delete mode 100644 libavfilter/inference.c
 delete mode 100644 libavfilter/inference.h
 create mode 100644 libavfilter/inference_backend/.clang-format
 create mode 100644 libavfilter/inference_backend/ff_base_inference.c
 create mode 100644 libavfilter/inference_backend/ff_base_inference.h
 create mode 100644 libavfilter/inference_backend/ff_inference_impl.c
 create mode 100644 libavfilter/inference_backend/ff_inference_impl.h
 create mode 100644 libavfilter/inference_backend/ff_list.c
 create mode 100644 libavfilter/inference_backend/ff_list.h
 create mode 100644 libavfilter/inference_backend/ff_proc_factory.c
 create mode 100644 libavfilter/inference_backend/ff_proc_factory.h
 create mode 100644 libavfilter/inference_backend/image.c
 create mode 100644 libavfilter/inference_backend/image.h
 create mode 100644 libavfilter/inference_backend/image_inference.c
 create mode 100644 libavfilter/inference_backend/image_inference.h
 create mode 100644 libavfilter/inference_backend/image_inference_async_preproc.c
 create mode 100644 libavfilter/inference_backend/image_inference_async_preproc.h
 create mode 100644 libavfilter/inference_backend/logger.c
 create mode 100644 libavfilter/inference_backend/logger.h
 create mode 100644 libavfilter/inference_backend/metaconverter.c
 create mode 100644 libavfilter/inference_backend/metaconverter.h
 create mode 100644 libavfilter/inference_backend/model_proc.c
 create mode 100644 libavfilter/inference_backend/model_proc.h
 create mode 100644 libavfilter/inference_backend/openvino_image_inference.c
 create mode 100644 libavfilter/inference_backend/openvino_image_inference.h
 create mode 100644 libavfilter/inference_backend/pre_proc.c
 create mode 100644 libavfilter/inference_backend/pre_proc.h
 create mode 100644 libavfilter/inference_backend/pre_proc_mocker.c
 create mode 100644 libavfilter/inference_backend/pre_proc_swscale.c
 create mode 100644 libavfilter/inference_backend/pre_proc_vaapi.c
 create mode 100644 libavfilter/inference_backend/queue.c
 create mode 100644 libavfilter/inference_backend/safe_queue.c
 create mode 100644 libavfilter/inference_backend/safe_queue.h
 create mode 100644 libavfilter/vf_ocv_overlay.c

diff --git a/configure b/configure
index f7feedb..44af7be 100755
--- a/configure
+++ b/configure
@@ -238,8 +238,7 @@ External library support:
   --enable-libgsm          enable GSM de/encoding via libgsm [no]
   --enable-libiec61883     enable iec61883 via libiec61883 [no]
   --enable-libilbc         enable iLBC de/encoding via libilbc [no]
-  --enable-libinference_engine enable intel inference engine as a DNN module
-                               backend [no]
+  --enable-libinference_engine_c_api enable dldt inference engine c api [no]
   --enable-libjack         enable JACK audio sound server [no]
   --enable-libjson_c       enable libjson-c [no]
   --enable-libklvanc       enable Kernel Labs VANC processing [no]
@@ -1726,7 +1725,7 @@ EXTERNAL_LIBRARY_LIST="
     libgsm
     libiec61883
     libilbc
-    libinference_engine
+    libinference_engine_c_api
     libjack
     libjson_c
     libklvanc
@@ -2309,6 +2308,7 @@ CONFIG_EXTRA="
     huffyuvdsp
     huffyuvencdsp
     idctdsp
+    image_inference
     iirfilter
     mdct15
     intrax8
@@ -2551,7 +2551,9 @@ cbs_mpeg2_select="cbs"
 cbs_vp9_select="cbs"
 dct_select="rdft"
 dirac_parse_select="golomb"
-dnn_suggest="libtensorflow libinference_engine"
+dnn_suggest="libtensorflow"
+image_inference_suggest="libinference_engine_c_api"
+image_inference_deps="libjson_c"
 error_resilience_select="me_cmp"
 faandct_deps="faan"
 faandct_select="fdctdsp"
@@ -3413,13 +3415,13 @@ fspp_filter_deps="gpl"
 geq_filter_deps="gpl"
 histeq_filter_deps="gpl"
 hqdn3d_filter_deps="gpl"
-inference_classify_filter_deps="libinference_engine libjson_c"
-inference_classify_filter_select="dnn"
-inference_detect_filter_deps="libinference_engine libjson_c"
-inference_detect_filter_select="dnn"
-inference_identify_filter_deps="libinference_engine libjson_c"
+inference_identify_filter_deps="libjson_c"
 inference_identify_filter_select="dnn"
-inference_metaconvert_filter_deps="libinference_engine libjson_c"
+inference_metaconvert_filter_deps="libjson_c"
+inference_classify_filter_deps="libinference_engine_c_api libjson_c"
+inference_classify_filter_select="image_inference"
+inference_detect_filter_deps="libinference_engine_c_api libjson_c"
+inference_detect_filter_select="image_inference"
 interlace_filter_deps="gpl"
 kerndeint_filter_deps="gpl"
 ladspa_filter_deps="ladspa libdl"
@@ -3434,6 +3436,7 @@ negate_filter_deps="lut_filter"
 nnedi_filter_deps="gpl"
 ocr_filter_deps="libtesseract"
 ocv_filter_deps="libopencv"
+ocv_overlay_filter_deps="libopencv"
 openclsrc_filter_deps="opencl"
 overlay_opencl_filter_deps="opencl"
 overlay_qsv_filter_deps="libmfx"
@@ -6255,13 +6258,13 @@ enabled rkmpp             && { require_pkg_config rkmpp rockchip_mpp  rockchip/r
                              }
 enabled vapoursynth       && require_pkg_config vapoursynth "vapoursynth-script >= 42" VSScript.h vsscript_init
 
-enabled libinference_engine &&
-    require_pkg_config libinference_engine dldt "ie_api_wrapper.h" IESizeOfContext
-
 enabled librdkafka  && require_pkg_config librdkafka rdkafka "librdkafka/rdkafka.h" rd_kafka_version
 
 enabled libjson_c && check_pkg_config libjson_c json-c json-c/json.h json_c_version
 
+enabled libinference_engine_c_api &&
+    require_pkg_config libinference_engine_c_api dldt_c_api "ie_c_api.h" ie_c_api_version
+
 if enabled gcrypt; then
     GCRYPT_CONFIG="${cross_prefix}libgcrypt-config"
     if "${GCRYPT_CONFIG}" --version > /dev/null 2>&1; then
diff --git a/fftools/ffmpeg.c b/fftools/ffmpeg.c
index 8152a77..7a2863d 100644
--- a/fftools/ffmpeg.c
+++ b/fftools/ffmpeg.c
@@ -1691,6 +1691,7 @@ static void print_report(int is_last_report, int64_t timer_start, int64_t cur_ti
                 if (!ft)
                     continue;
                 init_time += ft->init_working_time;
+                av_log(ft, AV_LOG_INFO, "init time:%"PRId64"\n", ft->init_working_time);
             }
         }
     }
@@ -1780,14 +1781,16 @@ static void print_report(int is_last_report, int64_t timer_start, int64_t cur_ti
         total_frames_num = 0;
         for (i = 0; i < nb_output_streams; i++) {
             ost = output_streams[i];
-            total_frames_num += ost->frame_number;
+            if (ost->enc_ctx->codec_type == AVMEDIA_TYPE_VIDEO)
+                total_frames_num += ost->frames_encoded;
         }
         float total_fps;
         total_fps = t > 1 ? total_frames_num / t : 0;
         av_bprintf(&buf, "| profiling | total frame=%d ", total_frames_num);
         av_bprintf(&buf, "fps=%.2f |", total_fps);
         total_fps = t > 1 ? total_frames_num / (t - init_time / 1000000.0 ): 0;
-        av_bprintf(&buf, ", fps without filter init=%.2f |", total_fps);
+        if (total_fps > 0)
+            av_bprintf(&buf, " fps without filter init=%.2f |", total_fps);
     }
 
     secs = FFABS(pts) / AV_TIME_BASE;
@@ -4291,6 +4294,40 @@ static int process_input(int file_index)
     int ret, thread_ret, i, j;
     int64_t duration;
     int64_t pkt_dts;
+    AVCodecContext *avctx;
+
+    for (i = 0; i < ifile->nb_streams; i++) {
+        ist = input_streams[ifile->ist_index + i];
+
+        if (load_balance && ist->filters && ist->filters[0]->filter) {
+            ret = avfilter_chain_occupation(ist->filters[0]->filter);
+            if (ret >= load_balance) {
+                for (j = 0; j < nb_filtergraphs; j++) {
+                    FilterGraph *fg = filtergraphs[j];
+                    avfilter_graph_set_parsed(fg->graph);
+                }
+                usleep(1000);
+                return 0;
+            }
+        }
+
+        avctx = ist->dec_ctx;
+        if (avctx && avctx->hw_frames_ctx) {
+            AVHWFramesContext *frames_ctx = (AVHWFramesContext*)avctx->hw_frames_ctx->data;
+
+            int empty = av_buffer_pool_is_empty(frames_ctx->pool);
+
+            if (empty) {
+                av_log(NULL, AV_LOG_VERBOSE, "Buffer pool is empty.\n");
+                for (j = 0; j < nb_filtergraphs; j++) {
+                    FilterGraph *fg = filtergraphs[j];
+                    avfilter_graph_set_parsed(fg->graph);
+                }
+                usleep(1000);
+                return 0;
+            }
+        }
+    }
 
     is  = ifile->ctx;
     ret = get_input_packet(ifile, &pkt);
@@ -4763,7 +4800,7 @@ static int transcode(void)
     for (i = 0; i < nb_output_streams; i++) {
         ost = output_streams[i];
         if (ost->encoding_needed) {
-            if (do_profiling_all) {
+            if (0) {
                 if (ost->enc_ctx->frame_number > 1 && ost->enc_ctx->sum_working_time > 1) {
                     double fps = (double)(ost->enc_ctx->frame_number * 1000000) / ost->enc_ctx->sum_working_time;
                     printf("| encode profiling | name=%s, frame=%d, fps=%.2f\n",
@@ -4784,7 +4821,7 @@ static int transcode(void)
     for (i = 0; i < nb_input_streams; i++) {
         ist = input_streams[i];
         if (ist->decoding_needed) {
-            if (do_profiling_all && !ist->dec_ctx->hwaccel) {
+            if (0) {
                 if (ist->dec_ctx->frame_number > 1 && ist->dec_ctx->sum_working_time > 1) {
                     double fps = (double)(ist->dec_ctx->frame_number * 1000000) / ist->dec_ctx->sum_working_time;
                     printf("| sw decode profiling | name=%s, frame=%d, fps=%.2f\n",
@@ -4817,7 +4854,7 @@ static int transcode(void)
                 if (ft->sum_working_time > 1) {
                     double fps = (double)(frame_cnt * 1000000) / ft->sum_working_time;
                     if (fps < 10000) { //some filter delivered too big fps is not we focused
-                        printf("| filter profiling | name=%s, init=%.2f ms, frame=%d, fps=%.2f\n",
+                        printf("| filter profiling | name=%s, init=%.2f ms, frame=%ld, fps=%.2f\n",
                                 ft->filter->name, (double)ft->init_working_time / 1000, frame_cnt, fps);
                     }
                 }
diff --git a/fftools/ffmpeg.h b/fftools/ffmpeg.h
index dd99dd2..eba5c10 100644
--- a/fftools/ffmpeg.h
+++ b/fftools/ffmpeg.h
@@ -591,6 +591,7 @@ extern float frame_drop_threshold;
 extern int do_benchmark;
 extern int do_benchmark_all;
 extern int do_profiling_all;
+extern int load_balance;
 extern int do_deinterlace;
 extern int do_hex_dump;
 extern int do_pkt_dump;
diff --git a/fftools/ffmpeg_opt.c b/fftools/ffmpeg_opt.c
index eda8f3e..dd1cc9c 100644
--- a/fftools/ffmpeg_opt.c
+++ b/fftools/ffmpeg_opt.c
@@ -95,6 +95,7 @@ int do_deinterlace    = 0;
 int do_benchmark      = 0;
 int do_benchmark_all  = 0;
 int do_profiling_all  = 0;
+int load_balance      = 0;
 int do_hex_dump       = 0;
 int do_pkt_dump       = 0;
 int copy_ts           = 0;
@@ -3299,6 +3300,10 @@ int ffmpeg_parse_options(int argc, char **argv)
         av_profiling_set(do_profiling_all);
     }
 
+    if (load_balance) {
+        av_load_balance_set(load_balance);
+    }
+
 fail:
     uninit_parse_context(&octx);
     if (ret < 0) {
@@ -3403,6 +3408,8 @@ const OptionDef options[] = {
       "add timings for each task" },
     { "profiling_all",  OPT_BOOL | OPT_EXPERT,                       { &do_profiling_all },
       "print performance info based on all running pipelines" },
+    { "load_balance",   HAS_ARG | OPT_INT | OPT_EXPERT,              { &load_balance },
+      "enable the load balance, set the number of accumulated frames allowed" },
     { "progress",       HAS_ARG | OPT_EXPERT,                        { .func_arg = opt_progress },
       "write program-readable progress information", "url" },
     { "stdin",          OPT_BOOL | OPT_EXPERT,                       { &stdin_interaction },
diff --git a/libavfilter/Makefile b/libavfilter/Makefile
index 11f0fb4..c3a637e 100644
--- a/libavfilter/Makefile
+++ b/libavfilter/Makefile
@@ -27,8 +27,23 @@ OBJS-$(HAVE_THREADS)                         += pthread.o
 # subsystems
 OBJS-$(CONFIG_QSVVPP)                        += qsvvpp.o
 DNN-OBJS-$(CONFIG_LIBTENSORFLOW)             += dnn_backend_tf.o
-DNN-OBJS-$(CONFIG_LIBINFERENCE_ENGINE)       += dnn_backend_intel_ie.o inference.o
 OBJS-$(CONFIG_DNN)                           += dnn_interface.o dnn_backend_native.o $(DNN-OBJS-yes)
+OBJS-$(CONFIG_IMAGE_INFERENCE)               += inference_backend/ff_base_inference.o             \
+                                                inference_backend/ff_inference_impl.o             \
+                                                inference_backend/ff_list.o                       \
+                                                inference_backend/ff_proc_factory.o               \
+                                                inference_backend/image.o                         \
+                                                inference_backend/image_inference.o               \
+                                                inference_backend/image_inference_async_preproc.o \
+                                                inference_backend/logger.o                        \
+                                                inference_backend/model_proc.o                    \
+                                                inference_backend/openvino_image_inference.o      \
+                                                inference_backend/pre_proc.o                      \
+                                                inference_backend/pre_proc_mocker.o               \
+                                                inference_backend/pre_proc_swscale.o              \
+                                                inference_backend/pre_proc_vaapi.o                \
+                                                inference_backend/safe_queue.o                    \
+                                                inference_backend/metaconverter.o                 \
 
 # audio filters
 OBJS-$(CONFIG_ABENCH_FILTER)                 += f_bench.o
@@ -258,10 +273,10 @@ OBJS-$(CONFIG_HWUPLOAD_FILTER)               += vf_hwupload.o
 OBJS-$(CONFIG_HYSTERESIS_FILTER)             += vf_hysteresis.o framesync.o
 OBJS-$(CONFIG_IDET_FILTER)                   += vf_idet.o
 OBJS-$(CONFIG_IL_FILTER)                     += vf_il.o
-OBJS-$(CONFIG_INFERENCE_CLASSIFY_FILTER)     += vf_inference_classify.o
-OBJS-$(CONFIG_INFERENCE_DETECT_FILTER)       += vf_inference_detect.o
 OBJS-$(CONFIG_INFERENCE_IDENTIFY_FILTER)     += vf_inference_identify.o
 OBJS-$(CONFIG_INFERENCE_METACONVERT_FILTER)  += vf_inference_metaconvert.o
+OBJS-$(CONFIG_INFERENCE_CLASSIFY_FILTER)     += vf_inference_classify.o
+OBJS-$(CONFIG_INFERENCE_DETECT_FILTER)       += vf_inference_detect.o
 OBJS-$(CONFIG_INFLATE_FILTER)                += vf_neighbor.o
 OBJS-$(CONFIG_INTERLACE_FILTER)              += vf_tinterlace.o
 OBJS-$(CONFIG_INTERLEAVE_FILTER)             += f_interleave.o
@@ -297,6 +312,7 @@ OBJS-$(CONFIG_NORMALIZE_FILTER)              += vf_normalize.o
 OBJS-$(CONFIG_NULL_FILTER)                   += vf_null.o
 OBJS-$(CONFIG_OCR_FILTER)                    += vf_ocr.o
 OBJS-$(CONFIG_OCV_FILTER)                    += vf_libopencv.o
+OBJS-$(CONFIG_OCV_OVERLAY_FILTER)            += vf_ocv_overlay.o
 OBJS-$(CONFIG_OSCILLOSCOPE_FILTER)           += vf_datascope.o
 OBJS-$(CONFIG_OVERLAY_FILTER)                += vf_overlay.o framesync.o
 OBJS-$(CONFIG_OVERLAY_OPENCL_FILTER)         += vf_overlay_opencl.o opencl.o \
@@ -480,6 +496,7 @@ TOOLS-$(CONFIG_LIBZMQ) += zmqsend
 
 clean::
 	$(RM) $(CLEANSUFFIXES:%=libavfilter/libmpcodecs/%)
+	$(RM) $(CLEANSUFFIXES:%=libavfilter/inference_backend/%)
 
 OPENCL = $(subst $(SRC_PATH)/,,$(wildcard $(SRC_PATH)/libavfilter/opencl/*.cl))
 .SECONDARY: $(OPENCL:.cl=.c)
diff --git a/libavfilter/allfilters.c b/libavfilter/allfilters.c
index 4588c2b..589bbad 100644
--- a/libavfilter/allfilters.c
+++ b/libavfilter/allfilters.c
@@ -244,10 +244,10 @@ extern AVFilter ff_vf_hwupload_cuda;
 extern AVFilter ff_vf_hysteresis;
 extern AVFilter ff_vf_idet;
 extern AVFilter ff_vf_il;
-extern AVFilter ff_vf_inference_classify;
-extern AVFilter ff_vf_inference_detect;
 extern AVFilter ff_vf_inference_identify;
 extern AVFilter ff_vf_inference_metaconvert;
+extern AVFilter ff_vf_inference_classify;
+extern AVFilter ff_vf_inference_detect;
 extern AVFilter ff_vf_inflate;
 extern AVFilter ff_vf_interlace;
 extern AVFilter ff_vf_interleave;
@@ -283,6 +283,7 @@ extern AVFilter ff_vf_normalize;
 extern AVFilter ff_vf_null;
 extern AVFilter ff_vf_ocr;
 extern AVFilter ff_vf_ocv;
+extern AVFilter ff_vf_ocv_overlay;
 extern AVFilter ff_vf_oscilloscope;
 extern AVFilter ff_vf_overlay;
 extern AVFilter ff_vf_overlay_opencl;
diff --git a/libavfilter/avfilter.c b/libavfilter/avfilter.c
index 2951bc4..8e3eef7 100644
--- a/libavfilter/avfilter.c
+++ b/libavfilter/avfilter.c
@@ -279,6 +279,7 @@ int avfilter_config_links(AVFilterContext *filter)
 {
     int (*config_link)(AVFilterLink *);
     unsigned i;
+    int64_t tm_init = 0;
     int ret;
 
     for (i = 0; i < filter->nb_inputs; i ++) {
@@ -300,7 +301,6 @@ int avfilter_config_links(AVFilterContext *filter)
         case AVLINK_INIT:
             continue;
         case AVLINK_STARTINIT:
-            av_log(filter, AV_LOG_INFO, "circular filter chain detected\n");
             return 0;
         case AVLINK_UNINIT:
             link->init_state = AVLINK_STARTINIT;
@@ -366,13 +366,18 @@ int avfilter_config_links(AVFilterContext *filter)
                     return AVERROR(ENOMEM);
             }
 
-            if ((config_link = link->dstpad->config_props))
+            if ((config_link = link->dstpad->config_props)) {
+                if (av_profiling_get())
+                    tm_init = av_gettime();
                 if ((ret = config_link(link)) < 0) {
                     av_log(link->dst, AV_LOG_ERROR,
                            "Failed to configure input pad on %s\n",
                            link->dst->name);
                     return ret;
                 }
+                if (av_profiling_get())
+                    filter->init_working_time += av_gettime() - tm_init;
+            }
 
             link->init_state = AVLINK_INIT;
         }
@@ -761,7 +766,6 @@ static void free_link(AVFilterLink *link)
 void avfilter_free(AVFilterContext *filter)
 {
     int i;
-    int64_t frame_cnt = 0;
 
     if (!filter)
         return;
@@ -1022,7 +1026,7 @@ int avfilter_init_str(AVFilterContext *filter, const char *args)
         tm_init = av_gettime();
     ret = avfilter_init_dict(filter, &options);
     if (av_profiling_get())
-        filter->init_working_time = av_gettime() - tm_init;
+        filter->init_working_time += av_gettime() - tm_init;
 
     if (ret < 0)
         goto fail;
diff --git a/libavfilter/avfilter.h b/libavfilter/avfilter.h
index e70142e..5639fe8 100644
--- a/libavfilter/avfilter.h
+++ b/libavfilter/avfilter.h
@@ -1164,6 +1164,18 @@ char *avfilter_graph_dump(AVFilterGraph *graph, const char *options);
 int avfilter_graph_request_oldest(AVFilterGraph *graph);
 
 /**
+ * To get the filter chain queue frames occupation number.
+ *
+ * @return the number of buffered frames in the chain
+ */
+int avfilter_chain_occupation(AVFilterContext *avctx);
+
+/**
+ * Set parsed filter to be ready on a filter graph.
+ */
+int avfilter_graph_set_parsed(AVFilterGraph *graph);
+
+/**
  * @}
  */
 
diff --git a/libavfilter/avfiltergraph.c b/libavfilter/avfiltergraph.c
index a149f8f..072f8f0 100644
--- a/libavfilter/avfiltergraph.c
+++ b/libavfilter/avfiltergraph.c
@@ -41,6 +41,7 @@
 #include "formats.h"
 #include "internal.h"
 #include "thread.h"
+#include "filters.h"
 
 #define OFFSET(x) offsetof(AVFilterGraph, x)
 #define F AV_OPT_FLAG_FILTERING_PARAM
@@ -298,6 +299,19 @@ AVFilterContext *avfilter_graph_get_filter(AVFilterGraph *graph, const char *nam
     return NULL;
 }
 
+int avfilter_graph_set_parsed(AVFilterGraph *graph)
+{
+    AVFilterContext *filter;
+    unsigned i;
+
+    for (i = 0; i < graph->nb_filters; i++)
+        if (!strncmp(graph->filters[i]->name, "Parsed", 6)) {
+            filter = graph->filters[i];
+            ff_filter_set_ready(filter, 300);
+        }
+    return 0;
+}
+
 static void sanitize_channel_layouts(void *log, AVFilterChannelLayouts *l)
 {
     if (!l)
@@ -1439,6 +1453,29 @@ int avfilter_graph_request_oldest(AVFilterGraph *graph)
     return 0;
 }
 
+int avfilter_chain_occupation(AVFilterContext *avctx)
+{
+    AVFilterLink *link = NULL;
+    int frm_num = 0;
+
+    if (!avctx)
+        return 0;
+
+    while (avctx) {
+        if (avctx->nb_inputs) {
+            link = avctx->inputs[0];
+            frm_num += ff_framequeue_queued_frames(&link->fifo);
+        }
+        if (avctx->nb_outputs) {
+            link = avctx->outputs[0];
+            avctx = link->dst;
+        } else
+            break;
+    }
+
+    return frm_num;
+}
+
 int ff_filter_graph_run_once(AVFilterGraph *graph)
 {
     AVFilterContext *filter;
diff --git a/libavfilter/dnn_backend_intel_ie.c b/libavfilter/dnn_backend_intel_ie.c
deleted file mode 100644
index 561cc15..0000000
--- a/libavfilter/dnn_backend_intel_ie.c
+++ /dev/null
@@ -1,419 +0,0 @@
-/*
- * Copyright (c) 2018 Pengfei Qu, Lin Xie
- *
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2.1 of the License, or (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-
-/**
- * @file
- * DNN inference functions interface for intel inference engine backend.
- */
-
-#include "dnn_backend_intel_ie.h"
-#include "libavformat/avio.h"
-#include <ie_api_wrapper.h>
-
-typedef struct DNNIntelIEModel {
-    void *context;
-    IEConfig config;
-    IEInputOutputInfo *input_infos;
-    IEInputOutputInfo *output_infos;
-} DNNIntelIEModel;
-
-static IETargetDeviceType get_device_type_id(DNNTargetDeviceType device_type) {
-    switch (device_type) {
-    case DNN_TARGET_DEVICE_DEFAULT:
-        return IE_Default;
-    case DNN_TARGET_DEVICE_BALANCED:
-        return IE_Balanced;
-    case DNN_TARGET_DEVICE_CPU:
-        return IE_CPU;
-    case DNN_TARGET_DEVICE_GPU:
-        return IE_GPU;
-    case DNN_TARGET_DEVICE_FPGA:
-        return IE_FPGA;
-    case DNN_TARGET_DEVICE_MYRIAD:
-        return IE_MYRIAD;
-    case DNN_TARGET_DEVICE_HDDL:
-        return IE_HDDL;
-    case DNN_TARGET_DEVICE_GNA:
-        return IE_GNA;
-    case DNN_TARGET_DEVICE_HETERO:
-        return IE_HETERO;
-    default:
-        return IE_Default;
-    }
-}
-
-static IELayoutType get_layout(DNNDataLayoutType layout)
-{
-    switch (layout) {
-    case DNN_DATA_LAYOUT_NCHW:
-        return IE_NCHW;
-    case DNN_DATA_LAYOUT_NHWC:
-        return IE_NHWC;
-    case DNN_DATA_LAYOUT_OIHW:
-        return IE_OIHW;
-    case DNN_DATA_LAYOUT_C:
-        return IE_C;
-    case DNN_DATA_LAYOUT_CHW:
-        return IE_CHW;
-    case DNN_DATA_LAYOUT_HW:
-        return IE_HW;
-    case DNN_DATA_LAYOUT_NC:
-        return IE_NC;
-    case DNN_DATA_LAYOUT_CN:
-        return IE_CN;
-    case DNN_DATA_LAYOUT_BLOCKED:
-        return IE_BLOCKED;
-    case DNN_DATA_LAYOUT_ANY:
-        return IE_ANY;
-    case DNN_DATA_LAYOUT_1D:
-        return IE_ANY;
-    default:
-        return IE_ANY;
-    }
-}
-
-static DNNDataLayoutType get_dnn_layout(IELayoutType layout)
-{
-    switch (layout) {
-    case IE_NCHW:
-        return DNN_DATA_LAYOUT_NCHW;
-    case IE_NHWC:
-        return DNN_DATA_LAYOUT_NHWC;
-    case IE_OIHW:
-        return DNN_DATA_LAYOUT_OIHW;
-    case IE_C:
-        return DNN_DATA_LAYOUT_C;
-    case IE_CHW:
-        return DNN_DATA_LAYOUT_CHW;
-    case IE_HW:
-        return DNN_DATA_LAYOUT_HW;
-    case IE_NC:
-        return DNN_DATA_LAYOUT_NC;
-    case IE_CN:
-        return DNN_DATA_LAYOUT_CN;
-    case IE_BLOCKED:
-        return DNN_DATA_LAYOUT_BLOCKED;
-    case IE_ANY:
-        return DNN_DATA_LAYOUT_ANY;
-    default:
-        return DNN_DATA_LAYOUT_ANY;
-    }
-}
-
-static IEPrecisionType get_precision(DNNDataPrecisionType precision)
-{
-    switch (precision) {
-    case DNN_DATA_PRECISION_MIXED:
-        return IE_MIXED;
-    case DNN_DATA_PRECISION_FP32:
-        return IE_FP32;
-    case DNN_DATA_PRECISION_FP16:
-        return IE_FP16;
-    case DNN_DATA_PRECISION_Q78:
-        return IE_Q78;
-    case DNN_DATA_PRECISION_I16:
-        return IE_I16;
-    case DNN_DATA_PRECISION_U8:
-        return IE_U8;
-    case DNN_DATA_PRECISION_I8:
-        return IE_I8;
-    case DNN_DATA_PRECISION_U16:
-        return IE_U16;
-    case DNN_DATA_PRECISION_I32:
-        return IE_I32;
-    case DNN_DATA_PRECISION_CUSTOM:
-        return IE_CUSTOM;
-    case DNN_DATA_PRECISION_UNSPECIFDNND:
-        return IE_UNSPECIFIED;
-    default:
-        return IE_FP32;
-    }
-}
-
-static DNNDataPrecisionType get_dnn_precision(IEPrecisionType precision)
-{
-    switch (precision) {
-    case IE_MIXED:
-        return DNN_DATA_PRECISION_MIXED;
-    case IE_FP32:
-        return DNN_DATA_PRECISION_FP32;
-    case IE_FP16:
-        return DNN_DATA_PRECISION_FP16;
-    case IE_Q78:
-        return DNN_DATA_PRECISION_Q78;
-    case IE_I16:
-        return DNN_DATA_PRECISION_I16;
-    case IE_U8:
-        return DNN_DATA_PRECISION_U8;
-    case IE_I8:
-        return DNN_DATA_PRECISION_I8;
-    case IE_U16:
-        return DNN_DATA_PRECISION_U16;
-    case IE_I32:
-        return DNN_DATA_PRECISION_I32;
-    case IE_CUSTOM:
-        return DNN_DATA_PRECISION_CUSTOM;
-    case IE_UNSPECIFIED:
-        return DNN_DATA_PRECISION_UNSPECIFDNND;
-    default:
-        return DNN_DATA_PRECISION_FP32;
-    }
-}
-
-static IEImageFormatType get_data_format(DNNDataFormat format)
-{
-    switch (format) {
-    case DNN_DATA_BGR_PACKED:
-    case DNN_DATA_BGRA_PACKED:
-        return IE_IMAGE_BGR_PACKED;
-    case DNN_DATA_BGR_PLANAR:
-    case DNN_DATA_BGRA_PLANAR:
-        return IE_IMAGE_BGR_PLANAR;
-    case DNN_DATA_RGB_PACKED:
-        return IE_IMAGE_RGB_PACKED;
-    case DNN_DATA_RGB_PLANAR:
-        return IE_IMAGE_RGB_PLANAR;
-    case DNN_DATA_GRAY_PLANAR:
-        return IE_IMAGE_GRAY_PLANAR;
-    case DNN_DATA_GENERIC_1D:
-        return IE_IMAGE_GENERIC_1D;
-    case DNN_DATA_GENERIC_2D:
-        return IE_IMAGE_GENERIC_2D;
-    default:
-        return IE_IMAGE_FORMAT_UNKNOWN;
-    }
-}
-
-static void set_model_config_internal(DNNIntelIEModel *ie_model, DNNModelIntelIEConfig *ie_config)
-{
-    ie_model->config.targetId      = get_device_type_id(ie_config->device);
-    ie_model->config.modelFileName = ie_config->model;
-    ie_model->config.cpuExtPath    = ie_config->cpu_extension;
-    ie_model->config.cldnnExtPath  = ie_config->gpu_extension;
-    ie_model->config.perfCounter   = 0;
-
-    ie_model->input_infos          = &(ie_model->config.inputInfos);
-    ie_model->output_infos         = &(ie_model->config.outputInfos);
-}
-
-static DNNReturnType get_execute_result_intel_ie(void *model, DNNIOData *result)
-{
-    unsigned int size = 0;
-    DNNIntelIEModel *ie_model = (DNNIntelIEModel *)model;
-
-    if (!model || !result)
-        return DNN_ERROR;
-
-    result->data[0] = IEGetResultSpace(ie_model->context, result->in_out_idx, &size);
-    if (!result->data)
-        return DNN_ERROR;
-
-    result->size = size;
-    result->precision = DNN_DATA_PRECISION_FP32;
-
-    return DNN_SUCCESS;
-}
-
-static DNNReturnType get_input_info_intel_ie(void *model, DNNModelInfo *info)
-{
-    int id = 0;
-    DNNIntelIEModel *ie_model = (DNNIntelIEModel *)model;
-
-    if (!model || !info)
-        return DNN_ERROR;
-
-    IEGetModelInputInfo(ie_model->context, ie_model->input_infos);
-
-    if (ie_model->input_infos->number > DNN_INPUT_OUTPUT_NUM)
-        return DNN_ERROR;
-
-    for (id = 0; id < ie_model->input_infos->number; id++) {
-        memcpy(&info->dims[id][0],
-               &ie_model->input_infos->tensorMeta[id].dims[0],
-               4 * sizeof(info->dims[id][0]));
-
-        info->layer_name[id] = ie_model->input_infos->tensorMeta[id].layer_name;
-        info->precision[id]  = get_dnn_precision(ie_model->input_infos->tensorMeta[id].precision);
-        info->layout[id]     = get_dnn_layout(ie_model->input_infos->tensorMeta[id].layout);
-    }
-    info->batch_size = ie_model->input_infos->batch_size;
-    info->number     = ie_model->input_infos->number;
-
-    return DNN_SUCCESS;
-}
-
-static DNNReturnType set_input_info_intel_ie(void *model, DNNModelInfo *info)
-{
-    int id = 0;
-    DNNIntelIEModel *ie_model = (DNNIntelIEModel *)model;
-
-    if (!model || !info || info->number > DNN_INPUT_OUTPUT_NUM)
-        return DNN_ERROR;
-
-    // image set to input 0
-    ie_model->input_infos->tensorMeta[0].precision = get_precision(info->precision[id]);
-    ie_model->input_infos->tensorMeta[0].layout    = get_layout(info->layout[id]);
-    ie_model->input_infos->tensorMeta[0].dataType  = info->is_image[id];
-
-    ie_model->input_infos->number = info->number;
-
-    IESetModelInputInfo(ie_model->context, ie_model->input_infos);
-
-    return DNN_SUCCESS;
-}
-
-static DNNReturnType get_output_info_intel_ie(void *model, DNNModelInfo *info)
-{
-    int id = 0;
-    DNNIntelIEModel *ie_model = (DNNIntelIEModel *)model;
-
-    if (!model || !info)
-        return DNN_ERROR;
-
-    IEGetModelOutputInfo(ie_model->context, ie_model->output_infos);
-
-    if (ie_model->output_infos->number > DNN_INPUT_OUTPUT_NUM)
-        return DNN_ERROR;
-
-    for (id = 0; id < ie_model->output_infos->number; id++) {
-        memcpy(&info->dims[id][0],
-               &ie_model->output_infos->tensorMeta[id].dims[0],
-               4 * sizeof(info->dims[id][0]));
-
-        info->layer_name[id] = ie_model->output_infos->tensorMeta[id].layer_name;
-        info->precision[id]  = get_dnn_precision(ie_model->output_infos->tensorMeta[id].precision);
-        info->layout[id]     = get_dnn_layout(ie_model->output_infos->tensorMeta[id].layout);
-    }
-    info->batch_size = ie_model->output_infos->batch_size;
-    info->number     = ie_model->output_infos->number;
-
-    return DNN_SUCCESS;
-}
-
-static DNNReturnType set_input_intel_ie(void *model, const DNNIOData *input)
-{
-    int i;
-    IEData data;
-    DNNIntelIEModel *ie_model = (DNNIntelIEModel *)model;
-
-    if (!model || !input)
-        return DNN_ERROR;
-
-    memset(&data, 0, sizeof(IEData));
-
-    for (i = 0; i < NUM_DATA_POINTS; i++) {
-        data.data[i]     = input->data[i];
-        data.linesize[i] = input->linesize[i];
-    }
-    data.width        = input->width;
-    data.height       = input->height;
-    data.channelNum   = input->channels;
-    data.batchIdx     = input->batch_idx;
-    data.precision    = get_precision(input->precision);
-    data.memType      = input->memory_type;
-    data.dataType     = input->is_image;
-    data.imageFormat  = get_data_format(input->data_format);
-
-    IESetInput(ie_model->context, input->in_out_idx, &data);
-
-    return DNN_SUCCESS;
-}
-
-static DNNReturnType create_model_intel_ie(void *model)
-{
-    DNNIntelIEModel *ie_model = (DNNIntelIEModel *)model;
-
-    if (!model)
-        return DNN_ERROR;
-
-    IECreateModel(ie_model->context, &ie_model->config);
-
-    return DNN_SUCCESS;
-}
-
-DNNModel* ff_dnn_load_model_intel_ie(void *config)
-{
-    DNNModel *model = NULL;
-    DNNIntelIEModel *ie_model = NULL;
-    DNNModelIntelIEConfig *ie_config = (DNNModelIntelIEConfig *)config;
-
-    if (!ie_config)
-        return NULL;
-
-    model = av_mallocz(sizeof(DNNModel));
-    if (!model)
-        return NULL;
-
-    ie_model = av_mallocz(sizeof(DNNIntelIEModel));
-    if (!ie_model) {
-        av_freep(&model);
-        return NULL;
-    }
-
-    set_model_config_internal(ie_model, ie_config);
-
-    ie_model->context = IEAllocateContext();
-    if (!ie_model->context) {
-        av_freep(&ie_model);
-        av_freep(&model);
-        return NULL;
-    }
-
-    IELoadModel(ie_model->context, &ie_model->config);
-
-    IESetBatchSize(ie_model->context, ie_config->batch_size);
-
-    model->model              = (void *)ie_model;
-    model->get_execute_result = &get_execute_result_intel_ie;
-    model->set_input          = &set_input_intel_ie;
-    model->get_input_info     = &get_input_info_intel_ie;
-    model->set_input_info     = &set_input_info_intel_ie;
-    model->get_output_info    = &get_output_info_intel_ie;
-    model->create_model       = &create_model_intel_ie;
-
-    return model;
-}
-
-DNNReturnType ff_dnn_execute_model_intel_ie(const DNNModel *model)
-{
-    DNNIntelIEModel *ie_model = NULL;
-
-    if (!model)
-        return DNN_ERROR;
-
-    ie_model = (DNNIntelIEModel *)model->model;
-
-    IEForward(ie_model->context, IE_INFER_MODE_SYNC);
-
-    return DNN_SUCCESS;
-}
-
-void ff_dnn_free_model_intel_ie(DNNModel** model)
-{
-    DNNIntelIEModel * ie_model = NULL;
-
-    if (*model) {
-        ie_model = (DNNIntelIEModel *)(*model)->model;
-        IEFreeContext(ie_model->context);
-        av_freep(&ie_model);
-        av_freep(model);
-    }
-}
-
diff --git a/libavfilter/dnn_backend_intel_ie.h b/libavfilter/dnn_backend_intel_ie.h
deleted file mode 100644
index 4879362..0000000
--- a/libavfilter/dnn_backend_intel_ie.h
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Copyright (c) 2018 Pengfei Qu, Lin Xie
- *
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2.1 of the License, or (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-
-/**
- * @file
- * DNN inference functions interface for intel inference engine backend.
- */
-
-
-#ifndef AVFILTER_DNN_BACKEND_INTEL_IE_H
-#define AVFILTER_DNN_BACKEND_INTEL_IE_H
-
-#include "dnn_interface.h"
-
-DNNModel *ff_dnn_load_model_intel_ie(void *model_config);
-
-DNNReturnType ff_dnn_execute_model_intel_ie(const DNNModel *model);
-
-void ff_dnn_free_model_intel_ie(DNNModel** model);
-
-DNNReturnType ff_dnn_create_model_intel_ie(const DNNModel *model);
-
-#endif
diff --git a/libavfilter/dnn_interface.c b/libavfilter/dnn_interface.c
index a321e67..86fc283 100644
--- a/libavfilter/dnn_interface.c
+++ b/libavfilter/dnn_interface.c
@@ -26,7 +26,6 @@
 #include "dnn_interface.h"
 #include "dnn_backend_native.h"
 #include "dnn_backend_tf.h"
-#include "dnn_backend_intel_ie.h"
 #include "libavutil/mem.h"
 
 DNNModule *ff_get_dnn_module(DNNBackendType backend_type)
@@ -54,16 +53,6 @@ DNNModule *ff_get_dnn_module(DNNBackendType backend_type)
         return NULL;
     #endif
         break;
-    case DNN_INTEL_IE:
-    #if (CONFIG_LIBINFERENCE_ENGINE == 1)
-        dnn_module->load_model_with_config = &ff_dnn_load_model_intel_ie;
-        dnn_module->execute_model          = &ff_dnn_execute_model_intel_ie;
-        dnn_module->free_model             = &ff_dnn_free_model_intel_ie;
-    #else
-        av_freep(&dnn_module);
-        return NULL;
-    #endif
-        break;
     default:
         av_log(NULL, AV_LOG_ERROR, "Module backend_type is not native or tensorflow\n");
         av_freep(&dnn_module);
diff --git a/libavfilter/dnn_interface.h b/libavfilter/dnn_interface.h
index 96c6131..722a04a 100644
--- a/libavfilter/dnn_interface.h
+++ b/libavfilter/dnn_interface.h
@@ -30,7 +30,7 @@
 
 typedef enum {DNN_SUCCESS, DNN_ERROR} DNNReturnType;
 
-typedef enum {DNN_NATIVE, DNN_TF, DNN_INTEL_IE} DNNBackendType;
+typedef enum {DNN_NATIVE, DNN_TF} DNNBackendType;
 
 typedef struct DNNData{
     float *data;
diff --git a/libavfilter/inference.c b/libavfilter/inference.c
deleted file mode 100644
index 97ce8fb..0000000
--- a/libavfilter/inference.c
+++ /dev/null
@@ -1,1042 +0,0 @@
-/*
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2.1 of the License, or (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-
-/**
- * @file
- * inference base function
- */
-
-#include "formats.h"
-#include "internal.h"
-#include "avfilter.h"
-#include "libavcodec/avcodec.h"
-#include "libavformat/avformat.h"
-#include "libswscale/swscale.h"
-#include "libavutil/pixdesc.h"
-#include "libavutil/avassert.h"
-#include "libavutil/imgutils.h"
-
-#include "inference.h"
-
-#if CONFIG_LIBJSON_C
-#include <json-c/json.h>
-#endif
-
-#if CONFIG_VAAPI
-#define VA_CALL(_FUNC)                                     \
-    {                                                      \
-        VAStatus _status = _FUNC;                          \
-        if (_status != VA_STATUS_SUCCESS)                  \
-        {                                                  \
-            printf(#_FUNC " failed, sts = %d (%s).\n",     \
-                    _status, vaErrorStr(_status));         \
-            return AVERROR(EINVAL);                        \
-        }                                                  \
-    }
-#endif
-
-struct _InferenceBaseContext
-{
-    char *infer_type;
-    int   batch_size;
-
-    DNNModule *module;
-    DNNModel  *model;
-
-    DNNModelInfo input_info;
-    DNNModelInfo output_info;
-
-    VideoPP vpp;
-
-    InferencePreProcess preprocess;
-};
-
-static int va_vpp_crop_and_scale(VAAPIVpp *va_vpp, AVFrame *input, Rect *crop_rect,
-        int scale_w, int scale_h, uint8_t *data[],  int stride[]);
-
-static int va_vpp_scale(VAAPIVpp *va_vpp, AVFrame *input,
-        int scale_w, int scale_h, uint8_t *data[],  int stride[]);
-
-static void infer_labels_buffer_free(void *opaque, uint8_t *data)
-{
-    int i;
-    LabelsArray *labels = (LabelsArray *)data;
-
-    for (i = 0; i < labels->num; i++)
-        av_freep(&labels->label[i]);
-
-    av_free(labels->label);
-
-    av_free(data);
-}
-
-// helper functions
-static void infer_labels_dump(uint8_t *data)
-{
-    int i;
-    LabelsArray *labels = (LabelsArray *)data;
-    printf("labels: ");
-    for (i = 0; i < labels->num; i++)
-        printf("%s ", labels->label[i]);
-    printf("\n");
-}
-
-int ff_get_file_size(FILE *fp)
-{
-    int file_size, current_pos;
-
-    if (!fp)
-        return -1;
-
-    current_pos = ftell(fp);
-
-    if (fseek(fp, 0, SEEK_END)) {
-        fprintf(stderr, "Couldn't seek to the end of feature file.\n");
-        return -1;
-    }
-
-    file_size = ftell(fp);
-
-    fseek(fp, current_pos, SEEK_SET);
-
-    return file_size;
-}
-
-
-static int fill_dnn_data_from_frame(DNNIOData *data,
-                                    const AVFrame *frame,
-                                    int batch_idx,
-                                    int is_image,
-                                    int input_idx)
-{
-    int i, channels_nb;
-    DNNDataFormat dnn_fmt;
-    DNNDataPrecisionType precision;
-    enum AVPixelFormat pix_fmt = frame->format;
-
-    switch (pix_fmt)
-    {
-    case AV_PIX_FMT_GRAY8:
-        precision = DNN_DATA_PRECISION_U8;
-        dnn_fmt = DNN_DATA_GRAY_PLANAR;
-        channels_nb = 1;
-        break;
-    case AV_PIX_FMT_BGRA:
-    case AV_PIX_FMT_BGR0:
-        precision = DNN_DATA_PRECISION_U8;
-        dnn_fmt = DNN_DATA_BGR_PACKED;
-        channels_nb = 4;
-        break;
-    case AV_PIX_FMT_BGR24:
-        precision = DNN_DATA_PRECISION_U8;
-        dnn_fmt = DNN_DATA_BGR_PACKED;
-        channels_nb = 3;
-        break;
-    case AV_PIX_FMT_RGBP:
-        precision = DNN_DATA_PRECISION_U8;
-        dnn_fmt = DNN_DATA_RGB_PLANAR;
-        channels_nb = 3;
-        break;
-    default:
-        av_log(NULL, AV_LOG_ERROR, "format unsupport!\n");
-        return AVERROR(EINVAL);
-    };
-
-    for (i = 0; i < NUM_DATA_POINTS; i++) {
-        data->data[i]     = frame->data[i];
-        data->linesize[i] = frame->linesize[i];
-    }
-    data->width          = frame->width;
-    data->height         = frame->height;
-    data->channels       = channels_nb;
-    data->data_format    = dnn_fmt;
-    data->precision      = precision;
-    data->memory_type    = DNN_MEM_HOST;
-    data->batch_idx      = batch_idx;
-    data->is_image       = is_image;
-    data->in_out_idx     = input_idx;
-
-    return 0;
-}
-
-static int sw_crop_and_scale(AVFrame *frame, Rect *crop_rect,
-                             int out_w,      int out_h,
-                             enum AVPixelFormat out_format,
-                             uint8_t *data[], int stride[])
-{
-    int ret = 0;
-    AVFrame *temp = NULL;
-    struct SwsContext *sws_ctx = NULL;
-    const AVPixFmtDescriptor *desc;
-    int x, y, w, h, hsub, vsub, bufsize;
-    int max_step[4]; ///< max pixel step for each plane, expressed as a number of bytes
-    enum AVPixelFormat expect_format = out_format;
-
-    if (!crop_rect)
-        return AVERROR(EINVAL);
-
-    temp = av_frame_alloc();
-    if (!temp)
-        return AVERROR(ENOMEM);
-
-    av_frame_ref(temp, frame);
-
-    desc = av_pix_fmt_desc_get(temp->format);
-    if (!desc) {
-        ret = AVERROR(EINVAL);
-        goto exit;
-    }
-    hsub = desc->log2_chroma_w;
-    vsub = desc->log2_chroma_h;
-    av_image_fill_max_pixsteps(max_step, NULL, desc);
-
-    /* cropping */
-    {
-        x = lrintf(crop_rect->x0);
-        y = lrintf(crop_rect->y0);
-        x = FFMAX(x, 0);
-        y = FFMAX(y, 0);
-        if (x >= frame->width || y >= frame->height) {
-            av_log(NULL, AV_LOG_ERROR, "Incorrect crop rect x:%d y:%d.\n", x, y);
-            ret = AVERROR(EINVAL);
-            goto exit;
-        }
-
-        w = lrintf(crop_rect->x1) - x;
-        h = lrintf(crop_rect->y1) - y;
-        w = FFMIN(w, frame->width - x);
-        h = FFMIN(h, frame->height - y);
-        if (w <= 0 || h <= 0) {
-            av_log(NULL, AV_LOG_ERROR, "Incorrect crop rect w:%d h:%d.\n", w, h);
-            ret = AVERROR(EINVAL);
-            goto exit;
-        }
-
-        temp->width  = w;
-        temp->height = h;
-
-        temp->data[0] += y * temp->linesize[0];
-        temp->data[0] += x * max_step[0];
-
-        for (int i = 1; i < 3; i ++) {
-            if (temp->data[i]) {
-                temp->data[i] += (y >> vsub) * temp->linesize[i];
-                temp->data[i] += (x * max_step[i]) >> hsub;
-            }
-        }
-
-        /* alpha plane */
-        if (temp->data[3]) {
-            temp->data[3] += y * temp->linesize[3];
-            temp->data[3] += x * max_step[3];
-        }
-    }
-
-    /* create scaling context */
-    sws_ctx = sws_getContext(temp->width, temp->height, temp->format,
-                             out_w, out_h, expect_format,
-                             SWS_BILINEAR, NULL, NULL, NULL);
-    if (!sws_ctx) {
-        av_log(NULL, AV_LOG_ERROR, "Create scaling context failed!\n");
-        ret = AVERROR(EINVAL);
-        goto exit;
-    }
-
-    if (!data[0]) {
-        bufsize = av_image_alloc(data, stride, out_w, out_h, expect_format, 1);
-        if (bufsize < 0) {
-            ret = AVERROR(ENOMEM);
-            goto exit;
-        }
-    }
-
-    sws_scale(sws_ctx, (const uint8_t * const*)temp->data,
-              temp->linesize, 0, temp->height, data, stride);
-exit:
-    av_frame_free(&temp);
-    sws_freeContext(sws_ctx);
-    return ret;
-}
-
-void av_split(char *str, const char *delim, char **array, int *num, int max)
-{
-    char *p;
-    int i = 0;
-
-    if (!str || !delim || !array || !num)
-        return;
-
-    while (p = strtok(str, delim)) {
-        int j = 0;
-        char *s;
-        size_t end;
-
-        /* remove head blanks */
-        while (p[j] == '\n' || p[j] == ' ')
-            j++;
-
-        if (!p[j]) continue;
-
-        /* remove tail blanks */
-        s   = p + j;
-        end = strlen(s) - 1;
-        while (s[end] == '\n' || s[end] == ' ')
-            s[end--] = '\0';
-
-        array[i++] = s;
-        av_assert0 (i < max);
-
-        /* string is cached */
-        str = NULL;
-    }
-
-    *num = i;
-}
-
-double av_norm(float vec[], size_t num)
-{
-    size_t i;
-    double result = 0.0;
-
-    for (i = 0; i < num; i++)
-        result += vec[i] * vec[i];
-
-    return sqrt(result);
-}
-
-double av_dot(float vec1[], float vec2[], size_t num)
-{
-    size_t i;
-    double result = 0.0;
-
-    for (i = 0; i < num; i++)
-        result += vec1[i] * vec2[i];
-
-    return result;
-}
-
-int ff_inference_base_create(AVFilterContext *ctx,
-                             InferenceBaseContext **base,
-                             InferenceParam *param)
-{
-    int i, ret;
-    InferenceBaseContext *s;
-    VideoPP *vpp;
-    DNNModelInfo *info;
-    DNNModelIntelIEConfig config;
-
-    if (!param)
-        return AVERROR(EINVAL);
-
-    s = av_mallocz(sizeof(*s));
-    if (!s)
-        return AVERROR(ENOMEM);
-
-    s->module = ff_get_dnn_module(param->backend_type);
-    if (!s->module) {
-        av_log(ctx, AV_LOG_ERROR, "could not create DNN backend module\n");
-        av_freep(&s);
-        return AVERROR(ENOMEM);
-    }
-
-    // parameter sanity check
-    if (param->batch_size <= 0) param->batch_size = 1;
-
-    config = (DNNModelIntelIEConfig) {
-        .model         = param->model_file,
-        .labels        = param->labels_file,
-        .device        = param->device_type,
-        .batch_size    = param->batch_size,
-        .cpu_extension = param->cpu_extension,
-        .gpu_extension = param->gpu_extension,
-    };
-    s->model = s->module->load_model_with_config(&config);
-    if (!s->model) {
-        av_log(ctx, AV_LOG_ERROR, "could not load DNN model\n");
-        av_freep(&s->module);
-        av_freep(&s);
-        return AVERROR(ENOMEM);
-    }
-
-#define DNN_ERR_CHECK(ctx) \
-    if (ret != DNN_SUCCESS) { \
-        av_log(ctx, AV_LOG_ERROR, "Error in '%s' line %d: %d\n", __FUNCTION__, __LINE__, ret); \
-        goto fail; \
-    }\
-
-    ret = s->model->get_input_info(s->model->model, &s->input_info);
-    DNN_ERR_CHECK(ctx);
-
-    ret = s->model->get_output_info(s->model->model, &s->output_info);
-    DNN_ERR_CHECK(ctx);
-
-    info = &s->input_info;
-    for (i = 0; i < info->number; i++) {
-        info->layout[i]    = param->input_layout;
-        info->precision[i] = param->input_precision;
-        info->is_image[i]  = param->input_is_image;
-    }
-    ret = s->model->set_input_info(s->model->model, info);
-    DNN_ERR_CHECK(ctx);
-
-    s->batch_size      = param->batch_size;
-    s->preprocess      = param->preprocess;
-
-    ret = s->model->create_model(s->model->model);
-    DNN_ERR_CHECK(ctx);
-
-    vpp = &s->vpp;
-
-    // vpp init
-    vpp->sw_vpp = av_mallocz(sizeof(*vpp->sw_vpp));
-    if (!vpp->sw_vpp)
-        goto fail;
-
-    vpp->expect_format          = AV_PIX_FMT_BGR24;
-    vpp->sw_vpp->scale          = &sws_scale;
-    vpp->sw_vpp->crop_and_scale = &sw_crop_and_scale;
-
-    *base = s;
-#undef DNN_ERR_CHECK
-    return 0;
-fail:
-    s->module->free_model(&s->model);
-    av_freep(&s->module);
-    av_freep(&s);
-    return ret;
-}
-
-int ff_inference_base_free(InferenceBaseContext **base)
-{
-    InferenceBaseContext *s = *base;
-
-    if (!s)
-        return 0;
-
-    // VPP clean up
-    for (int i = 0; i < MAX_VPP_NUM; i++) {
-        if (s->vpp.frames[i])
-            av_frame_free(&s->vpp.frames[i]);
-        if (s->vpp.sw_vpp->scale_contexts[i])
-            sws_freeContext(s->vpp.sw_vpp->scale_contexts[i]);
-    }
-    av_freep(&s->vpp.sw_vpp);
-
-#if CONFIG_VAAPI
-    if (s->vpp.va_vpp) {
-        va_vpp_device_free(s->vpp.va_vpp);
-        av_freep(&s->vpp.va_vpp);
-    }
-#endif
-
-    if (s->module) {
-        s->module->free_model(&s->model);
-        av_freep(&s->module);
-    }
-
-    av_freep(base);
-    return 0;
-}
-
-int ff_inference_base_submit_frame(InferenceBaseContext *base,
-                                   AVFrame *frame,
-                                   int input_idx,
-                                   int batch_idx)
-{
-    DNNIOData input = { };
-    fill_dnn_data_from_frame(&input, frame, batch_idx, 1, input_idx);
-    base->model->set_input(base->model->model, &input);
-#if CONFIG_VAAPI
-    if (base->vpp.va_vpp)
-        va_vpp_surface_release(base->vpp.va_vpp);
-#endif
-
-    return 0;
-}
-
-int ff_inference_base_infer(InferenceBaseContext *base)
-{
-    DNNReturnType dnn_ret;
-    dnn_ret = base->module->execute_model(base->model);
-    av_assert0(dnn_ret == DNN_SUCCESS);
-    return 0;
-}
-
-int ff_inference_base_filter_frame(InferenceBaseContext *base, AVFrame *in)
-{
-    DNNModelInfo *info = &base->input_info;
-    DNNReturnType dnn_ret;
-    DNNIOData input = { };
-
-    for (int i = 0; i < info->number; i++) {
-        AVFrame *processed_frame = NULL;
-        for (int j = 0; j < base->batch_size; j++) {
-            if (base->preprocess) {
-                if (base->preprocess(base, i, in, &processed_frame) < 0)
-                    return AVERROR(EINVAL);
-            }
-
-            if (!processed_frame) return -1;
-
-            fill_dnn_data_from_frame(&input, processed_frame, j, 1, i);
-            base->model->set_input(base->model->model, &input);
-#if CONFIG_VAAPI
-            if (base->vpp.va_vpp)
-                va_vpp_surface_release(base->vpp.va_vpp);
-#endif
-        }
-    }
-
-    dnn_ret = base->module->execute_model(base->model);
-    av_assert0(dnn_ret == DNN_SUCCESS);
-
-    return 0;
-}
-
-int ff_inference_base_get_infer_result(InferenceBaseContext *base,
-                                       int id,
-                                       InferTensorMeta *metadata)
-{
-    DNNModelInfo *info = &base->output_info;
-    DNNIOData     data = { };
-    DNNReturnType ret;
-
-    av_assert0(metadata != NULL);
-
-    av_assert0(id < DNN_INPUT_OUTPUT_NUM);
-
-    // TODO: change to layer name for multiple outputs
-    data.in_out_idx = id;
-
-    ret = base->model->get_execute_result(base->model->model, &data);
-    av_assert0(ret == DNN_SUCCESS);
-
-    metadata->dim_size  = 4;
-    memcpy(&metadata->dims[0], &info->dims[id][0],
-            metadata->dim_size * sizeof(metadata->dims[0]));
-
-    metadata->layout    = info->layout[id];
-    metadata->precision = info->precision[id];
-
-    metadata->data        = data.data[0];
-    metadata->total_bytes = data.size;
-
-    return 0;
-}
-
-DNNModelInfo* ff_inference_base_get_input_info(InferenceBaseContext *base)
-{
-    return &base->input_info;
-}
-
-DNNModelInfo* ff_inference_base_get_output_info(InferenceBaseContext *base)
-{
-    return &base->output_info;
-}
-
-VideoPP* ff_inference_base_get_vpp(InferenceBaseContext *base)
-{
-    return &base->vpp;
-}
-
-void ff_inference_dump_model_info(void *ctx, DNNModelInfo *info)
-{
-    int i;
-    for (i = 0; i < info->number; i++) {
-        size_t *p = &info->dims[i][0];
-        av_log(ctx, AV_LOG_DEBUG, "Info id:%d layer\"%-16s\" "
-               "batch size:%d - dim: %3lu %3lu %3lu %3lu - img:%d pre:%d layout:%d\n",
-               i, info->layer_name[i],
-               info->batch_size, p[0], p[1], p[2], p[3],
-               info->is_image[i], info->precision[i], info->layout[i]);
-    }
-}
-
-/*
- * VAAPI VPP APIs
- */
-
-#if CONFIG_VAAPI
-static int ff_vaapi_vpp_colour_standard(enum AVColorSpace av_cs)
-{
-    switch(av_cs) {
-#define CS(av, va) case AVCOL_SPC_ ## av: return VAProcColorStandard ## va;
-        CS(BT709,     BT709);
-        CS(BT470BG,   BT601);
-        // WORKAROUND: vaapi driver doesn't support all color space
-        CS(SMPTE170M, None); //SMPTE170M);
-        CS(SMPTE240M, None); //SMPTE240M);
-#undef CS
-    default:
-        return VAProcColorStandardNone;
-    }
-}
-
-int va_vpp_device_create(VAAPIVpp *va_vpp, AVFilterLink *inlink)
-{
-    AVFilterContext *avctx = inlink->dst;
-    VADisplay display = NULL;
-    VAImageFormat *image_list = NULL;
-    VAStatus vas;
-    int err, image_count;
-    AVBufferRef *device_ref = NULL;
-    AVHWFramesContext *hw_frames_ctx;
-
-    hw_frames_ctx = (AVHWFramesContext *)inlink->hw_frames_ctx->data;
-    av_assert0(hw_frames_ctx);
-
-    device_ref = av_buffer_ref(hw_frames_ctx->device_ref);
-    if (!device_ref) {
-        av_log(avctx, AV_LOG_ERROR, "A device reference create failed.\n");
-        return AVERROR(ENOMEM);
-    }
-
-    va_vpp->hwctx         = ((AVHWDeviceContext *)device_ref->data)->hwctx;
-    va_vpp->hw_frames_ref = inlink->hw_frames_ctx;
-
-    av_buffer_unref(&device_ref);
-
-    display = va_vpp->hwctx->display;
-
-    image_count = vaMaxNumImageFormats(display);
-    if (image_count <= 0) {
-        err = AVERROR(EIO);
-        goto fail;
-    }
-    image_list = av_malloc(image_count * sizeof(*image_list));
-    if (!image_list) {
-        err = AVERROR(ENOMEM);
-        goto fail;
-    }
-    vas = vaQueryImageFormats(display, image_list, &image_count);
-    if (vas != VA_STATUS_SUCCESS) {
-        err = AVERROR(EIO);
-        goto fail;
-    }
-
-    va_vpp->format_list = image_list;
-    va_vpp->nb_formats  = image_count;
-    va_vpp->va_config   = VA_INVALID_ID;
-    va_vpp->va_context  = VA_INVALID_ID;
-    va_vpp->va_surface  = VA_INVALID_ID;
-
-    va_vpp->scale          = &va_vpp_scale;
-    va_vpp->crop_and_scale = &va_vpp_crop_and_scale;
-
-    return VA_STATUS_SUCCESS;
-fail:
-    if (image_list)
-        av_free(image_list);
-    return err;
-}
-
-int va_vpp_device_free(VAAPIVpp *va_vpp)
-{
-    VAStatus vas;
-
-    if (!va_vpp)
-        return 0;
-
-    if (va_vpp->va_surface != VA_INVALID_ID) {
-        vas = vaDestroySurfaces(va_vpp->hwctx->display, &va_vpp->va_surface, 1);
-        if (vas != VA_STATUS_SUCCESS) {
-            av_log(NULL, AV_LOG_ERROR, "Failed to destroy surface %#x: "
-                    "%d (%s).\n", va_vpp->va_surface, vas, vaErrorStr(vas));
-        }
-    }
-
-    if (va_vpp->va_context != VA_INVALID_ID) {
-        vaDestroyContext(va_vpp->hwctx->display, va_vpp->va_context);
-        va_vpp->va_context = VA_INVALID_ID;
-    }
-
-    if (va_vpp->va_config != VA_INVALID_ID) {
-        vaDestroyConfig(va_vpp->hwctx->display, va_vpp->va_config);
-        va_vpp->va_config = VA_INVALID_ID;
-    }
-
-    av_free(va_vpp->format_list);
-
-    return VA_STATUS_SUCCESS;
-}
-
-int va_vpp_surface_alloc(VAAPIVpp *va_vpp, size_t width, size_t height, const char *format)
-{
-    int i, rt_format, fourcc;
-    VASurfaceAttrib surface_attrib;
-    enum AVPixelFormat av_format;
-
-    if (!va_vpp)
-        return -1;
-
-    if (format == NULL || strstr(format, "bgrx")) {
-        fourcc = VA_FOURCC_BGRX; rt_format = VA_RT_FORMAT_RGB32; av_format = AV_PIX_FMT_BGR0;
-    } else if (strstr(format, "rgbp")) {
-        fourcc = VA_FOURCC_RGBP; rt_format = VA_RT_FORMAT_RGBP;  av_format = AV_PIX_FMT_RGBP;
-    } else
-        return -1;
-
-    surface_attrib.type          = VASurfaceAttribPixelFormat;
-    surface_attrib.flags         = VA_SURFACE_ATTRIB_SETTABLE;
-    surface_attrib.value.type    = VAGenericValueTypeInteger;
-    surface_attrib.value.value.i = fourcc;
-
-    VA_CALL(vaCreateConfig(va_vpp->hwctx->display, VAProfileNone,
-                           VAEntrypointVideoProc, 0, 0, &va_vpp->va_config));
-
-    VA_CALL(vaCreateSurfaces(va_vpp->hwctx->display, rt_format, width, height,
-                             &va_vpp->va_surface, 1, &surface_attrib, 1));
-
-    VA_CALL(vaCreateContext(va_vpp->hwctx->display, va_vpp->va_config,
-                            width, height, VA_PROGRESSIVE,
-                            &va_vpp->va_surface, 1, &va_vpp->va_context));
-
-    for (i = 0; i < va_vpp->nb_formats; i++) {
-        if (va_vpp->format_list[i].fourcc == fourcc) {
-            va_vpp->va_format_selected = va_vpp->format_list[i];
-            break;
-        }
-    }
-
-    va_vpp->av_format = av_format;
-
-    return VA_STATUS_SUCCESS;
-}
-
-/* release mapped memory */
-int va_vpp_surface_release(VAAPIVpp *va_vpp)
-{
-    VA_CALL(vaUnmapBuffer(va_vpp->hwctx->display, va_vpp->va_image.buf));
-    VA_CALL(vaDestroyImage(va_vpp->hwctx->display, va_vpp->va_image.image_id));
-
-    return VA_STATUS_SUCCESS;
-}
-
-/* HW scale and map to system memory */
-static int va_vpp_scale(VAAPIVpp *va_vpp, AVFrame *input,
-                        int scale_w,      int scale_h,
-                        uint8_t *data[],  int stride[])
-{
-    return va_vpp_crop_and_scale(va_vpp, input, NULL, scale_w, scale_h, data, stride);
-}
-
-static int va_vpp_crop_and_scale(VAAPIVpp *va_vpp,
-                                 AVFrame *input,   Rect *crop_rect,
-                                 int scale_w,      int scale_h,
-                                 uint8_t *data[],  int stride[])
-{
-    int i;
-    void *address = NULL;
-    VAImage   *va_image_ptr;
-    VABufferID params_id;
-    VASurfaceID input_surface, output_surface;
-    VAProcPipelineParameterBuffer params;
-    VARectangle input_region;
-
-    input_surface = (VASurfaceID)(uintptr_t)input->data[3];
-    av_log(NULL, AV_LOG_DEBUG, "Using surface %#x for scale input.\n",
-           input_surface);
-
-    output_surface = va_vpp->va_surface;
-
-    if (crop_rect == NULL) {
-        input_region = (VARectangle) {
-            .x      = input->crop_left,
-            .y      = input->crop_top,
-            .width  = input->width -
-                     (input->crop_left + input->crop_right),
-            .height = input->height -
-                     (input->crop_top + input->crop_bottom),
-        };
-    } else {
-        int _x = lrintf(crop_rect->x0);
-        int _y = lrintf(crop_rect->y0);
-        _x = FFMAX(_x, 0);
-        _y = FFMAX(_y, 0);
-        if (_x >= input->width  || _y >= input->height) {
-            av_log(NULL, AV_LOG_ERROR, "Incorrect crop rect!\n");
-            return AVERROR(EINVAL);
-        }
-        input_region = (VARectangle) {
-            .x      = _x,
-            .y      = _y,
-            .width  = FFMIN(lrintf(crop_rect->x1) - _x, input->width - _x),
-            .height = FFMIN(lrintf(crop_rect->y1) - _y, input->height - _y),
-        };
-    }
-
-    memset(&params, 0, sizeof(params));
-
-    params.surface = input_surface;
-    params.surface_region = &input_region;
-    params.surface_color_standard =
-        ff_vaapi_vpp_colour_standard(input->colorspace);
-
-    params.output_region = 0;
-    params.output_background_color = 0xff000000;
-    params.output_color_standard = params.surface_color_standard;
-
-    params.pipeline_flags = 0;
-    params.filter_flags = VA_FILTER_SCALING_HQ;
-
-    VA_CALL(vaBeginPicture(va_vpp->hwctx->display, va_vpp->va_context, output_surface));
-
-    VA_CALL(vaCreateBuffer(va_vpp->hwctx->display, va_vpp->va_context,
-                           VAProcPipelineParameterBufferType,
-                           sizeof(params), 1, &params, &params_id));
-
-    VA_CALL(vaRenderPicture(va_vpp->hwctx->display, va_vpp->va_context,
-                            &params_id, 1));
-
-    VA_CALL(vaEndPicture(va_vpp->hwctx->display, va_vpp->va_context));
-
-    VA_CALL(vaDestroyBuffer(va_vpp->hwctx->display, params_id));
-
-    VA_CALL(vaSyncSurface(va_vpp->hwctx->display, output_surface));
-
-    // map surface to system memory
-    va_image_ptr = &va_vpp->va_image;
-
-    VA_CALL(vaCreateImage(va_vpp->hwctx->display, &va_vpp->va_format_selected,
-                          scale_w, scale_h, va_image_ptr));
-
-    VA_CALL(vaGetImage(va_vpp->hwctx->display, output_surface,
-                       0, 0, scale_w, scale_h,
-                       va_image_ptr->image_id));
-
-    VA_CALL(vaMapBuffer(va_vpp->hwctx->display, va_image_ptr->buf, &address));
-
-    for (i = 0; i < va_image_ptr->num_planes; i++) {
-        data[i]   = (uint8_t *)address + va_image_ptr->offsets[i];
-        stride[i] = va_image_ptr->pitches[i];
-    }
-
-    return VA_STATUS_SUCCESS;
-}
-#endif
-
-#if CONFIG_LIBJSON_C
-/*
- * model proc parsing functions using JSON-c
- */
-void *ff_read_model_proc(const char *path)
-{
-    int n, file_size;
-    json_object *proc_config = NULL;
-    uint8_t *proc_json = NULL;
-    json_tokener *tok = NULL;
-
-    FILE *fp = fopen(path, "rb");
-    if (!fp) {
-        fprintf(stderr, "File open error:%s\n", path);
-        return NULL;
-    }
-
-    file_size = ff_get_file_size(fp);
-
-    proc_json = av_mallocz(file_size + 1);
-    if (!proc_json)
-        goto end;
-
-    n = fread(proc_json, file_size, 1, fp);
-
-    UNUSED(n);
-
-    tok = json_tokener_new();
-    proc_config = json_tokener_parse_ex(tok, proc_json, file_size);
-    if (proc_config == NULL) {
-        enum json_tokener_error jerr;
-        jerr = json_tokener_get_error(tok);
-        fprintf(stderr, "Error before: %s\n", json_tokener_error_desc(jerr));
-        goto end;
-    }
-
-end:
-    if (proc_json)
-        av_freep(&proc_json);
-    if(tok)
-        json_tokener_free(tok);
-    fclose(fp);
-    return proc_config;
-}
-
-void ff_load_default_model_proc(ModelInputPreproc *preproc, ModelOutputPostproc *postproc)
-{
-    if (preproc) {
-        /*
-         * format is a little tricky, an ideal input format for IE is BGR planer
-         * however, neither soft csc nor hardware vpp could support that format.
-         * Here, we set a close soft format. The actual one coverted before sent
-         * to IE will be decided by user config and hardware vpp used or not.
-         */
-        preproc->color_format = AV_PIX_FMT_BGR24;
-        preproc->layer_name   = NULL;
-    }
-
-    if (postproc) {
-        // do nothing
-    }
-}
-
-int ff_parse_input_preproc(const void *json, ModelInputPreproc *m_preproc)
-{
-    json_object *jvalue, *preproc, *color, *layer, *object_class;
-    int ret;
-
-    ret = json_object_object_get_ex((json_object *)json, "input_preproc", &preproc);
-    if (!ret) {
-        av_log(NULL, AV_LOG_DEBUG, "No input_preproc.\n");
-        return 0;
-    }
-
-    // not support multiple inputs yet
-    av_assert0(json_object_array_length(preproc) <= 1);
-
-    jvalue = json_object_array_get_idx(preproc, 0);
-
-    ret = json_object_object_get_ex(jvalue, "color_format", &color);
-    if (ret) {
-        if (json_object_get_string(color) == NULL)
-            return -1;
-
-        av_log(NULL, AV_LOG_INFO, "Color Format:\"%s\"\n", json_object_get_string(color));
-
-        if (!strcmp(json_object_get_string(color), "BGR"))
-            m_preproc->color_format = AV_PIX_FMT_BGR24;
-        else if (!strcmp(json_object_get_string(color), "RGB"))
-            m_preproc->color_format = AV_PIX_FMT_RGB24;
-        else
-            return -1;
-    }
-
-    ret = json_object_object_get_ex(jvalue, "object_class", &object_class);
-    if (ret) {
-        if (json_object_get_string(object_class) == NULL)
-            return -1;
-
-        av_log(NULL, AV_LOG_INFO, "Object_class:\"%s\"\n", json_object_get_string(object_class));
-
-        m_preproc->object_class = (char *)json_object_get_string(object_class);
-    }
-
-    ret = json_object_object_get_ex(jvalue, "layer_name", &layer);
-    UNUSED(layer);
-
-    return 0;
-}
-
-// For detection, we now care labels only.
-// Layer name and type can be got from output blob.
-int ff_parse_output_postproc(const void *json, ModelOutputPostproc *m_postproc)
-{
-    json_object *jvalue, *postproc;
-    json_object *attribute, *converter, *labels, *layer, *method, *threshold;
-    json_object *tensor2text_scale, *tensor2text_precision;
-    int ret;
-    size_t jarraylen;
-
-    ret = json_object_object_get_ex((json_object *)json, "output_postproc", &postproc);
-    if (!ret) {
-        av_log(NULL, AV_LOG_DEBUG, "No output_postproc.\n");
-        return 0;
-    }
-
-    jarraylen = json_object_array_length(postproc);
-    av_assert0(jarraylen <= MAX_MODEL_OUTPUT);
-
-    for(int i = 0; i < jarraylen; i++){
-        jvalue = json_object_array_get_idx(postproc, i);
-        OutputPostproc *proc = &m_postproc->procs[i];
-
-#define FETCH_STRING(var, name)                                           \
-        do { ret = json_object_object_get_ex(jvalue, #name, &var);        \
-            if (ret) proc->name = (char *)json_object_get_string(var);    \
-        } while(0)
-#define FETCH_DOUBLE(var, name)                                           \
-        do { ret = json_object_object_get_ex(jvalue, #name, &var);        \
-            if (ret) proc->name = (double)json_object_get_double(var);    \
-        } while(0)
-#define FETCH_INTEGER(var, name)                                          \
-        do { ret = json_object_object_get_ex(jvalue, #name, &var);        \
-            if (ret) proc->name = (int)json_object_get_int(var);          \
-        } while(0)
-
-        FETCH_STRING(layer, layer_name);
-        FETCH_STRING(method, method);
-        FETCH_STRING(attribute, attribute_name);
-        FETCH_STRING(converter, converter);
-
-        FETCH_DOUBLE(threshold, threshold);
-        FETCH_DOUBLE(tensor2text_scale, tensor2text_scale);
-
-        FETCH_INTEGER(tensor2text_precision, tensor2text_precision);
-
-        // handle labels
-        ret = json_object_object_get_ex(jvalue, "labels", &labels);
-        if (ret) {
-            json_object *label;
-            size_t labels_num = json_object_array_length(labels);
-
-            if (labels_num > 0) {
-                AVBufferRef *ref    = NULL;
-                LabelsArray *larray = av_mallocz(sizeof(*larray));
-
-                if (!larray)
-                    return AVERROR(ENOMEM);
-
-                for(int i = 0; i < labels_num; i++){
-                    label = json_object_array_get_idx(labels, i);
-                    char *l = av_strdup(json_object_get_string(label));
-                    av_dynarray_add(&larray->label, &larray->num, l);
-                }
-
-                ref = av_buffer_create((uint8_t *)larray, sizeof(*larray),
-                        &infer_labels_buffer_free, NULL, 0);
-
-                proc->labels = ref;
-                 
-                if(ref)
-                    infer_labels_dump(ref->data);
-            }
-        }
-    }
-
-#undef FETCH_STRING
-#undef FETCH_DOUBLE
-#undef FETCH_INTEGER
-
-    return 0;
-}
-
-void ff_release_model_proc(const void *json,
-        ModelInputPreproc *preproc, ModelOutputPostproc *postproc)
-{
-    size_t index = 0;
-
-    if (!json) return;
-
-    if (postproc) {
-        for (index = 0; index < MAX_MODEL_OUTPUT; index++) {
-            if (postproc->procs[index].labels)
-                av_buffer_unref(&postproc->procs[index].labels);
-        }
-    }
-
-    json_object_put((json_object *)json);
-}
-#endif
diff --git a/libavfilter/inference.h b/libavfilter/inference.h
deleted file mode 100644
index 9f70e38..0000000
--- a/libavfilter/inference.h
+++ /dev/null
@@ -1,266 +0,0 @@
-/*
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2.1 of the License, or (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-
-#ifndef AVFILTER_INFERENCE_H
-#define AVFILTER_INFERENCE_H
-
-#if CONFIG_VAAPI
-#include <va/va.h>
-#endif
-
-#include "libavutil/common.h"
-#include "libswscale/swscale.h"
-#include "libavutil/hwcontext.h"
-#if CONFIG_VAAPI
-#include "libavutil/hwcontext_vaapi.h"
-#endif
-
-#include "dnn_interface.h"
-
-typedef struct _InferenceBaseContext InferenceBaseContext;
-typedef struct _InputPreproc         ModelInputPreproc;
-typedef struct _OutputPostproc       OutputPostproc;
-typedef struct _ModelOutputPostproc  ModelOutputPostproc;
-
-typedef int (*InferencePreProcess)(InferenceBaseContext *base, int index, AVFrame *in, AVFrame **out);
-
-#define UNUSED(x) (void)(x)
-
-typedef struct InferenceParam {
-    char  *model_file;
-    char  *labels_file;
-    int    backend_type;
-    int    device_type;
-    char  *cpu_extension;
-    char  *gpu_extension;
-
-    int    batch_size;
-
-    // TODO: inputs attributes are different
-    int    input_layout;
-    int    input_precision;
-    int    input_is_image; //!< image or data
-
-    InferencePreProcess preprocess;
-} InferenceParam;
-
-#define MAX_VPP_NUM DNN_INPUT_OUTPUT_NUM
-
-/*
- * Vpp device type detected according to frame format
- */
-typedef enum { VPP_DEVICE_HW, VPP_DEVICE_SW } VPPDevice;
-
-typedef struct _SwVpp    SwVpp;
-
-typedef struct _VAAPIVpp VAAPIVpp;
-
-/*
- * Generic rectangle structure consists of two diagonal points
- */
-typedef struct Rect {
-    float x0; float y0; float x1; float y1;
-} Rect;
-
-#if CONFIG_VAAPI
-struct _VAAPIVpp {
-    AVVAAPIDeviceContext *hwctx;
-    AVBufferRef  *hw_frames_ref;
-    VASurfaceID   va_surface;
-    VAConfigID    va_config;
-    VAContextID   va_context;
-
-    VAImageFormat *format_list; //!< Surface formats which can be used with this device.
-    int            nb_formats;
-
-    VAImage            va_image;
-    VAImageFormat      va_format_selected;
-    enum AVPixelFormat av_format;
-
-    int  (*scale)(VAAPIVpp *va_vpp, AVFrame *input,
-                  int scale_w,      int scale_h,
-                  uint8_t *data[],  int stride[]);
-
-    int  (*crop_and_scale)(VAAPIVpp *va_vpp, AVFrame *input,
-                           Rect *crop_rect,
-                           int scale_w, int scale_h,
-                           uint8_t *data[],  int stride[]);
-};
-#endif
-
-struct _SwVpp {
-    struct SwsContext *scale_contexts[MAX_VPP_NUM];
-
-    int  (*scale)(struct SwsContext *context,
-                  const uint8_t * const srcSlice[],
-                  const int srcStride[], int srcSliceY,
-                  int srcSliceH, uint8_t *const dst[],
-                  const int dstStride[]);
-
-    int  (*crop_and_scale)(AVFrame *frame, Rect *crop_rect,
-                           int   scale_w,  int   scale_h,
-                           enum AVPixelFormat scale_format,
-                           uint8_t *dst[], int   dstStride[]);
-};
-
-typedef struct VideoPP {
-    int       device;
-    int       expect_format;
-    AVFrame  *frames[MAX_VPP_NUM];  ///<! frames to save vpp output
-    SwVpp    *sw_vpp;
-#if CONFIG_VAAPI
-    VAAPIVpp *va_vpp;
-#endif
-} VideoPP;
-
-struct _InputPreproc {
-    int   color_format;     ///<! input data format
-    char *layer_name;       ///<! layer name of input
-    char *object_class;     ///<! interested object class
-};
-
-struct _OutputPostproc {
-    char  *layer_name;
-    char  *converter;
-    char  *attribute_name;
-    char  *method;
-    double threshold;
-    double tensor2text_scale;
-    int    tensor2text_precision;
-    AVBufferRef *labels;
-};
-
-#define MAX_MODEL_OUTPUT 4
-struct _ModelOutputPostproc {
-    OutputPostproc procs[MAX_MODEL_OUTPUT];
-};
-
-#define MAX_TENSOR_DIM_NUM 4
-typedef struct InferTensorMeta {
-    size_t  dim_size;
-    size_t  dims[MAX_TENSOR_DIM_NUM];
-    int     layout;
-    int     precision;
-    char   *layer_name;
-    char   *model_name;
-    void   *data;
-    size_t  total_bytes;
-    // AVBufferRef *labels;
-} InferTensorMeta;
-
-typedef struct InferDetection {
-    float   x_min;
-    float   y_min;
-    float   x_max;
-    float   y_max;
-    float   confidence;
-    int     label_id;
-    int     object_id;
-    AVBufferRef *label_buf;
-} InferDetection;
-
-/* dynamic bounding boxes array */
-typedef struct BBoxesArray {
-    InferDetection **bbox;
-    int              num;
-} BBoxesArray;
-
-/* dynamic labels array */
-typedef struct LabelsArray {
-    char **label;
-    int    num;
-} LabelsArray;
-
-typedef struct InferDetectionMeta {
-    BBoxesArray *bboxes;
-} InferDetectionMeta;
-
-typedef struct InferClassification {
-    int     detect_id;        ///< detected bbox index
-    char   *name;             ///< class name, e.g. emotion, age
-    char   *layer_name;       ///< output layer name
-    char   *model;            ///< model name
-    int     label_id;         ///< label index in labels
-    float   confidence;
-    float   value;
-    AVBufferRef *label_buf;   ///< label buffer
-    AVBufferRef *tensor_buf;  ///< output tensor buffer
-} InferClassification;
-
-/* dynamic classifications array */
-typedef struct ClassifyArray {
-    InferClassification **classifications;
-    int                   num;
-} ClassifyArray;
-
-typedef struct InferClassificationMeta {
-    ClassifyArray *c_array;
-} InferClassificationMeta;
-
-/* split strings by delimiter */
-void av_split(char *str, const char *delim, char **array, int *num, int max);
-
-/* 2-dimensional norm */
-double av_norm(float vec[], size_t num);
-
-/* Dot Product */
-double av_dot(float vec1[], float vec2[], size_t num);
-
-int ff_inference_base_create(AVFilterContext *avctx, InferenceBaseContext **base, InferenceParam *p);
-
-int ff_inference_base_free(InferenceBaseContext **base);
-
-int ff_inference_base_submit_frame(InferenceBaseContext *base, AVFrame *frame, int input_idx, int batch_idx);
-
-int ff_inference_base_infer(InferenceBaseContext *base);
-
-int ff_inference_base_filter_frame(InferenceBaseContext *base, AVFrame *in);
-
-int ff_inference_base_get_infer_result(InferenceBaseContext *base, int index, InferTensorMeta *metadata);
-
-DNNModelInfo* ff_inference_base_get_input_info(InferenceBaseContext *base);
-DNNModelInfo* ff_inference_base_get_output_info(InferenceBaseContext *base);
-VideoPP*      ff_inference_base_get_vpp(InferenceBaseContext *base);
-
-void ff_inference_dump_model_info(void *ctx, DNNModelInfo *info);
-
-#if CONFIG_VAAPI
-int va_vpp_device_create(VAAPIVpp *ctx, AVFilterLink *inlink);
-
-int va_vpp_device_free(VAAPIVpp *ctx);
-
-int va_vpp_surface_alloc(VAAPIVpp *ctx, size_t width, size_t height, const char *format);
-
-int va_vpp_surface_release(VAAPIVpp *ctx);
-#endif
-
-int ff_get_file_size(FILE *fp);
-
-#if CONFIG_LIBJSON_C
-void *ff_read_model_proc(const char *path);
-
-void ff_load_default_model_proc(ModelInputPreproc *preproc, ModelOutputPostproc *postproc);
-
-int ff_parse_input_preproc(const void *json, ModelInputPreproc *m_preproc);
-
-int ff_parse_output_postproc(const void *json, ModelOutputPostproc *m_postproc);
-
-void ff_release_model_proc(const void *json, ModelInputPreproc *preproc, ModelOutputPostproc *postproc);
-#endif
-
-#endif
diff --git a/libavfilter/inference_backend/.clang-format b/libavfilter/inference_backend/.clang-format
new file mode 100644
index 0000000..8e7f7ae
--- /dev/null
+++ b/libavfilter/inference_backend/.clang-format
@@ -0,0 +1,10 @@
+---
+# Language:        Cpp
+BasedOnStyle:  LLVM
+ColumnLimit:     120
+IndentWidth:     4
+AllowShortFunctionsOnASingleLine: None
+AllowShortBlocksOnASingleLine: false
+AlwaysBreakTemplateDeclarations: Yes
+CompactNamespaces: false
+...
diff --git a/libavfilter/inference_backend/ff_base_inference.c b/libavfilter/inference_backend/ff_base_inference.c
new file mode 100644
index 0000000..89ae7b4
--- /dev/null
+++ b/libavfilter/inference_backend/ff_base_inference.c
@@ -0,0 +1,130 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "ff_base_inference.h"
+#include "ff_inference_impl.h"
+#include "ff_proc_factory.h"
+#include "logger.h"
+#include <libavutil/avassert.h>
+#include <libavutil/mem.h>
+
+static const int log_levels[] = {AV_LOG_QUIET,   AV_LOG_ERROR, AV_LOG_WARNING, AV_LOG_INFO,
+                                 AV_LOG_VERBOSE, AV_LOG_DEBUG, AV_LOG_TRACE,   AV_LOG_PANIC};
+
+static void ff_log_function(int level, const char *file, const char *function, int line, const char *message) {
+    av_log(NULL, log_levels[level], "%s:%i : %s \t %s \n", file, line, function, message);
+}
+
+static void ff_trace_function(int level, const char *fmt, va_list vl) {
+    av_vlog(NULL, log_levels[level], fmt, vl);
+}
+
+FFBaseInference *av_base_inference_create(const char *inference_id) {
+    FFBaseInference *base_inference = (FFBaseInference *)av_mallocz(sizeof(*base_inference));
+    if (base_inference == NULL)
+        return NULL;
+
+    set_log_function(ff_log_function);
+    set_trace_function(ff_trace_function);
+
+    base_inference->inference_id = inference_id ? av_strdup(inference_id) : NULL;
+
+    base_inference->param.is_full_frame = TRUE; // default to true
+    base_inference->param.every_nth_frame = 1;  // default to 1
+    base_inference->param.nireq = 1;
+    base_inference->param.batch_size = 1;
+    base_inference->param.threshold = 0.5;
+    base_inference->param.vpp_device = VPP_DEVICE_SW;
+    base_inference->param.opaque = NULL;
+
+    base_inference->num_skipped_frames = UINT_MAX - 1; // always run inference on first frame
+
+    return base_inference;
+}
+
+int av_base_inference_set_params(FFBaseInference *base, FFInferenceParam *param) {
+    if (!base || !param)
+        return AVERROR(EINVAL);
+
+    if (base->initialized)
+        return 0;
+
+    base->param = *param;
+    base->inference = (void *)FFInferenceImplCreate(base);
+    base->initialized = TRUE;
+    base->post_proc = (void *)getPostProcFunctionByName(base->inference_id, base->param.model);
+    base->crop_full_frame =
+        (!param->crop_rect.x && !param->crop_rect.y && !param->crop_rect.width && !param->crop_rect.height) ? FALSE
+                                                                                                            : TRUE;
+
+    return 0;
+}
+
+void av_base_inference_release(FFBaseInference *base) {
+    if (!base)
+        return;
+
+    if (base->inference) {
+        FFInferenceImplRelease((FFInferenceImpl *)base->inference);
+        base->inference = NULL;
+    }
+
+    if (base->inference_id) {
+        av_free((void *)base->inference_id);
+        base->inference_id = NULL;
+    }
+
+    av_free(base);
+}
+
+int av_base_inference_send_frame(void *ctx, FFBaseInference *base, AVFrame *frame_in) {
+    if (!base || !frame_in)
+        return AVERROR(EINVAL);
+
+    return FFInferenceImplAddFrame(ctx, (FFInferenceImpl *)base->inference, frame_in);
+}
+
+int av_base_inference_get_frame(void *ctx, FFBaseInference *base, AVFrame **frame_out) {
+    if (!base || !frame_out)
+        return AVERROR(EINVAL);
+
+    return FFInferenceImplGetFrame(ctx, (FFInferenceImpl *)base->inference, frame_out);
+}
+
+int av_base_inference_frame_queue_empty(void *ctx, FFBaseInference *base) {
+    if (!base)
+        return AVERROR(EINVAL);
+
+    return FFInferenceImplGetQueueSize(ctx, (FFInferenceImpl *)base->inference) == 0 ? TRUE : FALSE;
+}
+
+int av_base_inference_resource_status(void *ctx, FFBaseInference *base) {
+    if (!base)
+        return AVERROR(EINVAL);
+
+    return FFInferenceImplResourceStatus(ctx, (FFInferenceImpl *)base->inference);
+}
+
+void av_base_inference_send_event(void *ctx, FFBaseInference *base, FF_INFERENCE_EVENT event) {
+    if (!base)
+        return;
+
+    FFInferenceImplSinkEvent(ctx, (FFInferenceImpl *)base->inference, event);
+}
diff --git a/libavfilter/inference_backend/ff_base_inference.h b/libavfilter/inference_backend/ff_base_inference.h
new file mode 100644
index 0000000..370f59c
--- /dev/null
+++ b/libavfilter/inference_backend/ff_base_inference.h
@@ -0,0 +1,238 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "image_inference.h"
+#include <libavutil/frame.h>
+#include <stdint.h>
+
+#if CONFIG_VAAPI
+#include <libavutil/hwcontext_vaapi.h>
+#endif
+
+typedef enum {
+    INFERENCE_EVENT_NONE,
+    INFERENCE_EVENT_EOS,
+} FF_INFERENCE_EVENT;
+
+typedef enum { VPP_DEVICE_SW, VPP_DEVICE_HW } VPPDevice;
+
+#ifndef TRUE
+/** The TRUE value of a UBool @stable ICU 2.0 */
+#define TRUE 1
+#endif
+
+#ifndef FALSE
+/** The FALSE value of a UBool @stable ICU 2.0 */
+#define FALSE 0
+#endif
+
+#define MOCKER_PRE_PROC_MAGIC 0x47474747
+
+typedef struct __FFBaseInference FFBaseInference;
+typedef struct __FFInferenceParam FFInferenceParam;
+typedef struct __ProcessedFrame ProcessedFrame;
+typedef struct __InputPreproc InputPreproc;
+typedef struct __OutputPostproc OutputPostproc;
+typedef struct __InputPreproc ModelInputPreproc;
+typedef struct __ModelOutputPostproc ModelOutputPostproc;
+
+#define FF_INFERENCE_OPTIONS                                                                                           \
+    char *model;                                                                                                       \
+    char *object_class;                                                                                                \
+    char *model_proc;                                                                                                  \
+    char *device;                                                                                                      \
+    int batch_size;                                                                                                    \
+    int every_nth_frame;                                                                                               \
+    int nireq;                                                                                                         \
+    char *cpu_streams;                                                                                                 \
+    char *infer_config;                                                                                                \
+    float threshold;                                                                                                   \
+    int realtime_qos;
+
+struct __FFInferenceParam {
+    // exposed options
+    FF_INFERENCE_OPTIONS
+
+    VPPDevice vpp_device; // VPPDevice default:SW
+    void *opaque;         // VADisplay for vaapi
+
+    Rectangle crop_rect;
+    int is_full_frame;
+};
+
+struct __FFBaseInference {
+    // unique infer string id
+    const char *inference_id;
+
+    FFInferenceParam param;
+
+    // other fields
+    int initialized;
+    int crop_full_frame;    // crop needed for full frame
+    void *inference;        // type: FFInferenceImpl*
+    void *pre_proc;         // type: PreProcFunction
+    void *post_proc;        // type: PostProcFunction
+    void *get_roi_pre_proc; // type: GetROIPreProcFunction
+
+    unsigned int num_skipped_frames;
+};
+
+/* ROI for analytics */
+typedef struct __FFVideoRegionOfInterestMeta {
+    char type_name[16]; ///<! type name, e.g. face, vechicle etc.
+    unsigned int index; ///<! mark as the serial no. in side data
+
+    unsigned int x;
+    unsigned int y;
+    unsigned int w;
+    unsigned int h;
+} FFVideoRegionOfInterestMeta;
+
+/* model preproc */
+struct __InputPreproc {
+    int color_format;   ///<! input data format
+    char *layer_name;   ///<! layer name of input
+    char *object_class; ///<! interested object class
+};
+
+/* model postproc */
+struct __OutputPostproc {
+    char *layer_name;
+    char *converter;
+    char *attribute_name;
+    char *method;
+    double threshold;
+    double tensor_to_text_scale;
+    int tensor_to_text_precision;
+    AVBufferRef *labels;
+};
+
+typedef enum {
+    ANY = 0,
+    NCHW = 1,
+    NHWC = 2,
+} IELayout;
+
+typedef enum {
+    FP32 = 10,
+    U8 = 40,
+} IEPrecision;
+
+#define FF_TENSOR_MAX_RANK 8
+typedef struct _IETensorMeta {
+    IEPrecision precision;           /**< tensor precision */
+    int ranks;                       /**< tensor rank */
+    size_t dims[FF_TENSOR_MAX_RANK]; /**< array describing tensor's dimensions */
+    IELayout layout;                 /**< tensor layout */
+    char *layer_name;                /**< tensor output layer name */
+    char *model_name;                /**< model name */
+    void *data;                      /**< tensor data */
+    size_t total_bytes;              /**< tensor size in bytes */
+    const char *element_id;          /**< id of pipeline element that produced current tensor */
+} IETensorMeta;
+
+/* dynamic labels array */
+typedef struct _LabelsArray {
+    char **label;
+    int num;
+} LabelsArray;
+
+typedef struct _InferDetection {
+    float x_min;
+    float y_min;
+    float x_max;
+    float y_max;
+    float confidence;
+    int label_id;
+    int object_id;
+    AVBufferRef *label_buf;
+} InferDetection;
+
+/* dynamic bounding boxes array */
+typedef struct _BBoxesArray {
+    InferDetection **bbox;
+    int num;
+} BBoxesArray;
+
+typedef struct _InferDetectionMeta {
+    BBoxesArray *bboxes;
+} InferDetectionMeta;
+
+typedef struct __InferenceROI {
+    AVFrame *frame;
+    FFVideoRegionOfInterestMeta roi;
+} InferenceROI;
+
+typedef struct __InferenceROIArray {
+    InferenceROI **infer_ROIs;
+    int num_infer_ROIs;
+} InferenceROIArray;
+
+typedef struct InferClassification {
+    int detect_id;    ///< detected bbox index
+    char *name;       ///< class name, e.g. emotion, age
+    char *layer_name; ///< output layer name
+    char *model;      ///< model name
+    char *attributes;
+    int label_id;     ///< label index in labels
+    float confidence;
+    float value;
+    AVBufferRef *label_buf;  ///< label buffer
+    AVBufferRef *tensor_buf; ///< output tensor buffer
+} InferClassification;
+
+/* dynamic classifications array */
+typedef struct ClassifyArray {
+    InferClassification **classifications;
+    int num;
+} ClassifyArray;
+
+typedef struct InferClassificationMeta {
+    ClassifyArray *c_array;
+} InferClassificationMeta;
+
+#define MAX_MODEL_OUTPUT 4
+struct __ModelOutputPostproc {
+    OutputPostproc procs[MAX_MODEL_OUTPUT];
+};
+
+typedef void (*PostProcFunction)(const OutputBlobArray *output_blobs, InferenceROIArray *infer_roi_array,
+                                 ModelOutputPostproc *model_postproc, const char *model_name,
+                                 const FFBaseInference *ff_base_inference);
+
+FFBaseInference *av_base_inference_create(const char *inference_id);
+
+int av_base_inference_set_params(FFBaseInference *base, FFInferenceParam *param);
+
+// TODO: add interface to set options separately
+
+void av_base_inference_release(FFBaseInference *base);
+
+int av_base_inference_send_frame(void *ctx, FFBaseInference *base, AVFrame *frame);
+
+int av_base_inference_get_frame(void *ctx, FFBaseInference *base, AVFrame **frame_out);
+
+int av_base_inference_frame_queue_empty(void *ctx, FFBaseInference *base);
+
+int av_base_inference_resource_status(void *ctx, FFBaseInference *base);
+
+void av_base_inference_send_event(void *ctx, FFBaseInference *base, FF_INFERENCE_EVENT event);
diff --git a/libavfilter/inference_backend/ff_inference_impl.c b/libavfilter/inference_backend/ff_inference_impl.c
new file mode 100644
index 0000000..92ceb0c
--- /dev/null
+++ b/libavfilter/inference_backend/ff_inference_impl.c
@@ -0,0 +1,559 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "ff_inference_impl.h"
+#include "ff_base_inference.h"
+#include "ff_list.h"
+#include "image_inference.h"
+#include "logger.h"
+#include "model_proc.h"
+#include <libavutil/avassert.h>
+#include <libavutil/log.h>
+#include <pthread.h>
+
+typedef enum {
+    INFERENCE_EXECUTED = 1,
+    INFERENCE_SKIPPED_PER_PROPERTY = 2, // frame skipped due to every-nth-frame set to value greater than 1
+    INFERENCE_SKIPPED_REALTIME_QOS = 3, // frame skipped due to realtime-qos policy
+    INFERENCE_SKIPPED_ROI = 4           // roi skipped because is_roi_classification_needed() returned false
+} InferenceStatus;
+
+typedef struct __Model {
+    const char *name;
+    char *object_class;
+    ImageInferenceContext *infer_ctx;
+    FFInferenceImpl *infer_impl;
+    // std::map<std::string, void *> proc;
+    void *input_preproc;
+
+    void *proc_config;
+    ModelInputPreproc model_preproc;
+    ModelOutputPostproc model_postproc;
+} Model;
+
+typedef struct __ROIMetaArray {
+    FFVideoRegionOfInterestMeta **roi_metas;
+    int num_metas;
+} ROIMetaArray;
+
+/* \brief output frames stored in queue */
+typedef struct __OutputFrame {
+    AVFrame *frame;
+    AVFrame *writable_frame;
+    int inference_count;
+} OutputFrame;
+
+/* \brief structure taken as IFramPtr carried by \func SubmitImage */
+typedef struct __InferenceResult {
+    InferenceROI inference_frame;
+    Model *model;
+} InferenceResult;
+
+struct __FFInferenceImpl {
+    int frame_num;
+    pthread_mutex_t _mutex; // Maybe not necessary for ffmpeg
+    const FFBaseInference *base_inference;
+
+    Model *model;
+
+    // output frames list
+    pthread_mutex_t output_frames_mutex;
+    ff_list_t *output_frames;
+    ff_list_t *processed_frames; // TODO: consider remove it if all output frames can be consumed instantly
+};
+
+static void SplitString(char *str, const char *delim, char **array, int *num, int max) {
+    char *p;
+    int i = 0;
+
+    if (!str || !delim || !array || !num)
+        return;
+
+    while (p = strtok(str, delim)) {
+        int j = 0;
+        char *s;
+        size_t end;
+
+        /* remove head blanks */
+        while (p[j] == '\n' || p[j] == ' ')
+            j++;
+
+        if (!p[j])
+            continue;
+
+        /* remove tail blanks */
+        s = p + j;
+        end = strlen(s) - 1;
+        while (s[end] == '\n' || s[end] == ' ')
+            s[end--] = '\0';
+
+        array[i++] = s;
+        av_assert0(i < max);
+
+        /* string is cached */
+        str = NULL;
+    }
+
+    *num = i;
+}
+
+static inline int avFormatToFourCC(int format) {
+    switch (format) {
+    case AV_PIX_FMT_NV12:
+        VAII_DEBUG("AV_PIX_FMT_NV12");
+        return FOURCC_NV12;
+    case AV_PIX_FMT_BGR0:
+        VAII_DEBUG("AV_PIX_FMT_BGR0");
+        return FOURCC_BGRX;
+    case AV_PIX_FMT_BGRA:
+        VAII_DEBUG("AV_PIX_FMT_BGRA");
+        return FOURCC_BGRA;
+    case AV_PIX_FMT_BGR24:
+        VAII_DEBUG("AV_PIX_FMT_BGR24");
+        return FOURCC_BGR;
+    case AV_PIX_FMT_RGBP:
+        VAII_DEBUG("AV_PIX_FMT_RGBP");
+        return FOURCC_RGBP;
+    case AV_PIX_FMT_YUV420P:
+        VAII_DEBUG("AV_PIX_FMT_YUV420P");
+        return FOURCC_I420;
+    case AV_PIX_FMT_VAAPI:
+        VAII_DEBUG("AV_PIX_FMT_VAAPI");
+        return FOURCC_RGBP;
+    }
+
+    VAII_LOGE("Unsupported AV Format: %d.", format);
+    return 0;
+}
+
+static void ff_buffer_map(AVFrame *frame, Image *image, MemoryType memoryType) {
+    const int n_planes = 4;
+
+    image->type = memoryType;
+    image->format = avFormatToFourCC(frame->format);
+    image->width = frame->width;
+    image->height = frame->height;
+    for (int i = 0; i < n_planes; i++) {
+        image->stride[i] = frame->linesize[i];
+    }
+
+#ifdef CONFIG_VAAPI
+    if (memoryType == MEM_TYPE_VAAPI) {
+        image->surface_id = (uint32_t)frame->data[3];
+        image->colorspace = frame->colorspace;
+    }
+#endif
+    if (memoryType == MEM_TYPE_SYSTEM) {
+        for (int i = 0; i < n_planes; i++) {
+            image->planes[i] = frame->data[i];
+        }
+    }
+}
+
+static int CheckObjectClass(const char *requested, const InferDetection *detection) {
+    LabelsArray *label_array = NULL;
+    if (!requested)
+        return 1;
+
+    if (!detection->label_buf)
+        return 1;
+
+    label_array = (LabelsArray *)detection->label_buf->data;
+    av_assert0(detection->label_id < label_array->num);
+
+    return !strcmp(requested, label_array->label[detection->label_id]) ? 1 : 0;
+}
+
+static inline void PushOutput(FFInferenceImpl *impl) {
+    ff_list_t *out = impl->output_frames;
+    ff_list_t *processed = impl->processed_frames;
+    while (!out->empty(out)) {
+        OutputFrame *front = (OutputFrame *)out->front(out);
+        AVFrame *frame = front->frame;
+        if (front->inference_count > 0) {
+            break; // inference not completed yet
+        }
+
+        processed->push_back(processed, frame);
+        out->pop_front(out);
+        av_free(front);
+    }
+}
+
+static void InferenceCompletionCallback(OutputBlobArray *blobs, UserDataBuffers *user_data) {
+    Model *model = NULL;
+    FFInferenceImpl *impl = NULL;
+    InferenceResult *result = NULL;
+    const FFBaseInference *base = NULL;
+    InferenceROIArray inference_frames_array = {};
+
+    if (0 == user_data->num_buffers)
+        return;
+
+    result = (InferenceResult *)user_data->frames[0];
+    model = result->model;
+    impl = model->infer_impl;
+    base = impl->base_inference;
+
+    for (int i = 0; i < user_data->num_buffers; i++) {
+        result = (InferenceResult *)user_data->frames[i];
+        av_dynarray_add(&inference_frames_array.infer_ROIs, &inference_frames_array.num_infer_ROIs,
+                        &result->inference_frame);
+    }
+
+    if (base->post_proc) {
+        ((PostProcFunction)base->post_proc)(blobs, &inference_frames_array, &model->model_postproc, model->name, base);
+    }
+
+    pthread_mutex_lock(&impl->output_frames_mutex);
+
+    for (int i = 0; i < inference_frames_array.num_infer_ROIs; i++) {
+        OutputFrame *output;
+        ff_list_t *out = impl->output_frames;
+        InferenceROI *frame = inference_frames_array.infer_ROIs[i];
+        iterator it = out->iterator_get(out);
+        while (it) {
+            output = (OutputFrame *)out->iterate_value(it);
+            if (frame->frame == output->frame || frame->frame == output->writable_frame) {
+                output->inference_count--;
+                break;
+            }
+            it = out->iterate_next(out, it);
+        }
+    }
+
+    PushOutput(impl);
+
+    for (int i = 0; i < user_data->num_buffers; i++)
+        av_free(user_data->frames[i]);
+
+    pthread_mutex_unlock(&impl->output_frames_mutex);
+
+    av_free(inference_frames_array.infer_ROIs);
+}
+
+static Model *CreateModel(FFBaseInference *base, const char *model_file, const char *model_proc_path,
+                          const char *object_class) {
+    int ret = 0;
+    Model *model = NULL;
+    const ImageInference *inference = image_inference_get_by_name("openvino");
+    const OutputBlobMethod *method = output_blob_method_get_by_name("openvino");
+    ImageInferenceContext *context = NULL;
+
+    VAII_LOGI("Loading model: device=%s, path=%s\n", base->param.device, model_file);
+    VAII_LOGI("Setting batch_size=%d, nireq=%d\n", base->param.batch_size, base->param.nireq);
+
+    context = image_inference_alloc(inference, method, "ffmpeg-image-infer");
+    model = (Model *)av_mallocz(sizeof(*model));
+    av_assert0(context && model);
+
+    if (model_proc_path) {
+        void *proc = model_proc_read_config_file(model_proc_path);
+        if (!proc) {
+            VAII_LOGE("Could not read proc config file:"
+                   "%s\n",
+                   model_proc_path);
+            av_assert0(proc);
+        }
+
+        if (model_proc_parse_input_preproc(proc, &model->model_preproc) < 0) {
+            VAII_ERROR("Parse input preproc error.\n");
+        }
+
+        if (model_proc_parse_output_postproc(proc, &model->model_postproc) < 0) {
+            VAII_ERROR("Parse output postproc error.\n");
+        }
+
+        model->proc_config = proc;
+    }
+
+    ret = context->inference->Create(context, MEM_TYPE_ANY, base->param.device, model_file, base->param.batch_size,
+                                     base->param.nireq, base->param.infer_config, NULL, InferenceCompletionCallback);
+    av_assert0(ret == 0);
+
+    // Create async pre_proc image inference backend
+    if (base->param.opaque) {
+        PreProcContext *preproc_ctx = NULL;
+        ImageInferenceContext *async_preproc_ctx = NULL;
+
+        const ImageInference *inference = image_inference_get_by_name("async_preproc");
+        async_preproc_ctx = image_inference_alloc(inference, NULL, "async-preproc-infer");
+        if (base->param.vpp_device == VPP_DEVICE_HW)
+            preproc_ctx = pre_proc_alloc(pre_proc_get_by_name("vaapi"));
+        else
+            preproc_ctx = pre_proc_alloc(pre_proc_get_by_name("mocker"));
+
+        av_assert0(async_preproc_ctx && preproc_ctx);
+
+        async_preproc_ctx->inference->CreateAsyncPreproc(async_preproc_ctx, context, preproc_ctx, 6,
+                                                         base->param.opaque);
+
+        // substitute for opevino image inference
+        context = async_preproc_ctx;
+    }
+
+    model->infer_ctx = context;
+    model->name = context->inference->GetModelName(context);
+    model->object_class = object_class ? av_strdup(object_class) : NULL;
+    model->input_preproc = NULL;
+
+    return model;
+}
+
+static void ReleaseModel(Model *model) {
+    ImageInferenceContext *ii_ctx;
+    if (!model)
+        return;
+
+    ii_ctx = model->infer_ctx;
+    ii_ctx->inference->Close(ii_ctx);
+    image_inference_free(ii_ctx);
+
+    model_proc_release_model_proc(model->proc_config, &model->model_preproc, &model->model_postproc);
+
+    if (model->object_class)
+        av_free(model->object_class);
+    av_free(model);
+}
+
+static void SubmitImage(Model *model, FFVideoRegionOfInterestMeta *meta, Image *image, AVFrame *frame) {
+    ImageInferenceContext *s = model->infer_ctx;
+    PreProcessor preProcessFunction = NULL;
+
+    InferenceResult *result = (InferenceResult *)malloc(sizeof(*result));
+    av_assert0(result);
+    result->inference_frame.frame = frame;
+    result->inference_frame.roi = *meta;
+    result->model = model;
+
+    image->rect = (Rectangle){.x = (int)meta->x, .y = (int)meta->y, .width = (int)meta->w, .height = (int)meta->h};
+#if 0
+    if (ff_base_inference->pre_proc && model.input_preproc) {
+        preProcessFunction = [&](InferenceBackend::Image &image) {
+            ((PreProcFunction)ff_base_inference->pre_proc)(model.input_preproc, image);
+        };
+    }
+    if (ff_base_inference->get_roi_pre_proc && model.input_preproc) {
+        preProcessFunction = ((GetROIPreProcFunction)ff_base_inference->get_roi_pre_proc)(model.input_preproc, meta);
+    }
+#endif
+    s->inference->SubmitImage(s, image, (IFramePtr)result, preProcessFunction);
+}
+
+static int SubmitImages(FFInferenceImpl *impl, const ROIMetaArray *metas, AVFrame *frame) {
+    int ret = 0;
+    Image image = {};
+
+    // TODO: map frame w/ different memory type to image
+    // BufferMapContext mapContext;
+
+    // map to the image according to the mem type
+    ff_buffer_map(frame, &image, frame->hw_frames_ctx ? MEM_TYPE_VAAPI : MEM_TYPE_SYSTEM);
+
+    for (int i = 0; i < metas->num_metas; i++) {
+        SubmitImage(impl->model, metas->roi_metas[i], &image, frame);
+    }
+
+    // ff_buffer_unmap(buffer, image, mapContext);
+
+    return ret;
+}
+
+FFInferenceImpl *FFInferenceImplCreate(FFBaseInference *ff_base_inference) {
+    Model *dnn_model = NULL;
+    FFInferenceImpl *impl = (FFInferenceImpl *)av_mallocz(sizeof(*impl));
+
+    av_assert0(impl && ff_base_inference && ff_base_inference->param.model);
+
+    dnn_model = CreateModel(ff_base_inference, ff_base_inference->param.model, ff_base_inference->param.model_proc,
+                            ff_base_inference->param.object_class);
+    dnn_model->infer_impl = impl;
+
+    impl->model = dnn_model;
+    impl->base_inference = ff_base_inference;
+    impl->output_frames = ff_list_alloc();
+    impl->processed_frames = ff_list_alloc();
+
+    av_assert0(impl->output_frames && impl->processed_frames);
+
+    pthread_mutex_init(&impl->_mutex, NULL);
+    pthread_mutex_init(&impl->output_frames_mutex, NULL);
+
+    return impl;
+}
+
+void FFInferenceImplRelease(FFInferenceImpl *impl) {
+    if (!impl)
+        return;
+
+    ReleaseModel(impl->model);
+
+    ff_list_free(impl->output_frames);
+    ff_list_free(impl->processed_frames);
+
+    pthread_mutex_destroy(&impl->_mutex);
+    pthread_mutex_destroy(&impl->output_frames_mutex);
+
+    av_free(impl);
+}
+
+int FFInferenceImplAddFrame(void *ctx, FFInferenceImpl *impl, AVFrame *frame) {
+    FFBaseInference *base_inference = (FFBaseInference *)impl->base_inference;
+    ROIMetaArray metas = {};
+    FFVideoRegionOfInterestMeta full_frame_meta = {};
+    int inference_count = 0;
+
+    InferenceStatus status = INFERENCE_EXECUTED;
+    if (++base_inference->num_skipped_frames < base_inference->param.every_nth_frame) {
+        status = INFERENCE_SKIPPED_PER_PROPERTY;
+    }
+
+    if (base_inference->param.realtime_qos) {
+        ImageInferenceContext *ii_ctx = impl->model->infer_ctx;
+        if (ii_ctx->inference->IsQueueFull(ii_ctx)) {
+            status = INFERENCE_SKIPPED_REALTIME_QOS;
+        }
+    }
+
+    if (status == INFERENCE_EXECUTED) {
+        base_inference->num_skipped_frames = 0;
+    }
+
+    // Collect all ROI metas into ROIMetaArray
+    if (base_inference->param.is_full_frame) {
+        if (base_inference->crop_full_frame) {
+            full_frame_meta.x = base_inference->param.crop_rect.x;
+            full_frame_meta.y = base_inference->param.crop_rect.y;
+            full_frame_meta.w = base_inference->param.crop_rect.width;
+            full_frame_meta.h = base_inference->param.crop_rect.height;
+            full_frame_meta.index = 0;
+        } else {
+            full_frame_meta.x = 0;
+            full_frame_meta.y = 0;
+            full_frame_meta.w = frame->width;
+            full_frame_meta.h = frame->height;
+            full_frame_meta.index = 0;
+        }
+        av_dynarray_add(&metas.roi_metas, &metas.num_metas, &full_frame_meta);
+    } else {
+        BBoxesArray *bboxes = NULL;
+        InferDetectionMeta *detect_meta = NULL;
+        AVFrameSideData *side_data = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_DETECTION);
+        if (side_data) {
+            detect_meta = (InferDetectionMeta *)(side_data->data);
+            av_assert0(detect_meta);
+            bboxes = detect_meta->bboxes;
+            if (bboxes) {
+                ModelInputPreproc *model_preproc = &impl->model->model_preproc;
+                for (int i = 0; i < bboxes->num; i++) {
+                    FFVideoRegionOfInterestMeta *roi_meta = NULL;
+                    if (!CheckObjectClass(model_preproc->object_class, bboxes->bbox[i]))
+                        continue;
+                    roi_meta = (FFVideoRegionOfInterestMeta *)av_malloc(sizeof(*roi_meta));
+                    if (roi_meta == NULL)
+                        goto exit;
+                    roi_meta->x = bboxes->bbox[i]->x_min;
+                    roi_meta->y = bboxes->bbox[i]->y_min;
+                    roi_meta->w = bboxes->bbox[i]->x_max - bboxes->bbox[i]->x_min;
+                    roi_meta->h = bboxes->bbox[i]->y_max - bboxes->bbox[i]->y_min;
+                    roi_meta->index = i;
+                    av_dynarray_add(&metas.roi_metas, &metas.num_metas, roi_meta);
+                }
+            }
+        }
+    }
+
+    // count number ROIs to run inference on
+    inference_count = (status == INFERENCE_EXECUTED) ? metas.num_metas : 0;
+    impl->frame_num++;
+
+    // push into output_frames queue
+    {
+        OutputFrame *output_frame;
+        ff_list_t *output = impl->output_frames;
+        ff_list_t *processed = impl->processed_frames;
+        pthread_mutex_lock(&impl->output_frames_mutex);
+
+        if (!inference_count && output->empty(output)) {
+            processed->push_back(processed, frame);
+            pthread_mutex_unlock(&impl->output_frames_mutex);
+            goto exit;
+        }
+
+        output_frame = (OutputFrame *)av_malloc(sizeof(*output_frame));
+        if (output_frame == NULL) {
+            pthread_mutex_unlock(&impl->output_frames_mutex);
+            goto exit;
+        }
+        output_frame->frame = frame;
+        output_frame->writable_frame = NULL; // TODO: alloc new frame if not writable
+        output_frame->inference_count = inference_count;
+        impl->output_frames->push_back(impl->output_frames, output_frame);
+
+        if (!inference_count) {
+            // If we don't need to run inference and there are no frames queued for inference then finish transform
+            pthread_mutex_unlock(&impl->output_frames_mutex);
+            goto exit;
+        }
+
+        pthread_mutex_unlock(&impl->output_frames_mutex);
+    }
+
+    SubmitImages(impl, &metas, frame);
+
+exit:
+    if (!base_inference->param.is_full_frame) {
+        for (int n = 0; n < metas.num_metas; n++)
+            av_free(metas.roi_metas[n]);
+    }
+    av_free(metas.roi_metas);
+    return 0;
+}
+
+int FFInferenceImplGetFrame(void *ctx, FFInferenceImpl *impl, AVFrame **frame) {
+    ff_list_t *l = impl->processed_frames;
+
+    if (l->empty(l) || !frame)
+        return AVERROR(EAGAIN);
+
+    pthread_mutex_lock(&impl->output_frames_mutex);
+    *frame = (AVFrame *)l->front(l);
+    l->pop_front(l);
+    pthread_mutex_unlock(&impl->output_frames_mutex);
+
+    return 0;
+}
+
+size_t FFInferenceImplGetQueueSize(void *ctx, FFInferenceImpl *impl) {
+    ff_list_t *out = impl->output_frames;
+    ff_list_t *pro = impl->processed_frames;
+    VAII_LOGI("output:%zu processed:%zu\n", out->size(out), pro->size(pro));
+    return out->size(out) + pro->size(pro);
+}
+
+size_t FFInferenceImplResourceStatus(void *ctx, FFInferenceImpl *impl) {
+    return impl->model->infer_ctx->inference->ResourceStatus(impl->model->infer_ctx);
+}
+
+void FFInferenceImplSinkEvent(void *ctx, FFInferenceImpl *impl, FF_INFERENCE_EVENT event) {
+    if (event == INFERENCE_EVENT_EOS) {
+        impl->model->infer_ctx->inference->Flush(impl->model->infer_ctx);
+    }
+}
diff --git a/libavfilter/inference_backend/ff_inference_impl.h b/libavfilter/inference_backend/ff_inference_impl.h
new file mode 100644
index 0000000..bcddf2d
--- /dev/null
+++ b/libavfilter/inference_backend/ff_inference_impl.h
@@ -0,0 +1,39 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "ff_base_inference.h"
+
+typedef struct __FFInferenceImpl FFInferenceImpl;
+
+FFInferenceImpl *FFInferenceImplCreate(FFBaseInference *ff_base_inference);
+
+void FFInferenceImplRelease(FFInferenceImpl *impl);
+
+int FFInferenceImplAddFrame(void *ctx, FFInferenceImpl *impl, AVFrame *frame);
+
+int FFInferenceImplGetFrame(void *ctx, FFInferenceImpl *impl, AVFrame **frame);
+
+size_t FFInferenceImplGetQueueSize(void *ctx, FFInferenceImpl *impl);
+
+size_t FFInferenceImplResourceStatus(void *ctx, FFInferenceImpl *impl);
+
+void FFInferenceImplSinkEvent(void *ctx, FFInferenceImpl *impl, FF_INFERENCE_EVENT event);
diff --git a/libavfilter/inference_backend/ff_list.c b/libavfilter/inference_backend/ff_list.c
new file mode 100644
index 0000000..33b00d7
--- /dev/null
+++ b/libavfilter/inference_backend/ff_list.c
@@ -0,0 +1,93 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "ff_list.h"
+#include "queue.c"
+
+static void *_ff_list_pop_back(void *thiz) {
+    return queue_pop_back((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+static void *_ff_list_pop_front(void *thiz) {
+    return queue_pop_front((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+static void _ff_list_push_back(void *thiz, void *item) {
+    queue_push_back((queue_t *)((ff_list_t *)thiz)->opaque, item);
+}
+
+static void _ff_list_push_front(void *thiz, void *item) {
+    queue_push_front((queue_t *)((ff_list_t *)thiz)->opaque, item);
+}
+
+static void *_ff_list_front(void *thiz) {
+    return queue_peek_front((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+static int _ff_list_empty(void *thiz) {
+    return queue_count((queue_t *)((ff_list_t *)thiz)->opaque) == 0;
+}
+
+static iterator _ff_list_iterator_get(void *thiz) {
+    return queue_iterate((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+static iterator _ff_list_iterate_next(void *thiz, iterator it) {
+    return queue_iterate_next((queue_t *)((ff_list_t *)thiz)->opaque, (queue_entry_t *)it);
+}
+
+static void *_ff_list_iterate_value(iterator it) {
+    return queue_iterate_value((queue_entry_t *)it);
+}
+
+static size_t _ff_list_size(void *thiz) {
+    return queue_count((queue_t *)((ff_list_t *)thiz)->opaque);
+}
+
+ff_list_t *ff_list_alloc(void) {
+    ff_list_t *thiz = (ff_list_t *)malloc(sizeof(*thiz));
+    if (!thiz)
+        return NULL;
+
+    queue_t *q = queue_create();
+    assert(q);
+
+    thiz->opaque = q;
+    thiz->size = _ff_list_size;
+    thiz->empty = _ff_list_empty;
+    thiz->front = _ff_list_front;
+    thiz->pop_back = _ff_list_pop_back;
+    thiz->pop_front = _ff_list_pop_front;
+    thiz->push_back = _ff_list_push_back;
+    thiz->push_front = _ff_list_push_front;
+    thiz->iterator_get = _ff_list_iterator_get;
+    thiz->iterate_next = _ff_list_iterate_next;
+    thiz->iterate_value = _ff_list_iterate_value;
+
+    return thiz;
+}
+
+void ff_list_free(ff_list_t *thiz) {
+    if (!thiz)
+        return;
+
+    queue_destroy((queue_t *)thiz->opaque);
+    free(thiz);
+}
\ No newline at end of file
diff --git a/libavfilter/inference_backend/ff_list.h b/libavfilter/inference_backend/ff_list.h
new file mode 100644
index 0000000..22e77a6
--- /dev/null
+++ b/libavfilter/inference_backend/ff_list.h
@@ -0,0 +1,55 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef __FF_LIST_H
+#define __FF_LIST_H
+
+typedef void *iterator;
+
+typedef struct __ff_list {
+    const void *opaque; // private data
+
+    void *(*pop_back)(void *thiz);
+
+    void *(*pop_front)(void *thiz);
+
+    void (*push_back)(void *thiz, void *item);
+
+    void (*push_front)(void *thiz, void *item);
+
+    void *(*front)(void *thiz);
+
+    void *(*next)(void *thiz, void *current);
+
+    int (*empty)(void *thiz);
+
+    unsigned long (*size)(void *thiz);
+
+    iterator (*iterator_get)(void *thiz);
+
+    iterator (*iterate_next)(void *thiz, iterator it);
+
+    void *(*iterate_value)(iterator it);
+} ff_list_t;
+
+ff_list_t *ff_list_alloc(void);
+void ff_list_free(ff_list_t *thiz);
+
+#endif // __FF_LIST_H
diff --git a/libavfilter/inference_backend/ff_proc_factory.c b/libavfilter/inference_backend/ff_proc_factory.c
new file mode 100644
index 0000000..af8106c
--- /dev/null
+++ b/libavfilter/inference_backend/ff_proc_factory.c
@@ -0,0 +1,686 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "ff_proc_factory.h"
+#include "logger.h"
+#include <libavutil/avassert.h>
+#include <math.h>
+
+static void infer_detect_metadata_buffer_free(void *opaque, uint8_t *data) {
+    BBoxesArray *bboxes = ((InferDetectionMeta *)data)->bboxes;
+
+    if (bboxes) {
+        int i;
+        for (i = 0; i < bboxes->num; i++) {
+            InferDetection *p = bboxes->bbox[i];
+            if (p->label_buf)
+                av_buffer_unref(&p->label_buf);
+            av_freep(&p);
+        }
+        av_free(bboxes->bbox);
+        av_freep(&bboxes);
+    }
+
+    av_free(data);
+}
+
+static void infer_classify_metadata_buffer_free(void *opaque, uint8_t *data) {
+    int i;
+    InferClassificationMeta *meta = (InferClassificationMeta *)data;
+    ClassifyArray *classes = meta->c_array;
+
+    if (classes) {
+        for (i = 0; i < classes->num; i++) {
+            InferClassification *c = classes->classifications[i];
+            av_freep(&c->attributes);
+            av_buffer_unref(&c->label_buf);
+            av_buffer_unref(&c->tensor_buf);
+            av_freep(&c);
+        }
+        av_free(classes->classifications);
+        av_freep(&classes);
+    }
+
+    av_free(data);
+}
+
+static int get_unbatched_size_in_bytes(OutputBlobContext *blob_ctx, size_t batch_size) {
+    const OutputBlobMethod *blob = blob_ctx->output_blob_method;
+    size_t size;
+    Dimensions *dim = blob->GetDims(blob_ctx);
+
+    if (dim->dims[0] != batch_size) {
+        VAII_ERROR("Blob last dimension should be equal to batch size");
+        av_assert0(0);
+    }
+    size = dim->dims[1];
+    for (int i = 2; i < dim->num_dims; i++) {
+        size *= dim->dims[i];
+    }
+    switch (blob->GetPrecision(blob_ctx)) {
+    case II_FP32:
+        size *= sizeof(float);
+        break;
+    case II_U8:
+    default:
+        break;
+    }
+    return size;
+}
+
+typedef struct DetectionObject {
+    int xmin, ymin, xmax, ymax, class_id;
+    float confidence;
+} DetectionObject;
+
+typedef struct DetectionObjectArray {
+    DetectionObject **objects;
+    int num_detection_objects;
+} DetectionObjectArray;
+
+static void DetectionObjectInit(DetectionObject *this, double x, double y, double h, double w, int class_id,
+                                float confidence, float h_scale, float w_scale) {
+    this->xmin = (int)((x - w / 2) * w_scale);
+    this->ymin = (int)((y - h / 2) * h_scale);
+    this->xmax = (int)(this->xmin + w * w_scale);
+    this->ymax = (int)(this->ymin + h * h_scale);
+    this->class_id = class_id;
+    this->confidence = confidence;
+}
+
+static int DetectionObjectCompare(const void *p1, const void *p2) {
+    return (*(DetectionObject **)p1)->confidence > (*(DetectionObject **)p2)->confidence ? 1 : -1;
+}
+
+static double IntersectionOverUnion(const DetectionObject *box_1, const DetectionObject *box_2) {
+    double width_of_overlap_area = fmin(box_1->xmax, box_2->xmax) - fmax(box_1->xmin, box_2->xmin);
+    double height_of_overlap_area = fmin(box_1->ymax, box_2->ymax) - fmax(box_1->ymin, box_2->ymin);
+    double area_of_overlap, area_of_union;
+    double box_1_area = (box_1->ymax - box_1->ymin) * (box_1->xmax - box_1->xmin);
+    double box_2_area = (box_2->ymax - box_2->ymin) * (box_2->xmax - box_2->xmin);
+
+    if (width_of_overlap_area < 0 || height_of_overlap_area < 0)
+        area_of_overlap = 0;
+    else
+        area_of_overlap = width_of_overlap_area * height_of_overlap_area;
+    area_of_union = box_1_area + box_2_area - area_of_overlap;
+    return area_of_overlap / area_of_union;
+}
+
+static int EntryIndex(int side, int lcoords, int lclasses, int location, int entry) {
+    int n = location / (side * side);
+    int loc = location % (side * side);
+    return n * side * side * (lcoords + lclasses + 1) + entry * side * side + loc;
+}
+
+#define YOLOV3_INPUT_SIZE 320 // TODO: add interface to get this info
+
+static void ParseYOLOV3Output(OutputBlobContext *blob_ctx, int image_width, int image_height,
+                              DetectionObjectArray *objects, const FFBaseInference *base) {
+    const OutputBlobMethod *blob = blob_ctx->output_blob_method;
+    Dimensions *dim = blob->GetDims(blob_ctx);
+    const int out_blob_h = (int)dim->dims[2];
+    const int out_blob_w = (int)dim->dims[3];
+    const int coords = 4, num = 3, classes = 80;
+    const float anchors[] = {10.0, 13.0, 16.0,  30.0,  33.0, 23.0,  30.0,  61.0,  62.0,
+                             45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0};
+    int side = out_blob_h;
+    int anchor_offset = 0;
+    int side_square = side * side;
+    const float *output_blob = (const float *)blob->GetData(blob_ctx);
+
+    av_assert0(out_blob_h == out_blob_w);
+    switch (side) {
+    case 13:
+    case 10:
+    case 19:
+        anchor_offset = 2 * 6;
+        break;
+    case 26:
+    case 20:
+    case 38:
+        anchor_offset = 2 * 3;
+        break;
+    case 52:
+    case 40:
+    case 76:
+        anchor_offset = 2 * 0;
+        break;
+    default:
+        VAII_ERROR("Invaild output size\n");
+        return;
+    }
+
+    for (int i = 0; i < side_square; ++i) {
+        int row = i / side;
+        int col = i % side;
+        for (int n = 0; n < num; ++n) {
+            int obj_index = EntryIndex(side, coords, classes, n * side * side + i, coords);
+            int box_index = EntryIndex(side, coords, classes, n * side * side + i, 0);
+            double x, y, width, height;
+
+            float scale = output_blob[obj_index];
+            float threshold = base->param.threshold;
+            if (scale < threshold)
+                continue;
+
+            x = (col + output_blob[box_index + 0 * side_square]) / side * YOLOV3_INPUT_SIZE;
+            y = (row + output_blob[box_index + 1 * side_square]) / side * YOLOV3_INPUT_SIZE;
+            width = exp(output_blob[box_index + 2 * side_square]) * anchors[anchor_offset + 2 * n];
+            height = exp(output_blob[box_index + 3 * side_square]) * anchors[anchor_offset + 2 * n + 1];
+
+            for (int j = 0; j < classes; ++j) {
+                int class_index = EntryIndex(side, coords, classes, n * side_square + i, coords + 1 + j);
+                float prob = scale * output_blob[class_index];
+                DetectionObject *obj = NULL;
+                if (prob < threshold)
+                    continue;
+                obj = av_mallocz(sizeof(*obj));
+                av_assert0(obj);
+                if (!base->crop_full_frame)
+                    DetectionObjectInit(obj, x, y, height, width, j, prob,
+                                    (float)(image_height) / (float)(YOLOV3_INPUT_SIZE),
+                                    (float)(image_width) / (float)(YOLOV3_INPUT_SIZE));
+                else
+                    DetectionObjectInit(obj, x, y, height, width, j, prob,
+                                    (float)(base->param.crop_rect.height) / (float)(YOLOV3_INPUT_SIZE),
+                                    (float)(base->param.crop_rect.width) / (float)(YOLOV3_INPUT_SIZE));
+                av_dynarray_add(&objects->objects, &objects->num_detection_objects, obj);
+            }
+        }
+    }
+}
+
+static void ExtractYOLOV3BoundingBoxes(const OutputBlobArray *blob_array, InferenceROIArray *infer_roi_array,
+                                       ModelOutputPostproc *model_postproc, const char *model_name,
+                                       const FFBaseInference *ff_base_inference) {
+    DetectionObjectArray obj_array = {};
+    BBoxesArray *boxes;
+    AVBufferRef *ref;
+    AVBufferRef *labels = NULL;
+    AVFrame *av_frame;
+    AVFrameSideData *side_data;
+    InferDetectionMeta *detect_meta;
+
+    av_assert0(blob_array->num_blobs == 3);           // This accepts networks with three layers
+    av_assert0(infer_roi_array->num_infer_ROIs == 1); // YoloV3 cannot support batch mode
+
+    for (int n = 0; n < blob_array->num_blobs; n++) {
+        OutputBlobContext *blob_ctx = blob_array->output_blobs[n];
+        const OutputBlobMethod *blob = blob_ctx->output_blob_method;
+        const char *layer_name = blob->GetOutputLayerName(blob_ctx);
+
+        if (model_postproc) {
+            int idx = findModelPostProcByName(model_postproc, layer_name);
+            if (idx != MAX_MODEL_OUTPUT)
+                labels = model_postproc->procs[idx].labels;
+        }
+
+        av_assert0(blob_ctx);
+        ParseYOLOV3Output(blob_ctx, infer_roi_array->infer_ROIs[0]->roi.w, infer_roi_array->infer_ROIs[0]->roi.h,
+                          &obj_array, ff_base_inference);
+    }
+
+    qsort(obj_array.objects, obj_array.num_detection_objects, sizeof(DetectionObject *), DetectionObjectCompare);
+    for (int i = 0; i < obj_array.num_detection_objects; ++i) {
+        DetectionObjectArray *d = &obj_array;
+        if (d->objects[i]->confidence == 0)
+            continue;
+        for (int j = i + 1; j < d->num_detection_objects; ++j)
+            if (IntersectionOverUnion(d->objects[i], d->objects[j]) >= 0.4)
+                d->objects[j]->confidence = 0;
+    }
+
+    boxes = av_mallocz(sizeof(*boxes));
+    av_assert0(boxes);
+
+    for (int i = 0; i < obj_array.num_detection_objects; ++i) {
+        InferDetection *new_bbox = NULL;
+        DetectionObject *object = obj_array.objects[i];
+        if (object->confidence < ff_base_inference->param.threshold)
+            continue;
+
+        new_bbox = (InferDetection *)av_mallocz(sizeof(*new_bbox));
+        av_assert0(new_bbox);
+
+        if (!ff_base_inference->crop_full_frame) {
+            new_bbox->x_min = object->xmin;
+            new_bbox->y_min = object->ymin;
+            new_bbox->x_max = object->xmax;
+            new_bbox->y_max = object->ymax;
+        } else {
+            int x_offset = ff_base_inference->param.crop_rect.x;
+            int y_offset = ff_base_inference->param.crop_rect.y;
+            new_bbox->x_min = object->xmin + x_offset;
+            new_bbox->y_min = object->ymin + y_offset;
+            new_bbox->x_max = object->xmax + x_offset;
+            new_bbox->y_max = object->ymax + y_offset;
+        }
+        new_bbox->confidence = object->confidence;
+        new_bbox->label_id = object->class_id;
+
+        new_bbox->x_min = new_bbox->x_min < 0 ? 0 : new_bbox->x_min;
+        new_bbox->y_min = new_bbox->y_min < 0 ? 0 : new_bbox->y_min;
+
+        if (labels)
+            new_bbox->label_buf = av_buffer_ref(labels);
+
+        av_dynarray_add(&boxes->bbox, &boxes->num, new_bbox);
+        VAII_LOGD("bbox %d %d %d %d\n", (int)new_bbox->x_min, (int)new_bbox->y_min,
+               (int)new_bbox->x_max, (int)new_bbox->y_max);
+    }
+
+    detect_meta = av_mallocz(sizeof(*detect_meta));
+    av_assert0(detect_meta);
+    detect_meta->bboxes = boxes;
+
+    ref = av_buffer_create((uint8_t *)detect_meta, sizeof(*detect_meta), &infer_detect_metadata_buffer_free, NULL, 0);
+    if (!ref) {
+        infer_detect_metadata_buffer_free(NULL, (uint8_t *)detect_meta);
+        VAII_ERROR("Create buffer ref failed.\n");
+        av_assert0(0);
+    }
+
+    av_frame = infer_roi_array->infer_ROIs[0]->frame;
+    // add meta data to side data
+    side_data = av_frame_new_side_data_from_buf(av_frame, AV_FRAME_DATA_INFERENCE_DETECTION, ref);
+    av_assert0(side_data);
+
+    // free all detection objects
+    for (int n = 0; n < obj_array.num_detection_objects; n++)
+        av_free(obj_array.objects[n]);
+    av_free(obj_array.objects);
+}
+
+static void ExtractBoundingBoxes(const OutputBlobArray *blob_array, InferenceROIArray *infer_roi_array,
+                                 ModelOutputPostproc *model_postproc, const char *model_name,
+                                 const FFBaseInference *ff_base_inference) {
+    for (int n = 0; n < blob_array->num_blobs; n++) {
+        AVBufferRef *labels = NULL;
+        BBoxesArray **boxes = NULL;
+        OutputBlobContext *ctx = blob_array->output_blobs[n];
+        const OutputBlobMethod *blob = ctx->output_blob_method;
+
+        const char *layer_name = blob->GetOutputLayerName(ctx);
+        const float *detections = (const float *)blob->GetData(ctx);
+
+        Dimensions *dim = blob->GetDims(ctx);
+        IILayout layout = blob->GetLayout(ctx);
+
+        int object_size = 0;
+        int max_proposal_count = 0;
+        float threshold = ff_base_inference->param.threshold;
+
+        switch (layout) {
+        case II_LAYOUT_NCHW:
+            object_size = dim->dims[3];
+            max_proposal_count = dim->dims[2];
+            break;
+        default:
+            VAII_ERROR("Unsupported output layout, boxes won't be extracted\n");
+            continue;
+        }
+
+        if (object_size != 7) { // SSD DetectionOutput format
+            VAII_ERROR("Unsupported output dimensions, boxes won't be extracted\n");
+            continue;
+        }
+
+        if (model_postproc) {
+            int idx = findModelPostProcByName(model_postproc, layer_name);
+            if (idx != MAX_MODEL_OUTPUT)
+                labels = model_postproc->procs[idx].labels;
+        }
+
+        boxes = (BBoxesArray **)av_mallocz_array(infer_roi_array->num_infer_ROIs, sizeof(boxes[0]));
+        av_assert0(boxes);
+
+        for (int i = 0; i < max_proposal_count; i++) {
+            int image_id = (int)detections[i * object_size + 0];
+            int label_id = (int)detections[i * object_size + 1];
+            float confidence = detections[i * object_size + 2];
+            float x_min = detections[i * object_size + 3];
+            float y_min = detections[i * object_size + 4];
+            float x_max = detections[i * object_size + 5];
+            float y_max = detections[i * object_size + 6];
+            if (image_id < 0 || (size_t)image_id >= infer_roi_array->num_infer_ROIs)
+                break;
+
+            if (confidence < threshold)
+                continue;
+
+            if (boxes[image_id] == NULL) {
+                boxes[image_id] = (BBoxesArray *)av_mallocz(sizeof(*boxes[image_id]));
+                av_assert0(boxes[image_id]);
+            }
+
+            /* using integer to represent */
+            {
+                FFVideoRegionOfInterestMeta *roi = &infer_roi_array->infer_ROIs[image_id]->roi;
+                InferDetection *new_bbox = (InferDetection *)av_mallocz(sizeof(*new_bbox));
+
+                int width = roi->w;
+                int height = roi->h;
+                int ix_min = (int)(x_min * width + 0.5);
+                int iy_min = (int)(y_min * height + 0.5);
+                int ix_max = (int)(x_max * width + 0.5);
+                int iy_max = (int)(y_max * height + 0.5);
+
+                if (ix_min < 0)
+                    ix_min = 0;
+                if (iy_min < 0)
+                    iy_min = 0;
+                if (ix_max > width)
+                    ix_max = width;
+                if (iy_max > height)
+                    iy_max = height;
+
+                av_assert0(new_bbox);
+                new_bbox->x_min = ix_min;
+                new_bbox->y_min = iy_min;
+                new_bbox->x_max = ix_max;
+                new_bbox->y_max = iy_max;
+                new_bbox->confidence = confidence;
+                new_bbox->label_id = label_id;
+                if (labels)
+                    new_bbox->label_buf = av_buffer_ref(labels);
+                av_dynarray_add(&boxes[image_id]->bbox, &boxes[image_id]->num, new_bbox);
+            }
+        }
+
+        for (int n = 0; n < infer_roi_array->num_infer_ROIs; n++) {
+            AVBufferRef *ref;
+            AVFrame *av_frame;
+            AVFrameSideData *sd;
+
+            InferDetectionMeta *detect_meta = (InferDetectionMeta *)av_malloc(sizeof(*detect_meta));
+            av_assert0(detect_meta);
+
+            detect_meta->bboxes = boxes[n];
+
+            ref = av_buffer_create((uint8_t *)detect_meta, sizeof(*detect_meta), &infer_detect_metadata_buffer_free,
+                                   NULL, 0);
+            if (ref == NULL) {
+                infer_detect_metadata_buffer_free(NULL, (uint8_t *)detect_meta);
+                av_assert0(ref);
+            }
+
+            av_frame = infer_roi_array->infer_ROIs[n]->frame;
+            // add meta data to side data
+            sd = av_frame_new_side_data_from_buf(av_frame, AV_FRAME_DATA_INFERENCE_DETECTION, ref);
+            if (sd == NULL) {
+                av_buffer_unref(&ref);
+                av_assert0(sd);
+            }
+            VAII_LOGD("av_frame:%p sd:%d\n", av_frame, av_frame->nb_side_data);
+        }
+
+        av_free(boxes);
+    }
+}
+
+static int CreateNewClassifySideData(AVFrame *frame, InferClassificationMeta *classify_meta) {
+    AVBufferRef *ref;
+    AVFrameSideData *new_sd;
+    ref = av_buffer_create((uint8_t *)classify_meta, sizeof(*classify_meta), &infer_classify_metadata_buffer_free, NULL,
+                           0);
+    if (!ref)
+        return AVERROR(ENOMEM);
+
+    // add meta data to side data
+    new_sd = av_frame_new_side_data_from_buf(frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION, ref);
+    if (!new_sd) {
+        av_buffer_unref(&ref);
+        VAII_ERROR("Could not add new side data\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return 0;
+}
+
+static av_cold void dump_softmax(char *name, int label_id, float conf, AVBufferRef *label_buf) {
+    LabelsArray *array = (LabelsArray *)label_buf->data;
+
+    VAII_LOGD("CLASSIFY META - Label id:%d %s:%s Conf:%f\n", label_id, name, array->label[label_id], conf);
+}
+
+static av_cold void dump_tensor_value(char *name, float value) {
+    VAII_LOGD("CLASSIFY META - %s:%1.2f\n", name, value);
+}
+
+static void find_max_element_index(const float *array, int len, int *index, float *value) {
+    int i;
+    *index = 0;
+    *value = array[0];
+    for (i = 1; i < len; i++) {
+        if (array[i] > *value) {
+            *index = i;
+            *value = array[i];
+        }
+    }
+}
+
+static int attributes_to_text(FFVideoRegionOfInterestMeta *meta, OutputPostproc *post_proc, void *data, Dimensions *dim,
+                              InferClassification *classification, InferClassificationMeta *classify_meta) {
+    const float *blob_data = (const float *)data;
+    uint32_t method_max, method_compound, method_index;
+
+    method_max = !strcmp(post_proc->method, "max");
+    method_compound = !strcmp(post_proc->method, "compound");
+    method_index = !strcmp(post_proc->method, "index");
+
+    if (!blob_data)
+        return -1;
+
+    if (method_max) {
+        int index;
+        float confidence;
+        size_t n = dim->dims[1];
+
+        find_max_element_index(data, n, &index, &confidence);
+
+        classification->detect_id = meta->index;
+        classification->name = post_proc->attribute_name;
+        classification->label_id = index;
+        classification->confidence = confidence;
+        classification->label_buf = av_buffer_ref(post_proc->labels);
+
+        if (classification->label_buf) {
+            dump_softmax(classification->name, classification->label_id, classification->confidence,
+                         classification->label_buf);
+        }
+    } else if (method_compound) {
+        int i;
+        double threshold = 0.5;
+        float confidence = 0;
+        LabelsArray *array;
+        classification->attributes = av_mallocz(4096*sizeof(char));
+        if (classification->attributes == NULL)
+            return -1;
+
+        if (post_proc->threshold != 0)
+            threshold = post_proc->threshold;
+
+        array = (LabelsArray *)post_proc->labels->data;
+        for (i = 0; i < array->num; i++) {
+            if (blob_data[i] >= threshold)
+                strncat(classification->attributes, array->label[i], (strlen(array->label[i]) + 1));
+            if (blob_data[i] > confidence)
+                confidence = blob_data[i];
+        }
+
+        classification->name = post_proc->attribute_name;
+        classification->confidence = confidence;
+
+        VAII_LOGD("Attributes: %s\n", classification->attributes);
+    } else if (method_index) {
+        int i;
+        LabelsArray *array;
+        classification->attributes = av_mallocz(4096*sizeof(char));
+        if (classification->attributes == NULL)
+            return -1;
+
+        array = (LabelsArray *)post_proc->labels->data;
+        for (i = 0; i < array->num; i++) {
+            int value = blob_data[i];
+            if (value < 0 || value >= array->num)
+                break;
+            strncat(classification->attributes, array->label[value], (strlen(array->label[value]) + 1));
+        }
+
+        classification->name = post_proc->attribute_name;
+
+        VAII_LOGD("Attributes: %s\n", classification->attributes);
+    }
+
+    return 0;
+}
+
+static int tensor_to_text(FFVideoRegionOfInterestMeta *meta, OutputPostproc *post_proc, void *data, Dimensions *dim,
+                          InferClassification *classification, InferClassificationMeta *classify_meta) {
+    // InferClassification *classify;
+    const float *blob_data = (const float *)data;
+    double scale = 1.0;
+
+    if (!blob_data)
+        return -1;
+
+    if (post_proc->tensor_to_text_scale != 0)
+        scale = post_proc->tensor_to_text_scale;
+
+    classification->detect_id = meta->index;
+    classification->name = post_proc->attribute_name;
+    classification->value = *blob_data * scale;
+
+    dump_tensor_value(classification->name, classification->value);
+    return 0;
+}
+
+static void Blob2RoiMeta(const OutputBlobArray *blob_array, InferenceROIArray *infer_roi_array,
+                         ModelOutputPostproc *model_postproc, const char *model_name,
+                         const FFBaseInference *ff_base_inference) {
+    int batch_size = infer_roi_array->num_infer_ROIs;
+
+    for (int n = 0; n < blob_array->num_blobs; n++) {
+        OutputBlobContext *ctx = blob_array->output_blobs[n];
+        const OutputBlobMethod *blob;
+        const char *layer_name;
+        uint8_t *data = NULL;
+        int size;
+        OutputPostproc *post_proc = NULL;
+        Dimensions *dimensions = NULL;
+
+        av_assert0(ctx);
+
+        blob = ctx->output_blob_method;
+        layer_name = blob->GetOutputLayerName(ctx);
+        data = (uint8_t *)blob->GetData(ctx);
+        dimensions = blob->GetDims(ctx);
+        size = get_unbatched_size_in_bytes(ctx, batch_size);
+
+        if (model_postproc) {
+            int proc_idx = findModelPostProcByName(model_postproc, layer_name);
+            if (proc_idx != MAX_MODEL_OUTPUT)
+                post_proc = &model_postproc->procs[proc_idx];
+        }
+
+        for (int b = 0; b < batch_size; b++) {
+            FFVideoRegionOfInterestMeta *meta = &infer_roi_array->infer_ROIs[b]->roi;
+            AVFrame *av_frame = infer_roi_array->infer_ROIs[b]->frame;
+            AVFrameSideData *sd = NULL;
+            InferClassificationMeta *classify_meta = NULL;
+            InferClassification *classification = NULL;
+
+            sd = av_frame_get_side_data(av_frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION);
+            if (sd) {
+                // append to exsiting side data
+                classify_meta = (InferClassificationMeta *)sd->data;
+                av_assert0(classify_meta);
+            } else {
+                ClassifyArray *classify_array = NULL;
+                // new classification meta data
+                classify_meta = av_mallocz(sizeof(*classify_meta));
+                classify_array = av_mallocz(sizeof(*classify_array));
+                av_assert0(classify_meta && classify_array);
+                classify_meta->c_array = classify_array;
+                av_assert0(0 == CreateNewClassifySideData(av_frame, classify_meta));
+            }
+
+            classification = av_mallocz(sizeof(*classification));
+            av_assert0(classification);
+            classification->layer_name = (char *)layer_name;
+            classification->model = (char *)model_name;
+
+            if (post_proc && post_proc->converter) {
+                if (!strcmp(post_proc->converter, "tensor_to_label")) {
+                    attributes_to_text(meta, post_proc, (void *)(data + b * size), dimensions, classification,
+                                       classify_meta);
+                } else if (!strcmp(post_proc->converter, "tensor_to_text")) {
+                    tensor_to_text(meta, post_proc, (void *)(data + b * size), dimensions, classification,
+                                   classify_meta);
+                } else {
+                    VAII_LOGE("Undefined converter:%s\n", post_proc->converter);
+                    av_free(classification);
+                    break;
+                }
+            } else {
+                // copy data to tensor buffer
+                classification->detect_id = meta->index;
+                classification->name = (char *)"default";
+                classification->tensor_buf = av_buffer_alloc(size);
+                av_assert0(classification->tensor_buf);
+                memcpy(classification->tensor_buf->data, data + b * size, size);
+            }
+
+            av_dynarray_add(&classify_meta->c_array->classifications, &classify_meta->c_array->num, classification);
+        }
+    }
+}
+
+PostProcFunction getPostProcFunctionByName(const char *name, const char *model) {
+    if (name == NULL || model == NULL)
+        return NULL;
+
+    if (!strcmp(name, "detect")) {
+        if (strstr(model, "yolo"))
+            return (PostProcFunction)ExtractYOLOV3BoundingBoxes;
+        else
+            return (PostProcFunction)ExtractBoundingBoxes;
+    } else if (!strcmp(name, "classify")) {
+        return (PostProcFunction)Blob2RoiMeta;
+    }
+    return NULL;
+}
+
+int findModelPostProcByName(ModelOutputPostproc *model_postproc, const char *layer_name) {
+    int proc_id;
+    // search model postproc
+    for (proc_id = 0; proc_id < MAX_MODEL_OUTPUT; proc_id++) {
+        char *proc_layer_name = model_postproc->procs[proc_id].layer_name;
+        // skip this output process
+        if (!proc_layer_name)
+            continue;
+        if (!strcmp(layer_name, proc_layer_name))
+            return proc_id;
+    }
+
+    VAII_LOGD("Could not find proc:%s\n", layer_name);
+    return proc_id;
+}
diff --git a/libavfilter/inference_backend/ff_proc_factory.h b/libavfilter/inference_backend/ff_proc_factory.h
new file mode 100644
index 0000000..9419c1f
--- /dev/null
+++ b/libavfilter/inference_backend/ff_proc_factory.h
@@ -0,0 +1,27 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "ff_base_inference.h"
+
+PostProcFunction getPostProcFunctionByName(const char *name, const char *model);
+
+int findModelPostProcByName(ModelOutputPostproc *model_postproc, const char *layer_name);
\ No newline at end of file
diff --git a/libavfilter/inference_backend/image.c b/libavfilter/inference_backend/image.c
new file mode 100644
index 0000000..1bc6c5d
--- /dev/null
+++ b/libavfilter/inference_backend/image.c
@@ -0,0 +1,92 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "image.h"
+#include "config.h"
+#include <assert.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+extern ImageMap image_map_vaapi;
+extern ImageMap image_map_mocker;
+
+static const ImageMap *const image_map_list[] = {
+#if CONFIG_VAAPI
+    &image_map_vaapi,
+#endif
+    &image_map_mocker, NULL};
+
+static const ImageMap *image_map_iterate(void **opaque) {
+    uintptr_t i = (uintptr_t)*opaque;
+    const ImageMap *im = image_map_list[i];
+
+    if (im != NULL)
+        *opaque = (void *)(i + 1);
+
+    return im;
+}
+
+const ImageMap *image_map_get_by_name(const char *name) {
+    const ImageMap *im = NULL;
+    void *opaque = 0;
+
+    if (name == NULL)
+        return NULL;
+
+    while ((im = image_map_iterate(&opaque)))
+        if (!strcmp(im->name, name))
+            return im;
+
+    return NULL;
+}
+
+ImageMapContext *image_map_alloc(const ImageMap *image_map) {
+    ImageMapContext *ret = NULL;
+
+    if (image_map == NULL)
+        return NULL;
+
+    ret = (ImageMapContext *)malloc(sizeof(*ret));
+    assert(ret);
+    memset(ret, 0, sizeof(*ret));
+
+    ret->mapper = image_map;
+    if (image_map->priv_size > 0) {
+        ret->priv = malloc(image_map->priv_size);
+        if (!ret->priv)
+            goto err;
+        memset(ret->priv, 0, image_map->priv_size);
+    }
+
+    return ret;
+err:
+    free(ret);
+    return NULL;
+}
+
+void image_map_free(ImageMapContext *context) {
+    if (context == NULL)
+        return;
+
+    if (context->priv)
+        free(context->priv);
+    free(context);
+}
diff --git a/libavfilter/inference_backend/image.h b/libavfilter/inference_backend/image.h
new file mode 100644
index 0000000..a65be54
--- /dev/null
+++ b/libavfilter/inference_backend/image.h
@@ -0,0 +1,90 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include <stdint.h>
+
+typedef enum MemoryType { MEM_TYPE_ANY = 0, MEM_TYPE_SYSTEM = 1, MEM_TYPE_OPENCL = 2, MEM_TYPE_VAAPI = 3 } MemoryType;
+
+typedef enum FourCC {
+    FOURCC_NV12 = 0x3231564E,
+    FOURCC_BGRA = 0x41524742,
+    FOURCC_BGRX = 0x58524742,
+    FOURCC_BGRP = 0x50524742,
+    FOURCC_BGR = 0x00524742,
+    FOURCC_RGBA = 0x41424752,
+    FOURCC_RGBX = 0x58424752,
+    FOURCC_RGBP = 0x50424752,
+    FOURCC_RGBP_F32 = 0x07282024,
+    FOURCC_I420 = 0x30323449,
+} FourCC;
+
+typedef struct Rectangle {
+    int x;
+    int y;
+    int width;
+    int height;
+} Rectangle;
+
+#define MAX_PLANES_NUMBER 4
+
+typedef struct Image {
+    MemoryType type;
+    union {
+        uint8_t *planes[MAX_PLANES_NUMBER]; // if type==SYSTEM
+        void *cl_mem;                       // if type==OPENCL
+        struct {                            // if type==VAAPI
+            uint32_t surface_id;
+            void *va_display;
+        };
+    };
+    int format; // FourCC
+    int colorspace;
+    int width;
+    int height;
+    int stride[MAX_PLANES_NUMBER];
+    Rectangle rect;
+} Image;
+
+typedef struct ImageMapContext ImageMapContext;
+
+// Map DMA/VAAPI image into system memory
+typedef struct ImageMap {
+    /* image mapper name. Must be non-NULL and unique among pre processing modules. */
+    const char *name;
+
+    Image (*Map)(ImageMapContext *context, const Image *image);
+
+    void (*Unmap)(ImageMapContext *context);
+
+    int priv_size;
+} ImageMap;
+
+struct ImageMapContext {
+    const ImageMap *mapper;
+    void *priv;
+};
+
+const ImageMap *image_map_get_by_name(const char *name);
+
+ImageMapContext *image_map_alloc(const ImageMap *image_map);
+
+void image_map_free(ImageMapContext *context);
\ No newline at end of file
diff --git a/libavfilter/inference_backend/image_inference.c b/libavfilter/inference_backend/image_inference.c
new file mode 100644
index 0000000..c3314bb
--- /dev/null
+++ b/libavfilter/inference_backend/image_inference.c
@@ -0,0 +1,199 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "image_inference.h"
+#include <limits.h>
+#include <string.h>
+
+extern OutputBlobMethod output_blob_method_openvino;
+extern ImageInference image_inference_openvino;
+extern ImageInference image_inference_async_preproc;
+
+static const ImageInference *const image_inference_list[] = {&image_inference_openvino, &image_inference_async_preproc,
+                                                             NULL};
+static const OutputBlobMethod *const output_blob_method_list[] = {&output_blob_method_openvino, NULL};
+
+static const ImageInference *image_inference_iterate(void **opaque) {
+    uintptr_t i = (uintptr_t)*opaque;
+    const ImageInference *ii = image_inference_list[i];
+
+    if (ii != NULL)
+        *opaque = (void *)(i + 1);
+
+    return ii;
+}
+
+static const OutputBlobMethod *output_blob_iterate(void **opaque) {
+    uintptr_t i = (uintptr_t)*opaque;
+    const OutputBlobMethod *obm = output_blob_method_list[i];
+
+    if (obm != NULL)
+        *opaque = (void *)(i + 1);
+
+    return obm;
+}
+
+const ImageInference *image_inference_get_by_name(const char *name) {
+    const ImageInference *ii = NULL;
+    void *opaque = 0;
+
+    if (name == NULL)
+        return NULL;
+
+    while ((ii = image_inference_iterate(&opaque)))
+        if (!strcmp(ii->name, name))
+            return (ImageInference *)ii;
+
+    return NULL;
+}
+
+ImageInferenceContext *image_inference_alloc(const ImageInference *infernce, const OutputBlobMethod *obm,
+                                             const char *instance_name) {
+    ImageInferenceContext *ret;
+
+    if (infernce == NULL)
+        return NULL;
+
+    ret = (ImageInferenceContext *)malloc(sizeof(*ret));
+    if (!ret)
+        return NULL;
+    memset(ret, 0, sizeof(*ret));
+
+    ret->inference = infernce;
+    ret->output_blob_method = obm;
+    ret->name = instance_name ? strdup(instance_name) : NULL;
+    if (infernce->priv_size > 0) {
+        ret->priv = malloc(infernce->priv_size);
+        if (!ret->priv)
+            goto err;
+        memset(ret->priv, 0, infernce->priv_size);
+    }
+
+    return ret;
+err:
+    free(ret->priv);
+    free(ret);
+    return NULL;
+}
+
+void image_inference_free(ImageInferenceContext *inference_context) {
+    if (inference_context == NULL)
+        return;
+
+    if (inference_context->priv)
+        free(inference_context->priv);
+    if (inference_context->name)
+        free(inference_context->name);
+    free(inference_context);
+}
+
+const OutputBlobMethod *output_blob_method_get_by_name(const char *name) {
+    const OutputBlobMethod *ob = NULL;
+    void *opaque = 0;
+
+    if (name == NULL)
+        return NULL;
+
+    while ((ob = output_blob_iterate(&opaque)))
+        if (!strcmp(ob->name, name))
+            return (OutputBlobMethod *)ob;
+
+    return NULL;
+}
+
+OutputBlobContext *output_blob_alloc(const OutputBlobMethod *obm) {
+    OutputBlobContext *ret;
+
+    if (obm == NULL)
+        return NULL;
+
+    ret = (OutputBlobContext *)malloc(sizeof(*ret));
+    if (!ret)
+        return NULL;
+    memset(ret, 0, sizeof(*ret));
+
+    ret->output_blob_method = obm;
+    if (obm->priv_size > 0) {
+        ret->priv = malloc(obm->priv_size);
+        if (!ret->priv)
+            goto err;
+        memset(ret->priv, 0, obm->priv_size);
+    }
+
+    return ret;
+err:
+    free(ret->priv);
+    free(ret);
+    return NULL;
+}
+
+void output_blob_free(OutputBlobContext *context) {
+    if (context == NULL)
+        return;
+
+    if (context->priv)
+        free(context->priv);
+    free(context);
+}
+
+#define FF_DYNARRAY_ADD(av_size_max, av_elt_size, av_array, av_size, av_success, av_failure)                           \
+    do {                                                                                                               \
+        size_t av_size_new = (av_size);                                                                                \
+        if (!((av_size) & ((av_size)-1))) {                                                                            \
+            av_size_new = (av_size) ? (av_size) << 1 : 1;                                                              \
+            if (av_size_new > (av_size_max) / (av_elt_size)) {                                                         \
+                av_size_new = 0;                                                                                       \
+            } else {                                                                                                   \
+                void *av_array_new = realloc((av_array), av_size_new * (av_elt_size));                                 \
+                if (!av_array_new)                                                                                     \
+                    av_size_new = 0;                                                                                   \
+                else                                                                                                   \
+                    (av_array) = (void **)av_array_new;                                                                \
+            }                                                                                                          \
+        }                                                                                                              \
+        if (av_size_new) {                                                                                             \
+            {av_success}(av_size)++;                                                                                   \
+        } else {                                                                                                       \
+            av_failure                                                                                                 \
+        }                                                                                                              \
+    } while (0)
+
+static void ii_freep(void *arg) {
+    void *val;
+
+    memcpy(&val, arg, sizeof(val));
+    memcpy(arg, &(void *){NULL}, sizeof(val));
+    free(val);
+}
+
+void image_inference_dynarray_add(void *tab_ptr, int *nb_ptr, void *elem) {
+    void **tab;
+    memcpy(&tab, tab_ptr, sizeof(tab));
+
+    FF_DYNARRAY_ADD(INT_MAX, sizeof(*tab), tab, *nb_ptr,
+                    {
+                        tab[*nb_ptr] = elem;
+                        memcpy(tab_ptr, &tab, sizeof(tab));
+                    },
+                    {
+                        *nb_ptr = 0;
+                        ii_freep(tab_ptr);
+                    });
+}
\ No newline at end of file
diff --git a/libavfilter/inference_backend/image_inference.h b/libavfilter/inference_backend/image_inference.h
new file mode 100644
index 0000000..6f92bfc
--- /dev/null
+++ b/libavfilter/inference_backend/image_inference.h
@@ -0,0 +1,163 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "image.h"
+#include "pre_proc.h"
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+
+typedef enum {
+    II_LAYOUT_ANY = 0,
+    II_LAYOUT_NCHW = 1,
+    II_LAYOUT_NHWC = 2,
+} IILayout;
+
+typedef enum {
+    II_FP32 = 10,
+    II_U8 = 40,
+} IIPrecision;
+
+/* Don't change this structure */
+#define II_MAX_DIMENSIONS 8
+typedef struct Dimensions {
+    size_t num_dims;
+    size_t dims[II_MAX_DIMENSIONS];
+} Dimensions;
+
+typedef void *IFramePtr;
+
+typedef struct UserDataBuffers {
+    IFramePtr *frames;
+    int num_buffers;
+} UserDataBuffers;
+
+typedef struct OutputBlobMethod OutputBlobMethod;
+typedef struct OutputBlobArray OutputBlobArray;
+typedef struct OutputBlobContext OutputBlobContext;
+typedef struct ImageInference ImageInference;
+typedef struct ImageInferenceContext ImageInferenceContext;
+
+/**
+ * \brief Callback function when a inference request completed.
+ * Image inference backend takes charge of memory management for @param Blobs and @param frames
+ * Caller is responsible to every IFramePtr after reinterpreted as customed data structure
+ */
+typedef void (*CallbackFunc)(OutputBlobArray *Blobs, UserDataBuffers *frames);
+typedef void (*PreProcessor)(Image *image);
+
+struct ImageInference {
+    /* image inference backend name. Must be non-NULL and unique among backends. */
+    const char *name;
+
+    /* create image inference engine */
+    int (*Create)(ImageInferenceContext *ctx, MemoryType type, const char *devices, const char *model, int batch_size,
+                  int nireq, const char *config, void *allocator, CallbackFunc callback);
+
+    /* create image inference engine w/ asynchronous input preprocessing */
+    int (*CreateAsyncPreproc)(ImageInferenceContext *async_preproc_context, ImageInferenceContext *inference_context,
+                              PreProcContext *preproc_context, int image_queue_size, void *opaque);
+
+    /* submit image */
+    void (*SubmitImage)(ImageInferenceContext *ctx, const Image *image, IFramePtr user_data,
+                        PreProcessor pre_processor);
+
+    const char *(*GetModelName)(ImageInferenceContext *ctx);
+
+    void (*GetModelInputInfo)(ImageInferenceContext *ctx, int *width, int *height, int *format);
+
+    int (*IsQueueFull)(ImageInferenceContext *ctx);
+
+    int (*ResourceStatus)(ImageInferenceContext *ctx);
+
+    void (*Flush)(ImageInferenceContext *ctx);
+
+    void (*Close)(ImageInferenceContext *ctx);
+
+    int priv_size; ///< size of private data to allocate for the backend
+};
+
+struct ImageInferenceContext {
+    const ImageInference *inference;
+    const OutputBlobMethod *output_blob_method;
+    char *name;
+    void *priv;
+};
+
+struct OutputBlobMethod {
+    /* output blob method name. Must be non-NULL and unique among output blob methods. */
+    const char *name;
+
+    const char *(*GetOutputLayerName)(OutputBlobContext *ctx);
+
+    IILayout (*GetLayout)(OutputBlobContext *ctx);
+
+    IIPrecision (*GetPrecision)(OutputBlobContext *ctx);
+
+    Dimensions *(*GetDims)(OutputBlobContext *ctx);
+
+    const void *(*GetData)(OutputBlobContext *ctx);
+
+    int priv_size; ///< size of private data to allocate for the output blob
+};
+
+struct OutputBlobContext {
+    const OutputBlobMethod *output_blob_method;
+    void *priv;
+};
+
+struct OutputBlobArray {
+    OutputBlobContext **output_blobs;
+    int num_blobs;
+};
+
+#define __STRING(x) #x
+
+#ifdef __cplusplus
+#define __CONFIG_KEY(name) KEY_##name
+#define __DECLARE_CONFIG_KEY(name) static const char *__CONFIG_KEY(name) = __STRING(name)
+__DECLARE_CONFIG_KEY(CPU_EXTENSION);          // library with implementation of custom layers
+__DECLARE_CONFIG_KEY(CPU_THREADS_NUM);        // threads number CPU plugin use for inference
+__DECLARE_CONFIG_KEY(CPU_THROUGHPUT_STREAMS); // number inference requests running in parallel
+__DECLARE_CONFIG_KEY(RESIZE_BY_INFERENCE);    // experimental, don't use
+#else
+#define KEY_CPU_EXTENSION __STRING(CPU_EXTENSION)                   // library with implementation of custom layers
+#define KEY_CPU_THREADS_NUM __STRING(CPU_THREADS_NUM)               // threads number CPU plugin use for inference
+#define KEY_CPU_THROUGHPUT_STREAMS __STRING(CPU_THROUGHPUT_STREAMS) // number inference requests running in parallel
+#define KEY_PRE_PROCESSOR_TYPE __STRING(PRE_PROCESSOR_TYPE)         // preprocessor, e.g. ie, ffmpeg, opencv, g-api etc.
+#define KEY_IMAGE_FORMAT __STRING(IMAGE_FORMAT)                     // image format, e.g. NV12, BGR, RGB etc.
+#endif
+
+const ImageInference *image_inference_get_by_name(const char *name);
+
+ImageInferenceContext *image_inference_alloc(const ImageInference *infernce, const OutputBlobMethod *blob,
+                                             const char *instance_name);
+
+void image_inference_free(ImageInferenceContext *inference_context);
+
+const OutputBlobMethod *output_blob_method_get_by_name(const char *name);
+
+OutputBlobContext *output_blob_alloc(const OutputBlobMethod *method);
+
+void output_blob_free(OutputBlobContext *context);
+
+void image_inference_dynarray_add(void *tab_ptr, int *nb_ptr, void *elem);
diff --git a/libavfilter/inference_backend/image_inference_async_preproc.c b/libavfilter/inference_backend/image_inference_async_preproc.c
new file mode 100644
index 0000000..5ab87ce
--- /dev/null
+++ b/libavfilter/inference_backend/image_inference_async_preproc.c
@@ -0,0 +1,245 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <assert.h>
+#include <string.h>
+
+#include "image_inference.h"
+#include "image_inference_async_preproc.h"
+#include "logger.h"
+
+#define MOCKER_PRE_PROC_MAGIC 0x47474747
+
+static void *AsyncPreprocWorkingFunction(void *arg);
+
+static void PreprocImagesFree(PreprocImage **imgs, size_t num_imgs) {
+    if (!imgs || !num_imgs)
+        return;
+
+    for (size_t i = 0; i < num_imgs; i++) {
+        if (imgs[i]) {
+            image_map_free(imgs[i]->img_map_ctx);
+            free(imgs[i]);
+        }
+    }
+    free(imgs);
+}
+
+static int ImageInferenceAsyncPreprocCreate(ImageInferenceContext *async_preproc_context,
+                                            ImageInferenceContext *inference_context, PreProcContext *preproc_context,
+                                            int image_queue_size, void *opaque) {
+    int ret = 0;
+    int width = 0, height = 0, format = 0;
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)async_preproc_context->priv;
+    PreProcInitParam pp_init_param = {};
+    assert(inference_context && preproc_context);
+
+    VAII_INFO("Using async preproc image inference.");
+
+    async_preproc->actual = inference_context;
+    async_preproc->pre_proc = preproc_context;
+
+    // TODO: create image pool
+    async_preproc->preproc_images = (PreprocImage **)malloc(image_queue_size * sizeof(*async_preproc->preproc_images));
+    if (!async_preproc->preproc_images) {
+        VAII_ERROR("Creat preproc images failed!");
+        goto err;
+    }
+    async_preproc->num_preproc_images = image_queue_size;
+
+    async_preproc->freeImages = SafeQueueCreate();
+    async_preproc->workingImages = SafeQueueCreate();
+    if (!async_preproc->freeImages || !async_preproc->workingImages) {
+        VAII_ERROR("Creat images queues failed!");
+        goto err;
+    }
+
+    inference_context->inference->GetModelInputInfo(inference_context, &width, &height, &format);
+    pp_init_param.va_display = opaque;
+    pp_init_param.num_surfaces = image_queue_size;
+    pp_init_param.width = width;
+    pp_init_param.height = height;
+    pp_init_param.format = format;
+
+    if (preproc_context->pre_proc->Init)
+        preproc_context->pre_proc->Init(preproc_context, &pp_init_param);
+
+    for (size_t n = 0; n < async_preproc->num_preproc_images; n++) {
+        PreprocImage *preproc_image = (PreprocImage *)malloc(sizeof(*preproc_image));
+        if (!preproc_image)
+            goto err;
+        memset(preproc_image, 0, sizeof(*preproc_image));
+
+        preproc_image->image.type = MEM_TYPE_ANY;
+        preproc_image->image.width = width;
+        preproc_image->image.height = height;
+        preproc_image->image.format = format;
+        if (MOCKER_PRE_PROC_MAGIC == (uint32_t)opaque)
+            preproc_image->img_map_ctx = image_map_alloc(image_map_get_by_name("mocker"));
+        else
+            preproc_image->img_map_ctx = image_map_alloc(image_map_get_by_name("vaapi"));
+        assert(preproc_image->img_map_ctx);
+        async_preproc->preproc_images[n] = preproc_image;
+        SafeQueuePush(async_preproc->freeImages, preproc_image);
+    }
+
+    ret = pthread_create(&async_preproc->async_thread, NULL, AsyncPreprocWorkingFunction, async_preproc_context);
+    if (ret != 0) {
+        VAII_ERROR("Create async preproc thread error!");
+        goto err;
+    }
+
+    return 0;
+err:
+    PreprocImagesFree(async_preproc->preproc_images, async_preproc->num_preproc_images);
+
+    if (async_preproc->freeImages)
+        SafeQueueDestroy(async_preproc->freeImages);
+    if (async_preproc->workingImages)
+        SafeQueueDestroy(async_preproc->workingImages);
+
+    return -1;
+}
+
+static int ImageInferenceAsyncPreprocCreateDummy(ImageInferenceContext *ctx, MemoryType type, const char *devices,
+                                                 const char *model, int batch_size, int nireq, const char *config,
+                                                 void *allocator, CallbackFunc callback) {
+    /* Leave empty */
+    return 0;
+}
+
+static void ImageInferenceAsyncPreprocSubmtImage(ImageInferenceContext *ctx, const Image *image, IFramePtr user_data,
+                                                 PreProcessor pre_processor) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+    PreProcContext *pp_ctx = async_preproc->pre_proc;
+    PreprocImage *pp_image = NULL;
+
+    pp_image = (PreprocImage *)SafeQueuePop(async_preproc->freeImages);
+    pp_ctx->pre_proc->Convert(pp_ctx, image, &pp_image->image, 1);
+    pp_image->user_data = user_data;
+    pp_image->pre_processor = pre_processor;
+    SafeQueuePush(async_preproc->workingImages, pp_image);
+}
+
+static const char *ImageInferenceAsyncPreprocGetModelName(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+    return infer->GetModelName(infer_ctx);
+}
+
+static int ImageInferenceAsyncPreprocIsQueueFull(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+    return infer->IsQueueFull(infer_ctx);
+}
+
+static int ImageInferenceResourceStatus(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    return infer_ctx->inference->ResourceStatus(infer_ctx);
+}
+
+static void ImageInferenceAsyncPreprocFlush(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+    // Since async image preproc is working in another independent thread,
+    // have to wait for all working images be processed and sent to inference engine,
+    // or the flushing cannot assure the un-batched frames can be used and released
+    SafeQueueWaitEmpty(async_preproc->workingImages);
+    return infer->Flush(infer_ctx);
+}
+
+static void ImageInferenceAsyncPreprocClose(ImageInferenceContext *ctx) {
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+    PreProcContext *pp_ctx = async_preproc->pre_proc;
+
+    if (async_preproc->async_thread) {
+        // add one empty request
+        PreprocImage pp_image = {};
+        SafeQueuePush(async_preproc->workingImages, &pp_image);
+        // wait for thread reaching empty request
+        pthread_join(async_preproc->async_thread, NULL);
+    }
+
+    for (size_t n = 0; n < async_preproc->num_preproc_images; n++) {
+        PreprocImage *pp_image = async_preproc->preproc_images[n];
+        pp_ctx->pre_proc->ReleaseImage(pp_ctx, &pp_image->image);
+    }
+
+    infer->Close(infer_ctx);
+    image_inference_free(infer_ctx);
+    pp_ctx->pre_proc->Destroy(pp_ctx);
+    pre_proc_free(pp_ctx);
+
+    PreprocImagesFree(async_preproc->preproc_images, async_preproc->num_preproc_images);
+
+    if (async_preproc->freeImages)
+        SafeQueueDestroy(async_preproc->freeImages);
+    if (async_preproc->workingImages)
+        SafeQueueDestroy(async_preproc->workingImages);
+}
+
+static void *AsyncPreprocWorkingFunction(void *arg) {
+    ImageInferenceContext *ctx = (ImageInferenceContext *)arg;
+    ImageInferenceAsyncPreproc *async_preproc = (ImageInferenceAsyncPreproc *)ctx->priv;
+    ImageInferenceContext *infer_ctx = async_preproc->actual;
+    const ImageInference *infer = infer_ctx->inference;
+
+    while (1) {
+        PreprocImage *pp_image = (PreprocImage *)SafeQueueFront(async_preproc->workingImages);
+        assert(pp_image);
+
+        // empty ctx means ending
+        if (!pp_image->img_map_ctx)
+            break;
+
+        {
+            ImageMapContext *map_ctx = pp_image->img_map_ctx;
+            Image image_sys = map_ctx->mapper->Map(map_ctx, &pp_image->image);
+            infer->SubmitImage(infer_ctx, &image_sys, pp_image->user_data, pp_image->pre_processor);
+            map_ctx->mapper->Unmap(map_ctx);
+        }
+        pp_image = (PreprocImage *)SafeQueuePop(async_preproc->workingImages);
+        SafeQueuePush(async_preproc->freeImages, pp_image);
+    }
+
+    return NULL;
+}
+
+ImageInference image_inference_async_preproc = {
+    .name = "async_preproc",
+    .priv_size = sizeof(ImageInferenceAsyncPreproc),
+    .CreateAsyncPreproc = ImageInferenceAsyncPreprocCreate,
+    .Create = ImageInferenceAsyncPreprocCreateDummy,
+    .SubmitImage = ImageInferenceAsyncPreprocSubmtImage,
+    .GetModelName = ImageInferenceAsyncPreprocGetModelName,
+    .IsQueueFull = ImageInferenceAsyncPreprocIsQueueFull,
+    .ResourceStatus = ImageInferenceResourceStatus,
+    .Flush = ImageInferenceAsyncPreprocFlush,
+    .Close = ImageInferenceAsyncPreprocClose,
+};
diff --git a/libavfilter/inference_backend/image_inference_async_preproc.h b/libavfilter/inference_backend/image_inference_async_preproc.h
new file mode 100644
index 0000000..89ccd0c
--- /dev/null
+++ b/libavfilter/inference_backend/image_inference_async_preproc.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "image_inference.h"
+#include "pre_proc.h"
+#include "safe_queue.h"
+#include <pthread.h>
+
+typedef struct PreprocImage {
+    Image image;
+    ImageMapContext *img_map_ctx;
+    IFramePtr user_data;        // Pass through to wrapped inference backend
+    PreProcessor pre_processor; // Pass through to wrapped inference backend
+} PreprocImage;
+
+typedef struct ImageInferenceAsyncPreproc {
+    ImageInferenceContext *actual;
+    PreProcContext *pre_proc;
+
+    PreprocImage **preproc_images;
+    size_t num_preproc_images;
+
+    // Threading
+    pthread_t async_thread;
+    SafeQueueT *freeImages;    // PreprocImage queue
+    SafeQueueT *workingImages; // PreprocImage queue
+} ImageInferenceAsyncPreproc;
\ No newline at end of file
diff --git a/libavfilter/inference_backend/logger.c b/libavfilter/inference_backend/logger.c
new file mode 100644
index 0000000..576d8a9
--- /dev/null
+++ b/libavfilter/inference_backend/logger.c
@@ -0,0 +1,57 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "logger.h"
+#include <stdio.h>
+#include <string.h>
+
+static VAIILogFuncPtr inference_log_function = default_log_function;
+
+void set_log_function(VAIILogFuncPtr log_func) {
+    inference_log_function = log_func;
+}
+
+void debug_log(int level, const char *file, const char *function, int line, const char *message) {
+    (*inference_log_function)(level, file, function, line, message);
+}
+
+void default_log_function(int level, const char *file, const char *function, int line, const char *message) {
+    const char log_level[][16] = {"DEFAULT", "ERROR", "WARNING", "INFO", "VERBOSE", "DEBUG", "TRACE", "MEMDUMP"};
+    fprintf(stderr, "%s \t %s:%i : %s \t %s \n", (char *)&log_level[level], file, line, function, message);
+}
+
+static VAIITraceFuncPtr inference_trace_function = default_trace_function;
+
+void set_trace_function(VAIITraceFuncPtr trace_func) {
+    inference_trace_function = trace_func;
+}
+
+void trace_log(int level, const char *fmt, ...) {
+    va_list args;
+    va_start(args, fmt);
+    (*inference_trace_function)(level, fmt, args);
+    va_end(args);
+}
+
+void default_trace_function(int level, const char *fmt, va_list vl) {
+    const char log_level[][16] = {"DEFAULT", "ERROR", "WARNING", "INFO", "VERBOSE", "DEBUG", "TRACE", "MEMDUMP"};
+    fprintf(stderr, "%s \t", (char *)&log_level[level]);
+    vprintf(fmt, vl);
+}
\ No newline at end of file
diff --git a/libavfilter/inference_backend/logger.h b/libavfilter/inference_backend/logger.h
new file mode 100644
index 0000000..71b6d4c
--- /dev/null
+++ b/libavfilter/inference_backend/logger.h
@@ -0,0 +1,88 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include <stdarg.h>
+
+enum {
+    VAII_ERROR_LOG_LEVEL = 1,
+    VAII_WARNING_LOG_LEVEL,
+    VAII_INFO_LOG_LEVEL,
+    VAII_VERBOSE_LOG_LEVEL,
+    VAII_DEBUG_LOG_LEVEL,
+    VAII_TRACE_LOG_LEVEL,
+    VAII_MEMDUMP_LOG_LEVEL,
+};
+
+#define VAII_DEBUG_LOG(level, message) debug_log(level, __FILE__, __FUNCTION__, __LINE__, message);
+
+#define VAII_MEMDUMP(message) VAII_DEBUG_LOG(VAII_MEMDUMP_LOG_LEVEL, message);
+#define VAII_TRACE(message) VAII_DEBUG_LOG(VAII_TRACE_LOG_LEVEL, message);
+#define VAII_DEBUG(message) VAII_DEBUG_LOG(VAII_DEBUG_LOG_LEVEL, message);
+#define VAII_INFO(message) VAII_DEBUG_LOG(VAII_INFO_LOG_LEVEL, message);
+#define VAII_FIXME(message) VAII_DEBUG_LOG(VAII_FIXME_LOG_LEVEL, message);
+#define VAII_WARNING(message) VAII_DEBUG_LOG(VAII_WARNING_LOG_LEVEL, message);
+#define VAII_ERROR(message) VAII_DEBUG_LOG(VAII_ERROR_LOG_LEVEL, message);
+
+#define VAII_LOGE(f_, ...) trace_log(VAII_ERROR_LOG_LEVEL, (f_), ##__VA_ARGS__);
+#define VAII_LOGW(f_, ...) trace_log(VAII_WARNING_LOG_LEVEL, (f_), ##__VA_ARGS__);
+#define VAII_LOGI(f_, ...) trace_log(VAII_INFO_LOG_LEVEL, (f_), ##__VA_ARGS__);
+#define VAII_LOGV(f_, ...) trace_log(VAII_VERBOSE_LOG_LEVEL, (f_), ##__VA_ARGS__);
+#define VAII_LOGD(f_, ...) trace_log(VAII_DEBUG_LOG_LEVEL, (f_), ##__VA_ARGS__);
+
+typedef void (*VAIILogFuncPtr)(int level, const char *file, const char *function, int line, const char *message);
+
+void set_log_function(VAIILogFuncPtr log_func);
+
+void debug_log(int level, const char *file, const char *function, int line, const char *message);
+
+void default_log_function(int level, const char *file, const char *function, int line, const char *message);
+
+typedef void (*VAIITraceFuncPtr)(int, const char *, va_list);
+
+void set_trace_function(VAIITraceFuncPtr trace_func);
+
+void trace_log(int level, const char *fmt, ...);
+
+void default_trace_function(int level, const char *fmt, va_list vl);
+
+#if defined(HAVE_ITT)
+#include "ittnotify.h"
+#include <string>
+
+static __itt_domain *itt_domain = NULL;
+inline void taskBegin(const char *name) {
+    if (itt_domain == NULL) {
+        itt_domain = __itt_domain_create("video-analytics");
+    }
+    __itt_task_begin(itt_domain, __itt_null, __itt_null, __itt_string_handle_create(name));
+}
+
+inline void taskEnd(void) {
+    __itt_task_end(itt_domain);
+}
+
+#else
+
+#define taskBegin(x)
+#define taskEnd(x)
+
+#endif
diff --git a/libavfilter/inference_backend/metaconverter.c b/libavfilter/inference_backend/metaconverter.c
new file mode 100644
index 0000000..ba26c98
--- /dev/null
+++ b/libavfilter/inference_backend/metaconverter.c
@@ -0,0 +1,209 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "metaconverter.h"
+#include "libavutil/avassert.h"
+#include "ff_base_inference.h"
+#include "logger.h"
+
+int convert_roi_detection(json_object *info_object, AVFrame *frame) {
+    AVFrameSideData *sd;
+    InferDetectionMeta *d_meta;
+    BBoxesArray *boxes = NULL;
+
+    if (!(sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_DETECTION)))
+        return 0;
+
+    d_meta = (InferDetectionMeta *)sd->data;
+
+    if (d_meta) {
+        json_object *resolution_object, *objects;
+
+        boxes = d_meta->bboxes;
+        if (boxes == NULL)
+            return 0;
+
+        resolution_object = json_object_new_object();
+        objects = json_object_new_array();
+
+        json_object_object_add(resolution_object, "width", json_object_new_int(frame->width));
+        json_object_object_add(resolution_object, "height", json_object_new_int(frame->height));
+        json_object_object_add(info_object, "resolution", resolution_object);
+
+        for (size_t i = 0; i < boxes->num; i++) {
+            LabelsArray *array;
+            int label_id;
+            json_object *box_object, *bounding_box, *detection_object;
+
+            if (!boxes->bbox[i]->label_buf) {
+                VAII_ERROR("No model proc for this model\n");
+                break;
+            }
+
+            array = (LabelsArray *)boxes->bbox[i]->label_buf->data;
+            label_id = boxes->bbox[i]->label_id;
+            box_object = json_object_new_object();
+            bounding_box = json_object_new_object();
+            detection_object = json_object_new_object();
+
+            json_object_object_add(box_object, "x_min", json_object_new_int(boxes->bbox[i]->x_min));
+            json_object_object_add(box_object, "y_min", json_object_new_int(boxes->bbox[i]->y_min));
+            json_object_object_add(box_object, "x_max", json_object_new_int(boxes->bbox[i]->x_max));
+            json_object_object_add(box_object, "y_max", json_object_new_int(boxes->bbox[i]->y_max));
+
+            json_object_object_add(bounding_box, "bounding_box", box_object);
+            json_object_object_add(detection_object, "detection", bounding_box);
+
+            json_object_object_add(detection_object, "object_id", json_object_new_int(i));
+            json_object_object_add(detection_object, "label", json_object_new_string(array->label[label_id]));
+            json_object_object_add(detection_object, "label_id", json_object_new_int(label_id));
+            json_object_object_add(detection_object, "confidence", json_object_new_double((double)boxes->bbox[i]->confidence));
+
+            json_object_array_add(objects, detection_object);
+        }
+
+        json_object_object_add(info_object, "Detection_Objects", objects);
+    }
+
+    return 1;
+}
+
+int convert_roi_tensor(json_object *info_object, AVFrame *frame) {
+    AVFrameSideData *sd;
+    InferClassificationMeta *c_meta;
+
+    if (!(sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION)))
+        return 0;
+
+    c_meta = (InferClassificationMeta *)sd->data;
+
+    if (c_meta) {
+        int i;
+        const int meta_num = c_meta->c_array->num;
+        json_object *objects = json_object_new_array();
+
+        for (i = 0; i < meta_num; i++) {
+            InferClassification *c = c_meta->c_array->classifications[i];
+            json_object *classify_object, *item_object;
+
+            if (!strcmp(c->name, "default")) {
+                VAII_ERROR("No model proc for this model\n");
+                break;
+            }
+
+            classify_object = json_object_new_object();
+            item_object = json_object_new_object();
+
+            json_object_object_add(item_object, "detect_id", json_object_new_int(c->detect_id));
+
+            if (!strcmp(c->name, "age")) {
+                json_object_object_add(item_object, "value", json_object_new_double((double)c->value));
+            }
+
+            if (!strcmp(c->name, "gender") || !strcmp(c->name, "emotion") || !strcmp(c->name, "color") ||
+                        !strcmp(c->name, "type") || !strcmp(c->name, "face_id")) {
+                LabelsArray *array = (LabelsArray *)c->label_buf->data;
+                json_object_object_add(item_object, "label", json_object_new_string(array->label[c->label_id]));
+                json_object_object_add(item_object, "label_id", json_object_new_int(c->label_id));
+                json_object_object_add(item_object, "confidence", json_object_new_double((double)c->confidence));
+            }
+
+            if (!strcmp(c->name, "license_plate")) {
+                json_object_object_add(item_object, "label", json_object_new_string(c->attributes));
+            }
+
+            if (!strcmp(c->name, "person-attributes")) {
+                json_object_object_add(item_object, "confidence", json_object_new_double((double)c->confidence));
+                json_object_object_add(item_object, "label", json_object_new_string(c->attributes));
+            }
+
+            json_object_object_add(classify_object, c->name, item_object);
+            json_object_array_add(objects, classify_object);
+        }
+
+        json_object_object_add(info_object, "Classification_Objects", objects);
+    }
+
+    return 1;
+}
+
+int tensors_to_file(AVFilterContext *ctx, AVFrame *frame, json_object *info_object) {
+    AVFrameSideData *sd;
+    MetaConvertContext *s = ctx->priv;
+    InferClassificationMeta *c_meta;
+
+    static uint32_t frame_num = 0;
+
+    if (!(sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION)))
+        return 0;
+
+    c_meta = (InferClassificationMeta *)sd->data;
+
+    if (c_meta) {
+        int i;
+        uint32_t index = 0;
+        char filename[1024] = {0};
+        const int meta_num = c_meta->c_array->num;
+        for (i = 0; i < meta_num; i++) {
+            FILE *f = NULL;
+            InferClassification *c = c_meta->c_array->classifications[i];
+            //TODO:check model and layer
+            if (!c->tensor_buf || !c->tensor_buf->data)
+                continue;
+
+            snprintf(filename, sizeof(filename), "%s/%s_frame_%u_idx_%u.tensor", s->location,
+                    s->method, frame_num, index);
+            f = fopen(filename, "wb");
+            if (!f) {
+                VAII_LOGW("Failed to open/create file: %s\n", filename);
+            } else {
+                fwrite(c->tensor_buf->data, sizeof(float), c->tensor_buf->size / sizeof(float), f);
+                fclose(f);
+            }
+            index++;
+        }
+    }
+
+    frame_num++;
+
+    return 0;
+}
+
+int detection_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *info_object) {
+    int ret;
+
+    ret = convert_roi_detection(info_object, frame);
+    return ret;
+}
+
+int classification_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *info_object) {
+    int ret;
+
+    ret = convert_roi_tensor(info_object, frame);
+    return ret;
+}
+
+int all_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *info_object) {
+    int ret1, ret2;
+
+    ret1 = convert_roi_detection(info_object, frame);
+    ret2 = convert_roi_tensor(info_object, frame);
+    return (ret1 + ret2);
+}
diff --git a/libavfilter/inference_backend/metaconverter.h b/libavfilter/inference_backend/metaconverter.h
new file mode 100644
index 0000000..e9c7d2d
--- /dev/null
+++ b/libavfilter/inference_backend/metaconverter.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include <libavutil/frame.h>
+#include "libavfilter/avfilter.h"
+#include <json-c/json.h>
+
+#define OFFSET(x) offsetof(MetaConvertContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+typedef struct MetaConvertContext {
+    const AVClass *class;
+
+    char *model;
+    char *converter;
+    char *method;
+    char *location;
+    char *layer;
+
+    int frame_number;
+    FILE *f;
+
+    int (*convert_func)(AVFilterContext *ctx, AVFrame *frame, json_object *info_object);
+
+} MetaConvertContext;
+
+int detection_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *info_object);
+
+int classification_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *info_object);
+
+int all_to_json(AVFilterContext *ctx, AVFrame *frame, json_object *info_object);
+
+int tensors_to_file(AVFilterContext *ctx, AVFrame *frame, json_object *info_object);
+
+int convert_roi_detection(json_object *info_object, AVFrame *frame);
+
+int convert_roi_tensor(json_object *info_object, AVFrame *frame);
\ No newline at end of file
diff --git a/libavfilter/inference_backend/model_proc.c b/libavfilter/inference_backend/model_proc.c
new file mode 100644
index 0000000..88490f4
--- /dev/null
+++ b/libavfilter/inference_backend/model_proc.c
@@ -0,0 +1,275 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "model_proc.h"
+#include "libavutil/avassert.h"
+#include "logger.h"
+#include <json-c/json.h>
+
+// helper functions
+static void infer_labels_dump(uint8_t *data) {
+    int i;
+    LabelsArray *labels = (LabelsArray *)data;
+    printf("labels: ");
+    for (i = 0; i < labels->num; i++)
+        printf("%s ", labels->label[i]);
+    printf("\n");
+}
+
+void infer_labels_buffer_free(void *opaque, uint8_t *data) {
+    int i;
+    LabelsArray *labels = (LabelsArray *)data;
+
+    for (i = 0; i < labels->num; i++)
+        av_freep(&labels->label[i]);
+
+    av_free(labels->label);
+
+    av_free(data);
+}
+
+int model_proc_get_file_size(FILE *fp) {
+    int file_size, current_pos;
+
+    if (!fp)
+        return -1;
+
+    current_pos = ftell(fp);
+
+    if (fseek(fp, 0, SEEK_END)) {
+        fprintf(stderr, "Couldn't seek to the end of feature file.\n");
+        return -1;
+    }
+
+    file_size = ftell(fp);
+
+    fseek(fp, current_pos, SEEK_SET);
+
+    return file_size;
+}
+/*
+ * model proc parsing functions using JSON-c
+ */
+void *model_proc_read_config_file(const char *path) {
+    int n, file_size;
+    json_object *proc_config = NULL;
+    uint8_t *proc_json = NULL;
+    json_tokener *tok = NULL;
+
+    FILE *fp = fopen(path, "rb");
+    if (!fp) {
+        fprintf(stderr, "File open error:%s\n", path);
+        return NULL;
+    }
+
+    file_size = model_proc_get_file_size(fp);
+
+    proc_json = av_mallocz(file_size + 1);
+    if (!proc_json)
+        goto end;
+
+    n = fread(proc_json, file_size, 1, fp);
+
+    UNUSED(n);
+
+    tok = json_tokener_new();
+    proc_config = json_tokener_parse_ex(tok, proc_json, file_size);
+    if (proc_config == NULL) {
+        enum json_tokener_error jerr;
+        jerr = json_tokener_get_error(tok);
+        fprintf(stderr, "Error before: %s\n", json_tokener_error_desc(jerr));
+        goto end;
+    }
+
+end:
+    if (proc_json)
+        av_freep(&proc_json);
+    if (tok)
+        json_tokener_free(tok);
+    fclose(fp);
+    return proc_config;
+}
+
+void model_proc_load_default_config_file(ModelInputPreproc *preproc, ModelOutputPostproc *postproc) {
+    if (preproc) {
+        /*
+         * format is a little tricky, an ideal input format for IE is BGR planer
+         * however, neither soft csc nor hardware vpp could support that format.
+         * Here, we set a close soft format. The actual one coverted before sent
+         * to IE will be decided by user config and hardware vpp used or not.
+         */
+        preproc->color_format = AV_PIX_FMT_BGR24;
+        preproc->layer_name = NULL;
+    }
+
+    if (postproc) {
+        // do nothing
+    }
+}
+
+int model_proc_parse_input_preproc(const void *json, ModelInputPreproc *m_preproc) {
+    json_object *jvalue, *preproc, *color, *layer, *object_class;
+    int ret;
+
+    ret = json_object_object_get_ex((json_object *)json, "input_preproc", &preproc);
+    if (!ret) {
+        VAII_DEBUG("No input_preproc.\n");
+        return 0;
+    }
+
+    // not support multiple inputs yet
+    av_assert0(json_object_array_length(preproc) <= 1);
+
+    jvalue = json_object_array_get_idx(preproc, 0);
+
+    ret = json_object_object_get_ex(jvalue, "color_format", &color);
+    if (ret) {
+        if (json_object_get_string(color) == NULL)
+            return -1;
+
+        VAII_LOGI("Color Format:\"%s\"\n", json_object_get_string(color));
+
+        if (!strcmp(json_object_get_string(color), "BGR"))
+            m_preproc->color_format = AV_PIX_FMT_BGR24;
+        else if (!strcmp(json_object_get_string(color), "RGB"))
+            m_preproc->color_format = AV_PIX_FMT_RGB24;
+        else
+            return -1;
+    }
+
+    ret = json_object_object_get_ex(jvalue, "object_class", &object_class);
+    if (ret) {
+        if (json_object_get_string(object_class) == NULL)
+            return -1;
+
+        VAII_LOGI("Object_class:\"%s\"\n", json_object_get_string(object_class));
+
+        m_preproc->object_class = (char *)json_object_get_string(object_class);
+    }
+
+    ret = json_object_object_get_ex(jvalue, "layer_name", &layer);
+    UNUSED(layer);
+
+    return 0;
+}
+
+// For detection, we now care labels only.
+// Layer name and type can be got from output blob.
+int model_proc_parse_output_postproc(const void *json, ModelOutputPostproc *m_postproc) {
+    json_object *jvalue, *postproc;
+    json_object *attribute, *converter, *labels, *layer, *method, *threshold;
+    json_object *tensor_to_text_scale, *tensor_to_text_precision;
+    int ret;
+    size_t jarraylen;
+
+    ret = json_object_object_get_ex((json_object *)json, "output_postproc", &postproc);
+    if (!ret) {
+        VAII_DEBUG("No output_postproc.\n");
+        return 0;
+    }
+
+    jarraylen = json_object_array_length(postproc);
+    av_assert0(jarraylen <= MAX_MODEL_OUTPUT);
+
+    for (int i = 0; i < jarraylen; i++) {
+        OutputPostproc *proc = &m_postproc->procs[i];
+        jvalue = json_object_array_get_idx(postproc, i);
+
+#define FETCH_STRING(var, name)                                                                                        \
+    do {                                                                                                               \
+        ret = json_object_object_get_ex(jvalue, #name, &var);                                                          \
+        if (ret)                                                                                                       \
+            proc->name = (char *)json_object_get_string(var);                                                          \
+    } while (0)
+#define FETCH_DOUBLE(var, name)                                                                                        \
+    do {                                                                                                               \
+        ret = json_object_object_get_ex(jvalue, #name, &var);                                                          \
+        if (ret)                                                                                                       \
+            proc->name = (double)json_object_get_double(var);                                                          \
+    } while (0)
+#define FETCH_INTEGER(var, name)                                                                                       \
+    do {                                                                                                               \
+        ret = json_object_object_get_ex(jvalue, #name, &var);                                                          \
+        if (ret)                                                                                                       \
+            proc->name = (int)json_object_get_int(var);                                                                \
+    } while (0)
+
+        FETCH_STRING(layer, layer_name);
+        FETCH_STRING(method, method);
+        FETCH_STRING(attribute, attribute_name);
+        FETCH_STRING(converter, converter);
+
+        FETCH_DOUBLE(threshold, threshold);
+        FETCH_DOUBLE(tensor_to_text_scale, tensor_to_text_scale);
+
+        FETCH_INTEGER(tensor_to_text_precision, tensor_to_text_precision);
+
+        // handle labels
+        ret = json_object_object_get_ex(jvalue, "labels", &labels);
+        if (ret) {
+            json_object *label;
+            size_t labels_num = json_object_array_length(labels);
+
+            if (labels_num > 0) {
+                AVBufferRef *ref = NULL;
+                LabelsArray *larray = av_mallocz(sizeof(*larray));
+
+                if (!larray)
+                    return AVERROR(ENOMEM);
+
+                for (int i = 0; i < labels_num; i++) {
+                    char *copy = NULL;
+                    label = json_object_array_get_idx(labels, i);
+                    copy = av_strdup(json_object_get_string(label));
+                    av_dynarray_add(&larray->label, &larray->num, copy);
+                }
+
+                ref = av_buffer_create((uint8_t *)larray, sizeof(*larray), &infer_labels_buffer_free, NULL, 0);
+
+                proc->labels = ref;
+
+                if (ref)
+                    infer_labels_dump(ref->data);
+            }
+        }
+    }
+
+#undef FETCH_STRING
+#undef FETCH_DOUBLE
+#undef FETCH_INTEGER
+
+    return 0;
+}
+
+void model_proc_release_model_proc(const void *json, ModelInputPreproc *preproc, ModelOutputPostproc *postproc) {
+    size_t index = 0;
+
+    if (!json)
+        return;
+
+    if (postproc) {
+        for (index = 0; index < MAX_MODEL_OUTPUT; index++) {
+            if (postproc->procs[index].labels)
+                av_buffer_unref(&postproc->procs[index].labels);
+        }
+    }
+
+    json_object_put((json_object *)json);
+}
\ No newline at end of file
diff --git a/libavfilter/inference_backend/model_proc.h b/libavfilter/inference_backend/model_proc.h
new file mode 100644
index 0000000..65a1f71
--- /dev/null
+++ b/libavfilter/inference_backend/model_proc.h
@@ -0,0 +1,39 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "ff_base_inference.h"
+
+#define UNUSED(x) (void)(x)
+
+void *model_proc_read_config_file(const char *path);
+
+void model_proc_load_default_config_file(ModelInputPreproc *preproc, ModelOutputPostproc *postproc);
+
+int model_proc_parse_input_preproc(const void *json, ModelInputPreproc *m_preproc);
+
+int model_proc_parse_output_postproc(const void *json, ModelOutputPostproc *m_postproc);
+
+void model_proc_release_model_proc(const void *json, ModelInputPreproc *preproc, ModelOutputPostproc *postproc);
+
+int model_proc_get_file_size(FILE *fp);
+
+void infer_labels_buffer_free(void *opaque, uint8_t *data);
\ No newline at end of file
diff --git a/libavfilter/inference_backend/openvino_image_inference.c b/libavfilter/inference_backend/openvino_image_inference.c
new file mode 100644
index 0000000..25293bd
--- /dev/null
+++ b/libavfilter/inference_backend/openvino_image_inference.c
@@ -0,0 +1,644 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <assert.h>
+#include <string.h>
+
+#include "image_inference.h"
+#include "logger.h"
+#include "openvino_image_inference.h"
+
+#define II_MAX(a, b) ((a) > (b) ? (a) : (b))
+#define II_MIN(a, b) ((a) > (b) ? (b) : (a))
+
+typedef enum { VPP_DEVICE_HW, VPP_DEVICE_SW } DEVICE_TYPE;
+
+static void *WorkingFunction(void *arg);
+
+static inline int getNumberChannels(int format) {
+    switch (format) {
+    case FOURCC_BGRA:
+    case FOURCC_BGRX:
+    case FOURCC_RGBA:
+    case FOURCC_RGBX:
+        return 4;
+    case FOURCC_BGR:
+        return 3;
+    }
+    return 0;
+}
+
+static IEColorFormat FormatNameToIEColorFormat(const char *format) {
+    static const char *formats[] = {"NV12", "RGB", "BGR", "RGBX", "BGRX", "RGBA", "BGRA"};
+    const IEColorFormat ie_color_formats[] = {NV12, RGB, BGR, RGBX, BGRX, RGBX, BGRX};
+
+    int num_formats = sizeof(formats) / sizeof(formats[0]);
+    for (int i = 0; i < num_formats; i++) {
+        if (!strcmp(format, formats[i]))
+            return ie_color_formats[i];
+    }
+
+    VAII_ERROR("Unsupported color format by Inference Engine preprocessing");
+    return RAW;
+}
+
+static inline void RectToIERoi(roi_t *roi, const Rectangle *rect) {
+    roi->id = 0;
+    roi->posX = rect->x;
+    roi->posY = rect->y;
+    roi->sizeX = rect->width;
+    roi->sizeY = rect->height;
+}
+
+static void GetNextImageBuffer(ImageInferenceContext *ctx, const BatchRequest *request, Image *image) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    const char *input_name;
+    dimensions_t blob_dims = {};
+    int batchIndex, plane_size;
+
+    VAII_DEBUG(__FUNCTION__);
+    assert(vino->num_inputs != 0);
+
+    input_name = vino->inputs[0]->name; // assuming one input layer
+    infer_request_get_blob_dims(request->infer_request, input_name, &blob_dims);
+
+    memset(image, 0, sizeof(*image));
+    image->width = blob_dims.dims[3];  // W
+    image->height = blob_dims.dims[2]; // H
+    image->format = FOURCC_RGBP;
+    batchIndex = request->buffers.num_buffers;
+    plane_size = image->width * image->height;
+    image->planes[0] = (uint8_t *)infer_request_get_blob_data(request->infer_request, input_name) +
+                       batchIndex * plane_size * blob_dims.dims[1];
+    image->planes[1] = image->planes[0] + plane_size;
+    image->planes[2] = image->planes[1] + plane_size;
+    image->stride[0] = image->width;
+    image->stride[1] = image->width;
+    image->stride[2] = image->width;
+}
+
+static inline Image ApplyCrop(const Image *src) {
+    int planes_count;
+    int rect_x, rect_y, rect_width, rect_height;
+    Image dst = *src;
+    dst.rect = (Rectangle){0};
+
+    VAII_DEBUG(__FUNCTION__);
+
+    planes_count = GetPlanesCount(src->format);
+    if (!src->rect.width && !src->rect.height) {
+        dst = *src;
+        for (int i = 0; i < planes_count; i++)
+            dst.planes[i] = src->planes[i];
+        return dst;
+    }
+
+    if (src->rect.x >= src->width || src->rect.y >= src->height || src->rect.x + src->rect.width <= 0 ||
+        src->rect.y + src->rect.height <= 0) {
+        fprintf(stderr, "ERROR: ApplyCrop: Requested rectangle is out of image boundaries\n");
+        assert(0);
+    }
+
+    rect_x = II_MAX(src->rect.x, 0);
+    rect_y = II_MAX(src->rect.y, 0);
+    rect_width = II_MIN(src->rect.width - (rect_x - src->rect.x), src->width - rect_x);
+    rect_height = II_MIN(src->rect.height - (rect_y - src->rect.y), src->height - rect_y);
+
+    switch (src->format) {
+    case FOURCC_NV12: {
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x;
+        dst.planes[1] = src->planes[1] + (rect_y / 2) * src->stride[1] + rect_x;
+        break;
+    }
+    case FOURCC_I420: {
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x;
+        dst.planes[1] = src->planes[1] + (rect_y / 2) * src->stride[1] + (rect_x / 2);
+        dst.planes[2] = src->planes[2] + (rect_y / 2) * src->stride[2] + (rect_x / 2);
+        break;
+    }
+    case FOURCC_RGBP: {
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x;
+        dst.planes[1] = src->planes[1] + rect_y * src->stride[1] + rect_x;
+        dst.planes[2] = src->planes[2] + rect_y * src->stride[2] + rect_x;
+        break;
+    }
+    case FOURCC_BGR: {
+        int channels = 3;
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x * channels;
+        break;
+    }
+    default: {
+        int channels = 4;
+        dst.planes[0] = src->planes[0] + rect_y * src->stride[0] + rect_x * channels;
+        break;
+    }
+    }
+
+    if (rect_width)
+        dst.width = rect_width;
+    if (rect_height)
+        dst.height = rect_height;
+
+    return dst;
+}
+
+static void SubmitImagePreProcess(ImageInferenceContext *ctx, const BatchRequest *request, const Image *pSrc,
+                                  PreProcessor preProcessor) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+
+    if (vino->resize_by_inference) {
+        const char *input_name = vino->inputs[0]->name;
+        // ie preprocess can only support system memory right now
+        assert(pSrc->type == MEM_TYPE_SYSTEM);
+        if (pSrc->format != FOURCC_NV12) {
+            roi_t roi, *_roi = NULL;
+            if (pSrc->rect.width != 0 && pSrc->rect.height != 0) {
+                RectToIERoi(&roi, &pSrc->rect);
+                _roi = &roi;
+            }
+            infer_request_set_blob(request->infer_request, input_name, pSrc->width, pSrc->height, vino->ie_color_format,
+                                   (uint8_t **)pSrc->planes, _roi);
+        } else {
+            Image src = {};
+            src = ApplyCrop(pSrc);
+            infer_request_set_blob(request->infer_request, input_name, src.width, src.height, vino->ie_color_format,
+                                   src.planes, NULL);
+        }
+    } else {
+        Image src = {};
+        Image dst = {};
+
+        dst.type = pSrc->type;
+        GetNextImageBuffer(ctx, request, &dst);
+
+        if (pSrc->planes[0] != dst.planes[0]) { // only convert if different buffers
+            if (!vino->vpp_ctx) {
+                vino->vpp_ctx = pre_proc_alloc(pre_proc_get_by_type(MEM_TYPE_SYSTEM));
+                assert(vino->vpp_ctx);
+            }
+#ifdef HAVE_GAPI
+            vino->vpp_ctx->pre_proc->Convert(vino->vpp_ctx, &src, &dst, 0);
+#else
+            if (pSrc->type == MEM_TYPE_SYSTEM)
+                src = ApplyCrop(pSrc);
+            else
+                src = *pSrc;
+            vino->vpp_ctx->pre_proc->Convert(vino->vpp_ctx, &src, &dst, 0);
+#endif
+            // model specific pre-processing
+            if (preProcessor)
+                preProcessor(&dst);
+        }
+    }
+}
+
+static int OpenVINOImageInferenceCreate(ImageInferenceContext *ctx, MemoryType type, const char *devices,
+                                        const char *model, int batch_size, int nireq, const char *configs,
+                                        void *allocator, CallbackFunc callback) {
+    int ret = 0, cpu_extension_needed = 0;
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    VAII_DEBUG("Create");
+
+    if (!model || !devices) {
+        VAII_ERROR("No model or device!");
+        return -1;
+    }
+
+    if (!callback) {
+        VAII_ERROR("Callback function is not assigned!");
+        return -1;
+    }
+
+    vino->core = ie_core_create();
+    if (!vino->core) {
+        VAII_ERROR("Create ie core failed!");
+        return -1;
+    }
+
+    if (configs) {
+        const char *pre_processor_name = NULL, *multi_device_list = NULL, *hetero_device_list = NULL;
+
+        ie_core_set_config(vino->core, configs, devices);
+        pre_processor_name = ie_core_get_config(vino->core, KEY_PRE_PROCESSOR_TYPE);
+        vino->resize_by_inference = (pre_processor_name && !strcmp(pre_processor_name, "ie")) ? 1 : 0;
+
+        multi_device_list = ie_core_get_config(vino->core, "MULTI_DEVICE_PRIORITIES");
+        hetero_device_list = ie_core_get_config(vino->core, "TARGET_FALLBACK");
+        if (multi_device_list && strstr(multi_device_list, "CPU") ||
+            hetero_device_list && strstr(hetero_device_list, "CPU")) {
+            cpu_extension_needed = 1;
+        }
+    }
+
+    // Extension for custom layers
+    if (cpu_extension_needed || strstr(devices, "CPU")) {
+        const char *cpu_ext = ie_core_get_config(vino->core, KEY_CPU_EXTENSION);
+        ie_core_add_extension(vino->core, cpu_ext, "CPU");
+        VAII_DEBUG("Cpu extension loaded!");
+    }
+
+    // Load network
+    vino->network = ie_network_create(vino->core, model, NULL);
+    if (!vino->network) {
+        VAII_ERROR("Create network failed!");
+        goto err;
+    }
+
+    vino->batch_size = batch_size;
+
+    // Check model input
+    vino->num_inputs = ie_network_get_input_number(vino->network);
+    vino->num_outputs = ie_network_get_output_number(vino->network);
+    if (vino->num_inputs == 0) {
+        VAII_ERROR("Input layer not found!");
+        goto err;
+    }
+
+    vino->inputs = (ie_input_info_t **)malloc(vino->num_inputs * sizeof(*vino->inputs));
+    vino->outputs = (ie_output_info_t **)malloc(vino->num_outputs * sizeof(*vino->outputs));
+    if (!vino->inputs || !vino->outputs) {
+        VAII_ERROR("Alloc in/outputs ptr failed!");
+        goto err;
+    }
+    memset(vino->inputs,  0, vino->num_inputs *  sizeof(*vino->inputs));
+    memset(vino->outputs, 0, vino->num_outputs * sizeof(*vino->outputs));
+
+    for (size_t i = 0; i < vino->num_inputs; i++) {
+        vino->inputs[i] = (ie_input_info_t *)malloc(sizeof(*vino->inputs[i]));
+        if (!vino->inputs[i])
+            goto err;
+        memset(vino->inputs[i], 0, sizeof(*vino->inputs[i]));
+    }
+    for (size_t i = 0; i < vino->num_outputs; i++) {
+        vino->outputs[i] = (ie_output_info_t *)malloc(sizeof(*vino->outputs[i]));
+        if (!vino->outputs[i])
+            goto err;
+        memset(vino->outputs[i], 0, sizeof(*vino->outputs[i]));
+    }
+
+    ie_network_get_all_inputs(vino->network, vino->inputs);
+
+    if (batch_size > 1) {
+        for (int i = 0; i < vino->num_inputs; i++)
+            ie_network_input_reshape(vino->network, vino->inputs[i], batch_size);
+    }
+
+    ie_network_get_all_outputs(vino->network, vino->outputs);
+
+    ie_input_info_set_precision(vino->inputs[0], "U8");
+    ie_input_info_set_layout(vino->inputs[0], "NCHW");
+    if (vino->resize_by_inference) {
+        const char *image_format_name = ie_core_get_config(vino->core, KEY_IMAGE_FORMAT);
+        if (image_format_name == NULL) {
+            VAII_ERROR("Input image format name must be specified!");
+            goto err;
+        }
+        vino->ie_color_format = FormatNameToIEColorFormat(image_format_name);
+        ie_input_info_set_preprocess(vino->inputs[0], vino->network, RESIZE_BILINEAR, vino->ie_color_format);
+    }
+
+    // Create infer requests
+    if (nireq == 0) {
+        VAII_ERROR("Input layer not found!");
+        goto err;
+    }
+
+    vino->infer_requests = ie_network_create_infer_requests(vino->network, nireq, devices);
+    if (!vino->infer_requests) {
+        VAII_ERROR("Creat infer requests failed!");
+        goto err;
+    }
+
+    vino->batch_requests = (BatchRequest **)malloc(nireq * sizeof(*vino->batch_requests));
+    if (!vino->batch_requests) {
+        VAII_ERROR("Creat batch requests failed!");
+        goto err;
+    }
+    vino->num_batch_requests = nireq;
+
+    vino->freeRequests = SafeQueueCreate();
+    vino->workingRequests = SafeQueueCreate();
+    if (!vino->freeRequests || !vino->workingRequests) {
+        VAII_ERROR("Creat request queues failed!");
+        goto err;
+    }
+
+    for (size_t n = 0; n < vino->infer_requests->num_reqs; n++) {
+        BatchRequest *batch_request = (BatchRequest *)malloc(sizeof(*batch_request));
+        if (!batch_request)
+            goto err;
+        memset(batch_request, 0, sizeof(*batch_request));
+        batch_request->infer_request = vino->infer_requests->requests[n];
+        vino->batch_requests[n] = batch_request;
+        SafeQueuePush(vino->freeRequests, batch_request);
+    }
+
+    // TODO: handle allocator
+
+    vino->model_name = strdup(model);
+    if (!vino->model_name) {
+        VAII_ERROR("Copy model name failed!");
+        goto err;
+    }
+
+    vino->callback = callback;
+    ret = pthread_create(&vino->working_thread, NULL, WorkingFunction, ctx);
+    if (ret != 0) {
+        VAII_ERROR("Create thread error!");
+        goto err;
+    }
+
+    pthread_mutex_init(&vino->flush_mutex, NULL);
+
+    return 0;
+err:
+    if (vino->inputs) {
+        for (size_t i = 0; i < vino->num_inputs; i++)
+            if (vino->inputs[i])
+                free(vino->inputs[i]);
+        free(vino->inputs);
+    }
+    if (vino->outputs) {
+        for (size_t i = 0; i < vino->num_outputs; i++)
+            if (vino->outputs[i])
+                free(vino->outputs[i]);
+        free(vino->outputs);
+    }
+    if (vino->batch_requests) {
+        for (size_t i = 0; i < vino->num_batch_requests; i++)
+            if (vino->batch_requests[i])
+                free(vino->batch_requests[i]);
+        free(vino->batch_requests);
+    }
+    if (vino->freeRequests)
+        SafeQueueDestroy(vino->freeRequests);
+    if (vino->workingRequests)
+        SafeQueueDestroy(vino->workingRequests);
+    ie_network_destroy(vino->network);
+    ie_core_destroy(vino->core);
+    return -1;
+}
+
+static void OpenVINOImageInferenceSubmtImage(ImageInferenceContext *ctx, const Image *image, IFramePtr user_data,
+                                             PreProcessor pre_processor) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    const Image *pSrc = image;
+    BatchRequest *request = NULL;
+
+    VAII_DEBUG(__FUNCTION__);
+
+    // pop() call blocks if freeRequests is empty, i.e all requests still in workingRequests list and not completed
+    request = (BatchRequest *)SafeQueuePop(vino->freeRequests);
+
+    SubmitImagePreProcess(ctx, request, pSrc, pre_processor);
+
+    image_inference_dynarray_add(&request->buffers.frames, &request->buffers.num_buffers, user_data);
+
+    // start inference asynchronously if enough buffers for batching
+    if (request->buffers.num_buffers >= vino->batch_size) {
+#if 1 // TODO: remove when license-plate-recognition-barrier model will take one input
+        if (vino->num_inputs > 1 && !strcmp(vino->inputs[1]->name, "seq_ind")) {
+            // 'seq_ind' input layer is some relic from the training
+            // it should have the leading 0.0f and rest 1.0f
+            dimensions_t dims = {};
+            float *blob_data;
+            int maxSequenceSizePerPlate;
+
+            infer_request_get_blob_dims(request->infer_request, vino->inputs[1]->name, &dims);
+            maxSequenceSizePerPlate = dims.dims[0];
+            blob_data = (float *)infer_request_get_blob_data(request->infer_request, vino->inputs[1]->name);
+            blob_data[0] = 0.0f;
+            for (int n = 1; n < maxSequenceSizePerPlate; n++)
+                blob_data[n] = 1.0f;
+        }
+#endif
+        infer_request_infer_async(request->infer_request);
+        SafeQueuePush(vino->workingRequests, request);
+    } else {
+        SafeQueuePushFront(vino->freeRequests, request);
+    }
+}
+
+static const char *OpenVINOImageInferenceGetModelName(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    return vino->model_name;
+}
+
+static void OpenVINOImageInferenceGetModelInputInfo(ImageInferenceContext *ctx, int *width, int *height, int *format) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    dimensions_t *input_blob_dims = &vino->inputs[0]->dim; // assuming one input layer
+    assert(input_blob_dims->ranks > 2);
+    *width = input_blob_dims->dims[3];  // W
+    *height = input_blob_dims->dims[2]; // H
+    *format = FOURCC_RGBP;
+}
+
+static int OpenVINOImageInferenceIsQueueFull(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    return SafeQueueEmpty(vino->freeRequests);
+}
+
+static int OpenVINOImageInferenceResourceStatus(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    return SafeQueueSize(vino->freeRequests) * vino->batch_size;
+}
+
+static void OpenVINOImageInferenceFlush(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    BatchRequest *request = NULL;
+
+    pthread_mutex_lock(&vino->flush_mutex);
+
+    if (vino->already_flushed) {
+        pthread_mutex_unlock(&vino->flush_mutex);
+        return;
+    }
+
+    vino->already_flushed = 1;
+
+    request = (BatchRequest *)SafeQueuePop(vino->freeRequests);
+    if (request->buffers.num_buffers > 0) {
+        // push the last request to infer
+        infer_request_infer_async(request->infer_request);
+        SafeQueuePush(vino->workingRequests, request);
+        SafeQueueWaitEmpty(vino->workingRequests);
+    } else {
+        SafeQueuePush(vino->freeRequests, request);
+    }
+
+    pthread_mutex_unlock(&vino->flush_mutex);
+}
+
+static void OpenVINOImageInferenceClose(ImageInferenceContext *ctx) {
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+
+    if (vino->working_thread) {
+        // add one empty request
+        BatchRequest batch_request = {};
+        SafeQueuePush(vino->workingRequests, &batch_request);
+        // wait for thread reaching empty request
+        pthread_join(vino->working_thread, NULL);
+    }
+
+    if (vino->batch_requests) {
+        for (size_t i = 0; i < vino->num_batch_requests; i++)
+            if (vino->batch_requests[i])
+                free(vino->batch_requests[i]);
+        free(vino->batch_requests);
+    }
+    if (vino->freeRequests)
+        SafeQueueDestroy(vino->freeRequests);
+    if (vino->workingRequests)
+        SafeQueueDestroy(vino->workingRequests);
+
+    if (vino->model_name)
+        free(vino->model_name);
+
+    pthread_mutex_destroy(&vino->flush_mutex);
+
+    if (vino->vpp_ctx) {
+        vino->vpp_ctx->pre_proc->Destroy(vino->vpp_ctx);
+        pre_proc_free(vino->vpp_ctx);
+    }
+
+    if (vino->inputs) {
+        for (size_t i = 0; i < vino->num_inputs; i++)
+            if (vino->inputs[i])
+                free(vino->inputs[i]);
+        free(vino->inputs);
+    }
+    if (vino->outputs) {
+        for (size_t i = 0; i < vino->num_outputs; i++)
+            if (vino->outputs[i])
+                free(vino->outputs[i]);
+        free(vino->outputs);
+    }
+
+    ie_network_destroy(vino->network);
+    ie_core_destroy(vino->core);
+}
+
+static void *WorkingFunction(void *arg) {
+    ImageInferenceContext *ctx = (ImageInferenceContext *)arg;
+    OpenVINOImageInference *vino = (OpenVINOImageInference *)ctx->priv;
+    VAII_DEBUG(__FUNCTION__);
+
+#define IE_REQUEST_WAIT_RESULT_READY -1
+    for (;;) {
+        IEStatusCode sts;
+        BatchRequest *request = (BatchRequest *)SafeQueueFront(vino->workingRequests);
+        assert(request);
+        VAII_DEBUG("loop");
+        if (!request->buffers.num_buffers) {
+            break;
+        }
+        sts = (IEStatusCode)infer_request_wait(request->infer_request, IE_REQUEST_WAIT_RESULT_READY);
+        if (IE_STATUS_OK == sts) {
+            OutputBlobArray blob_array = {};
+            ie_output_info_t **outputs = vino->outputs;
+
+            for (size_t i = 0; i < vino->num_outputs; i++) {
+                OpenVINOOutputBlob *vino_blob;
+                OutputBlobContext *blob_ctx = output_blob_alloc(ctx->output_blob_method);
+                assert(blob_ctx);
+                vino_blob = (OpenVINOOutputBlob *)blob_ctx->priv;
+                vino_blob->name = ie_info_get_name(outputs[i]);
+                vino_blob->blob = infer_request_get_blob(request->infer_request, vino_blob->name);
+                image_inference_dynarray_add(&blob_array.output_blobs, &blob_array.num_blobs, blob_ctx);
+            }
+
+            vino->callback(&blob_array, &request->buffers);
+
+            for (int n = 0; n < blob_array.num_blobs; n++) {
+                OutputBlobContext *blob_ctx = blob_array.output_blobs[n];
+                OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)blob_ctx->priv;
+                infer_request_put_blob(vino_blob->blob);
+                output_blob_free(blob_ctx);
+            }
+            blob_array.num_blobs = 0;
+            free(blob_array.output_blobs);
+        } else {
+            fprintf(stderr, "Inference Error: %d model: %s\n", sts, vino->model_name);
+            assert(0);
+        }
+
+        // move request from workingRequests to freeRequests list
+        request = (BatchRequest *)SafeQueuePop(vino->workingRequests);
+        // clear buffers
+        if (request->buffers.frames) {
+            free(request->buffers.frames);
+            request->buffers.frames = NULL;
+        }
+        request->buffers.num_buffers = 0;
+        SafeQueuePush(vino->freeRequests, request);
+    }
+
+    VAII_DEBUG("EXIT");
+
+    return NULL;
+}
+
+static const char *OpenVINOOutputBlobGetOutputLayerName(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    return vino_blob->name;
+}
+
+static Dimensions *OpenVINOOutputBlobGetDims(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    return (Dimensions *)ie_blob_get_dims(vino_blob->blob);
+}
+
+static IILayout OpenVINOOutputBlobGetLayout(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    return (IILayout)ie_blob_get_layout(vino_blob->blob);
+}
+
+static IIPrecision OpenVINOOutputBlobGetPrecision(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    return (IIPrecision)ie_blob_get_precision(vino_blob->blob);
+}
+
+static const void *OpenVINOOutputBlobGetData(OutputBlobContext *ctx) {
+    OpenVINOOutputBlob *vino_blob = (OpenVINOOutputBlob *)ctx->priv;
+    return ie_blob_get_data(vino_blob->blob);
+}
+
+OutputBlobMethod output_blob_method_openvino = {
+    .name = "openvino",
+    .priv_size = sizeof(OpenVINOOutputBlob),
+    .GetOutputLayerName = OpenVINOOutputBlobGetOutputLayerName,
+    .GetDims = OpenVINOOutputBlobGetDims,
+    .GetLayout = OpenVINOOutputBlobGetLayout,
+    .GetPrecision = OpenVINOOutputBlobGetPrecision,
+    .GetData = OpenVINOOutputBlobGetData,
+};
+
+ImageInference image_inference_openvino = {
+    .name = "openvino",
+    .priv_size = sizeof(OpenVINOImageInference),
+    .Create = OpenVINOImageInferenceCreate,
+    .SubmitImage = OpenVINOImageInferenceSubmtImage,
+    .GetModelName = OpenVINOImageInferenceGetModelName,
+    .GetModelInputInfo = OpenVINOImageInferenceGetModelInputInfo,
+    .IsQueueFull = OpenVINOImageInferenceIsQueueFull,
+    .ResourceStatus = OpenVINOImageInferenceResourceStatus,
+    .Flush = OpenVINOImageInferenceFlush,
+    .Close = OpenVINOImageInferenceClose,
+};
diff --git a/libavfilter/inference_backend/openvino_image_inference.h b/libavfilter/inference_backend/openvino_image_inference.h
new file mode 100644
index 0000000..ee7c61a
--- /dev/null
+++ b/libavfilter/inference_backend/openvino_image_inference.h
@@ -0,0 +1,71 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include <ie_c_api.h>
+#include "image_inference.h"
+#include "pre_proc.h"
+#include "safe_queue.h"
+#include <pthread.h>
+
+typedef struct BatchRequest {
+    infer_request_t *infer_request;
+    UserDataBuffers buffers;
+    // TODO: alloc_context
+} BatchRequest;
+
+typedef struct OpenVINOImageInference {
+    int resize_by_inference;
+    IEColorFormat ie_color_format;
+
+    CallbackFunc callback;
+
+    // Inference Engine
+    ie_core_t *core;
+    ie_network_t *network;
+    ie_input_info_t **inputs;
+    ie_output_info_t **outputs;
+    size_t num_inputs;
+    size_t num_outputs;
+    char *model_name;
+    infer_requests_t *infer_requests;
+
+    BatchRequest **batch_requests;
+    size_t num_batch_requests;
+
+    // Threading
+    int batch_size;
+    pthread_t working_thread;
+    SafeQueueT *freeRequests;    // BatchRequest queue
+    SafeQueueT *workingRequests; // BatchRequest queue
+
+    // VPP
+    PreProcContext *vpp_ctx;
+
+    int already_flushed;
+    pthread_mutex_t flush_mutex;
+
+} OpenVINOImageInference;
+
+typedef struct OpenVINOOutputBlob {
+    const char *name;
+    ie_blob_t *blob;
+} OpenVINOOutputBlob;
diff --git a/libavfilter/inference_backend/pre_proc.c b/libavfilter/inference_backend/pre_proc.c
new file mode 100644
index 0000000..1ae935e
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc.c
@@ -0,0 +1,137 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "pre_proc.h"
+#include <stdlib.h>
+#include <string.h>
+
+extern PreProc pre_proc_swscale;
+extern PreProc pre_proc_opencv;
+extern PreProc pre_proc_gapi;
+extern PreProc pre_proc_vaapi;
+extern PreProc pre_proc_mocker;
+
+static const PreProc *const pre_proc_list[] = {
+#if HAVE_FFMPEG || CONFIG_SWSCALE
+    &pre_proc_swscale,
+#endif
+#if HAVE_OPENCV
+    &pre_proc_opencv,
+#endif
+#if HAVE_GAPI
+    &pre_proc_gapi,
+#endif
+#if CONFIG_VAAPI
+    &pre_proc_vaapi,
+#endif
+    &pre_proc_mocker,  NULL};
+
+int GetPlanesCount(int fourcc) {
+    switch (fourcc) {
+    case FOURCC_BGRA:
+    case FOURCC_BGRX:
+    case FOURCC_BGR:
+    case FOURCC_RGBA:
+    case FOURCC_RGBX:
+        return 1;
+    case FOURCC_NV12:
+        return 2;
+    case FOURCC_BGRP:
+    case FOURCC_RGBP:
+    case FOURCC_I420:
+        return 3;
+    }
+
+    return 0;
+}
+
+static const PreProc *pre_proc_iterate(void **opaque) {
+    uintptr_t i = (uintptr_t)*opaque;
+    const PreProc *pp = pre_proc_list[i];
+
+    if (pp != NULL)
+        *opaque = (void *)(i + 1);
+
+    return pp;
+}
+
+const PreProc *pre_proc_get_by_name(const char *name) {
+    const PreProc *pp = NULL;
+    void *opaque = 0;
+
+    if (name == NULL)
+        return NULL;
+
+    while ((pp = pre_proc_iterate(&opaque)))
+        if (!strcmp(pp->name, name))
+            return pp;
+
+    return NULL;
+}
+
+const PreProc *pre_proc_get_by_type(MemoryType type) {
+    const PreProc *ret = NULL;
+
+    if (type == MEM_TYPE_SYSTEM) {
+        ret = pre_proc_get_by_name("swscale");
+        if (!ret)
+            ret = pre_proc_get_by_name("gapi");
+        if (!ret)
+            ret = pre_proc_get_by_name("opencv");
+    } else if (type == MEM_TYPE_VAAPI) {
+        ret = pre_proc_get_by_name("vaapi");
+    }
+
+    return ret;
+}
+
+PreProcContext *pre_proc_alloc(const PreProc *pre_proc) {
+    PreProcContext *ret;
+
+    if (pre_proc == NULL)
+        return NULL;
+
+    ret = (PreProcContext *)malloc(sizeof(*ret));
+    if (!ret)
+        return NULL;
+    memset(ret, 0, sizeof(*ret));
+
+    ret->pre_proc = pre_proc;
+    if (pre_proc->priv_size > 0) {
+        ret->priv = malloc(pre_proc->priv_size);
+        if (!ret->priv)
+            goto err;
+        memset(ret->priv, 0, pre_proc->priv_size);
+    }
+
+    return ret;
+err:
+    free(ret);
+    return NULL;
+}
+
+void pre_proc_free(PreProcContext *context) {
+    if (context == NULL)
+        return;
+
+    if (context->priv)
+        free(context->priv);
+    free(context);
+}
diff --git a/libavfilter/inference_backend/pre_proc.h b/libavfilter/inference_backend/pre_proc.h
new file mode 100644
index 0000000..69a967c
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc.h
@@ -0,0 +1,72 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#pragma once
+
+#include "config.h"
+#include "image.h"
+
+typedef struct PreProcContext PreProcContext;
+
+typedef struct PreProcInitParam {
+    union {
+        struct { // VAAPI
+            void *va_display;
+            int num_surfaces;
+            int width;
+            int height;
+            int format; // FourCC
+        };
+        void *reserved; // Others
+    };
+} PreProcInitParam;
+
+typedef struct PreProc {
+    /* image pre processing module name. Must be non-NULL and unique among pre processing modules. */
+    const char *name;
+
+    int (*Init)(PreProcContext *context, PreProcInitParam *param);
+
+    void (*Destroy)(PreProcContext *context);
+
+    void (*Convert)(PreProcContext *context, const Image *src, Image *dst, int bAllocateDestination);
+
+    // to be called if Convert called with bAllocateDestination = true
+    void (*ReleaseImage)(PreProcContext *context, Image *dst);
+
+    MemoryType mem_type;
+
+    int priv_size; ///< size of private data to allocate for pre processing
+} PreProc;
+
+struct PreProcContext {
+    const PreProc *pre_proc;
+    void *priv;
+};
+
+int GetPlanesCount(int fourcc);
+
+const PreProc *pre_proc_get_by_name(const char *name);
+
+const PreProc *pre_proc_get_by_type(MemoryType type);
+
+PreProcContext *pre_proc_alloc(const PreProc *pre_proc);
+
+void pre_proc_free(PreProcContext *context);
diff --git a/libavfilter/inference_backend/pre_proc_mocker.c b/libavfilter/inference_backend/pre_proc_mocker.c
new file mode 100644
index 0000000..c6686e2
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc_mocker.c
@@ -0,0 +1,61 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "pre_proc.h"
+#include <assert.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+static void MockerPreProcConvert(PreProcContext *context, const Image *src, Image *dst, int bAllocateDestination) {
+    assert(src->type == MEM_TYPE_SYSTEM);
+    memcpy(dst, src, sizeof(*src));
+}
+
+static void MockerPreProcDestroy(PreProcContext *context) {
+    // empty
+}
+
+static void MockerPreProcReleaseImage(PreProcContext *context, Image *image) {
+    // empty
+}
+
+static Image MockerMap(ImageMapContext *context, const Image *image) {
+    assert(image);
+    return *image;
+}
+
+static void MockerUnmap(ImageMapContext *context) {
+    // empty
+}
+
+ImageMap image_map_mocker = {
+    .name = "mocker",
+    .Map = MockerMap,
+    .Unmap = MockerUnmap,
+};
+
+PreProc pre_proc_mocker = {
+    .name = "mocker",
+    .mem_type = MEM_TYPE_SYSTEM,
+    .Convert = MockerPreProcConvert,
+    .ReleaseImage = MockerPreProcReleaseImage,
+    .Destroy = MockerPreProcDestroy,
+};
diff --git a/libavfilter/inference_backend/pre_proc_swscale.c b/libavfilter/inference_backend/pre_proc_swscale.c
new file mode 100644
index 0000000..0a2bb37
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc_swscale.c
@@ -0,0 +1,252 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "pre_proc.h"
+#include "logger.h"
+#include <assert.h>
+#include <libavutil/imgutils.h>
+#include <libswscale/swscale.h>
+// #define DEBUG
+
+#if CONFIG_SWSCALE || HAVE_FFMPEG
+
+#ifdef DEBUG
+static void DumpBGRpToFile(const Image *out_image) {
+    FILE *fp;
+    char file_name[256] = {};
+    static int dump_frame_num = 0;
+
+    sprintf(file_name, "ff_pre_proc%03d.rgb", dump_frame_num++);
+    fp = fopen(file_name, "w+b");
+    assert(fp);
+
+    const uint8_t *b_channel = out_image->planes[0];
+    const uint8_t *g_channel = out_image->planes[1];
+    const uint8_t *r_channel = out_image->planes[2];
+
+    int size = out_image->height * out_image->width * 3;
+    uint8_t *data = (uint8_t *)malloc(size);
+    memset(data, 0, size);
+
+    for (int i = 0; i < out_image->height; i++) {
+        for (int j = 0; j < out_image->width; j++) {
+            data[3 * j + i * 3 * out_image->width] = r_channel[j + i * out_image->width];
+            data[3 * j + i * 3 * out_image->width + 1] = g_channel[j + i * out_image->width];
+            data[3 * j + i * 3 * out_image->width + 2] = b_channel[j + i * out_image->width];
+        }
+    }
+    fwrite(data, out_image->height * out_image->width * 3, 1, fp);
+    free(data);
+    fclose(fp);
+}
+
+static void DumpRGBpToFile(const Image *out_image) {
+    FILE *fp;
+    char file_name[256] = {};
+    static int dump_frame_num = 0;
+
+    sprintf(file_name, "ff_rgbp_source%03d.rgb", dump_frame_num++);
+    fp = fopen(file_name, "w+b");
+    assert(fp);
+
+    const uint8_t *b_channel = out_image->planes[2];
+    const uint8_t *g_channel = out_image->planes[1];
+    const uint8_t *r_channel = out_image->planes[0];
+
+    int size = out_image->height * out_image->width * 3;
+    uint8_t *data = (uint8_t *)malloc(size);
+    memset(data, 0, size);
+
+    for (int i = 0; i < out_image->height; i++) {
+        for (int j = 0; j < out_image->width; j++) {
+            data[3 * j + i * 3 * out_image->width] = r_channel[j + i * out_image->width];
+            data[3 * j + i * 3 * out_image->width + 1] = g_channel[j + i * out_image->width];
+            data[3 * j + i * 3 * out_image->width + 2] = b_channel[j + i * out_image->width];
+        }
+    }
+    fwrite(data, out_image->height * out_image->width * 3, 1, fp);
+    free(data);
+    fclose(fp);
+}
+
+static inline void DumpImageInfo(const Image *p) {
+    VAII_LOGI("Image w:%d h:%d f:%x, plane: %p %p %p  stride: %d %d %d \n", p->width, p->height,
+           p->format, p->planes[0], p->planes[1], p->planes[2], p->stride[0], p->stride[1], p->stride[2]);
+}
+
+#endif
+
+static inline enum AVPixelFormat FOURCC2FFmpegFormat(int format) {
+    switch (format) {
+    case FOURCC_NV12:
+        return AV_PIX_FMT_NV12;
+    case FOURCC_BGRA:
+        return AV_PIX_FMT_BGRA;
+    case FOURCC_BGRX:
+        return AV_PIX_FMT_BGRA;
+    case FOURCC_BGR:
+        return AV_PIX_FMT_BGR24;
+    case FOURCC_RGBP:
+        return AV_PIX_FMT_RGBP;
+    case FOURCC_I420:
+        return AV_PIX_FMT_YUV420P;
+    }
+    return AV_PIX_FMT_NONE;
+}
+
+typedef struct FFPreProc {
+    struct SwsContext *sws_context[3];
+    Image image_yuv;
+    Image image_bgr;
+} FFPreProc;
+
+static void FFPreProcConvert(PreProcContext *context, const Image *src, Image *dst, int bAllocateDestination) {
+    FFPreProc *ff_pre_proc = (FFPreProc *)context->priv;
+    struct SwsContext **sws_context = ff_pre_proc->sws_context;
+    Image *image_yuv = &ff_pre_proc->image_yuv;
+    Image *image_bgr = &ff_pre_proc->image_bgr;
+    uint8_t *gbr_planes[4] = {};
+
+    // if identical format and resolution
+    if (src->format == dst->format && src->format == FOURCC_RGBP && src->width == dst->width &&
+        src->height == dst->height) {
+        int planes_count = GetPlanesCount(src->format);
+        // RGB->BGR
+        Image src_bgr = *src;
+        src_bgr.planes[0] = src->planes[2];
+        src_bgr.planes[2] = src->planes[0];
+        for (int i = 0; i < planes_count; i++) {
+            if (src_bgr.width == src_bgr.stride[i]) {
+                memcpy(dst->planes[i], src_bgr.planes[i], src_bgr.width * src_bgr.height * sizeof(uint8_t));
+            } else {
+                int dst_stride = dst->stride[i] * sizeof(uint8_t);
+                int src_stride = src_bgr.stride[i] * sizeof(uint8_t);
+                for (int r = 0; r < src_bgr.height; r++) {
+                    memcpy(dst->planes[i] + r * dst_stride, src_bgr.planes[i] + r * src_stride, dst->width);
+                }
+            }
+        }
+
+        return;
+    }
+#define PLANE_NUM 3
+    // init image YUV
+    if (image_yuv->width != dst->width || image_yuv->height != dst->height) {
+        int ret = 0;
+        image_yuv->width = dst->width;
+        image_yuv->height = dst->height;
+        image_yuv->format = src->format; // no CSC for the 1st stage
+
+        if (image_yuv->planes[0])
+            av_freep(&image_yuv->planes[0]);
+        ret = av_image_alloc(image_yuv->planes, image_yuv->stride, image_yuv->width, image_yuv->height,
+                             FOURCC2FFmpegFormat(image_yuv->format), 16);
+        if (ret < 0) {
+            fprintf(stderr, "Alloc yuv image buffer error!\n");
+            assert(0);
+        }
+    }
+
+    // init image BGR24
+    if (image_bgr->width != image_yuv->width || image_bgr->height != image_yuv->height) {
+        int ret = 0;
+        image_bgr->width = image_yuv->width;
+        image_bgr->height = image_yuv->height;
+        image_bgr->format = FOURCC_BGR; // YUV -> BGR packed for the 2nd stage
+
+        if (image_bgr->planes[0])
+            av_freep(&image_bgr->planes[0]);
+        ret = av_image_alloc(image_bgr->planes, image_bgr->stride, image_bgr->width, image_bgr->height,
+                             FOURCC2FFmpegFormat(image_bgr->format), 16);
+        if (ret < 0) {
+            fprintf(stderr, "Alloc bgr image buffer error!\n");
+            assert(0);
+        }
+    }
+
+    sws_context[0] = sws_getCachedContext(sws_context[0], src->width, src->height, FOURCC2FFmpegFormat(src->format),
+                                          image_yuv->width, image_yuv->height, FOURCC2FFmpegFormat(image_yuv->format),
+                                          SWS_FAST_BILINEAR, NULL, NULL, NULL);
+    sws_context[1] = sws_getCachedContext(sws_context[1], image_yuv->width, image_yuv->height,
+                                          FOURCC2FFmpegFormat(image_yuv->format), image_bgr->width, image_bgr->height,
+                                          FOURCC2FFmpegFormat(image_bgr->format), SWS_FAST_BILINEAR, NULL, NULL, NULL);
+    sws_context[2] = sws_getCachedContext(sws_context[2], image_bgr->width, image_bgr->height,
+                                          FOURCC2FFmpegFormat(image_bgr->format), dst->width, dst->height,
+                                          AV_PIX_FMT_GBRP, SWS_FAST_BILINEAR, NULL, NULL, NULL);
+
+    for (int i = 0; i < 3; i++)
+        assert(sws_context[i]);
+
+    // BGR->GBR
+    gbr_planes[0] = dst->planes[1];
+    gbr_planes[1] = dst->planes[0];
+    gbr_planes[2] = dst->planes[2];
+
+    // stage 1: yuv -> yuv, resize to dst size
+    if (!sws_scale(sws_context[0], (const uint8_t *const *)src->planes, src->stride, 0, src->height,
+                   image_yuv->planes, image_yuv->stride)) {
+        fprintf(stderr, "Error on FFMPEG sws_scale stage 1\n");
+        assert(0);
+    }
+    // stage 2: yuv -> bgr packed, no resize
+    if (!sws_scale(sws_context[1], (const uint8_t *const *)image_yuv->planes, image_yuv->stride, 0, image_yuv->height,
+                   image_bgr->planes, image_bgr->stride)) {
+        fprintf(stderr, "Error on FFMPEG sws_scale stage 2\n");
+        assert(0);
+    }
+    // stage 3: bgr -> gbr planer, no resize
+    if (!sws_scale(sws_context[2], (const uint8_t *const *)image_bgr->planes, image_bgr->stride, 0, image_bgr->height,
+                   gbr_planes, dst->stride)) {
+        fprintf(stderr, "Error on FFMPEG sws_scale stage 3\n");
+        assert(0);
+    }
+
+    /* dump pre-processed image to file */
+    // DumpBGRpToFile(dst);
+}
+
+static void FFPreProcDestroy(PreProcContext *context) {
+    FFPreProc *ff_pre_proc = (FFPreProc *)context->priv;
+
+    for (int i = 0; i < 3; i++) {
+        if (ff_pre_proc->sws_context[i]) {
+            sws_freeContext(ff_pre_proc->sws_context[i]);
+            ff_pre_proc->sws_context[i] = NULL;
+        }
+    }
+    if (ff_pre_proc->image_yuv.planes[0]) {
+        av_freep(&ff_pre_proc->image_yuv.planes[0]);
+        ff_pre_proc->image_yuv.planes[0] = NULL;
+    }
+    if (ff_pre_proc->image_bgr.planes[0]) {
+        av_freep(&ff_pre_proc->image_bgr.planes[0]);
+        ff_pre_proc->image_bgr.planes[0] = NULL;
+    }
+}
+
+PreProc pre_proc_swscale = {
+    .name = "swscale",
+    .priv_size = sizeof(FFPreProc),
+    .mem_type = MEM_TYPE_SYSTEM,
+    .Convert = FFPreProcConvert,
+    .Destroy = FFPreProcDestroy,
+};
+
+#endif
diff --git a/libavfilter/inference_backend/pre_proc_vaapi.c b/libavfilter/inference_backend/pre_proc_vaapi.c
new file mode 100644
index 0000000..84da835
--- /dev/null
+++ b/libavfilter/inference_backend/pre_proc_vaapi.c
@@ -0,0 +1,267 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "pre_proc.h"
+#include <assert.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#if CONFIG_VAAPI
+#include <va/va.h>
+#include <va/va_vpp.h>
+#endif
+
+#if CONFIG_VAAPI
+
+#define VA_CALL(_FUNC)                                                                                                 \
+    do {                                                                                                               \
+        VAStatus _status = _FUNC;                                                                                      \
+        if (_status != VA_STATUS_SUCCESS) {                                                                            \
+            printf(#_FUNC " failed, sts = %d (%s).\n", _status, vaErrorStr(_status));                                  \
+            assert(0);                                                                                                 \
+        }                                                                                                              \
+    } while (0)
+
+typedef struct _VAAPIPreProc {
+    VADisplay display;
+    VAConfigID va_config;
+    VAContextID va_context;
+    VAImageFormat *format_list; //!< Surface formats which can be used with this device.
+    int nb_formats;
+    VAImage va_image;
+    VAImageFormat va_format_selected;
+} VAAPIPreProc;
+
+static uint32_t Fourcc2RTFormat(int format_fourcc) {
+    switch (format_fourcc) {
+#if VA_MAJOR_VERSION >= 1
+    case VA_FOURCC_I420:
+        return VA_FOURCC_I420;
+#endif
+    case VA_FOURCC_NV12:
+        return VA_RT_FORMAT_YUV420;
+    case VA_FOURCC_RGBP:
+        return VA_RT_FORMAT_RGBP;
+    default:
+        return VA_RT_FORMAT_RGB32;
+    }
+}
+
+static VASurfaceID CreateVASurface(VADisplay va_display, const Image *src) {
+    unsigned int rtformat = Fourcc2RTFormat(src->format);
+    VASurfaceID va_surface_id;
+    VASurfaceAttrib surface_attrib;
+    surface_attrib.type = VASurfaceAttribPixelFormat;
+    surface_attrib.flags = VA_SURFACE_ATTRIB_SETTABLE;
+    surface_attrib.value.type = VAGenericValueTypeInteger;
+    surface_attrib.value.value.i = src->format;
+
+    VA_CALL(vaCreateSurfaces(va_display, rtformat, src->width, src->height, &va_surface_id, 1, &surface_attrib, 1));
+    return va_surface_id;
+}
+
+static int VAAPIPreProcInit(PreProcContext *context, PreProcInitParam *param) {
+    VAAPIPreProc *vaapi_pre_proc = (VAAPIPreProc *)context->priv;
+    VADisplay va_display = (VADisplay)param->va_display;
+    VAConfigID va_config = VA_INVALID_ID;
+    VAContextID va_context = VA_INVALID_ID;
+    VAImageFormat *image_list = NULL;
+    int image_count;
+
+    image_count = vaMaxNumImageFormats(va_display);
+    if (image_count <= 0) {
+        return -1;
+    }
+    image_list = malloc(image_count * sizeof(*image_list));
+    if (!image_list) {
+        return -1;
+    }
+
+    VA_CALL(vaQueryImageFormats(va_display, image_list, &image_count));
+    VA_CALL(vaCreateConfig(va_display, VAProfileNone, VAEntrypointVideoProc, NULL, 0, &va_config));
+    VA_CALL(vaCreateContext(va_display, va_config, 0, 0, VA_PROGRESSIVE, 0, 0, &va_context));
+
+    vaapi_pre_proc->display = va_display;
+    vaapi_pre_proc->format_list = image_list;
+    vaapi_pre_proc->nb_formats = image_count;
+    vaapi_pre_proc->va_config = va_config;
+    vaapi_pre_proc->va_context = va_context;
+
+    for (int i = 0; i < vaapi_pre_proc->nb_formats; i++) {
+        if (vaapi_pre_proc->format_list[i].fourcc == VA_FOURCC_RGBP) {
+            vaapi_pre_proc->va_format_selected = vaapi_pre_proc->format_list[i];
+            break;
+        }
+    }
+    return VA_STATUS_SUCCESS;
+}
+
+static void VAAPIPreProcConvert(PreProcContext *context, const Image *src, Image *dst, int bAllocateDestination) {
+    VAAPIPreProc *vaapi_pre_proc = (VAAPIPreProc *)context->priv;
+    VAProcPipelineParameterBuffer pipeline_param = {};
+    VARectangle surface_region = {};
+    VABufferID pipeline_param_buf_id = VA_INVALID_ID;
+
+    VADisplay va_display = vaapi_pre_proc->display;
+    VAContextID va_context = vaapi_pre_proc->va_context;
+    VASurfaceID src_surface = (VASurfaceID)src->surface_id;
+
+    if (dst->type == MEM_TYPE_ANY) {
+        dst->surface_id = CreateVASurface(vaapi_pre_proc->display, dst);
+        dst->va_display = va_display;
+        dst->type = MEM_TYPE_VAAPI;
+    }
+
+    pipeline_param.surface = src_surface;
+    surface_region = (VARectangle){.x = (int16_t)src->rect.x,
+                                   .y = (int16_t)src->rect.y,
+                                   .width = (uint16_t)src->rect.width,
+                                   .height = (uint16_t)src->rect.height};
+    if (surface_region.width > 0 && surface_region.height > 0)
+        pipeline_param.surface_region = &surface_region;
+
+    // pipeline_param.filter_flags = VA_FILTER_SCALING_HQ; // High-quality scaling method
+    pipeline_param.filter_flags = VA_FILTER_SCALING_DEFAULT;
+
+    VA_CALL(vaCreateBuffer(va_display, va_context, VAProcPipelineParameterBufferType, sizeof(pipeline_param), 1,
+                           &pipeline_param, &pipeline_param_buf_id));
+
+    VA_CALL(vaBeginPicture(va_display, va_context, (VASurfaceID)dst->surface_id));
+
+    VA_CALL(vaRenderPicture(va_display, va_context, &pipeline_param_buf_id, 1));
+
+    VA_CALL(vaEndPicture(va_display, va_context));
+
+    VA_CALL(vaDestroyBuffer(va_display, pipeline_param_buf_id));
+}
+
+static void VAAPIPreProcReleaseImage(PreProcContext *context, Image *image) {
+    VAAPIPreProc *vaapi_pre_proc = (VAAPIPreProc *)context->priv;
+
+    if (!vaapi_pre_proc)
+        return;
+
+    if (image->type == MEM_TYPE_VAAPI && image->surface_id && image->surface_id != VA_INVALID_ID) {
+        VA_CALL(vaDestroySurfaces(vaapi_pre_proc->display, (uint32_t *)&image->surface_id, 1));
+        image->type = MEM_TYPE_ANY;
+    }
+}
+
+static void VAAPIPreProcDestroy(PreProcContext *context) {
+    VAAPIPreProc *vaapi_pre_proc = (VAAPIPreProc *)context->priv;
+
+    if (!vaapi_pre_proc)
+        return;
+
+    if (vaapi_pre_proc->va_context != VA_INVALID_ID) {
+        vaDestroyContext(vaapi_pre_proc->display, vaapi_pre_proc->va_context);
+        vaapi_pre_proc->va_context = VA_INVALID_ID;
+    }
+
+    if (vaapi_pre_proc->va_config != VA_INVALID_ID) {
+        vaDestroyConfig(vaapi_pre_proc->display, vaapi_pre_proc->va_config);
+        vaapi_pre_proc->va_config = VA_INVALID_ID;
+    }
+
+    if (vaapi_pre_proc->format_list) {
+        free(vaapi_pre_proc->format_list);
+        vaapi_pre_proc->format_list = NULL;
+    }
+}
+
+typedef struct VAAPIImageMap {
+    VADisplay va_display;
+    VAImage va_image;
+} VAAPIImageMap;
+
+static Image VAAPIMap(ImageMapContext *context, const Image *image) {
+    VAAPIImageMap *m = (VAAPIImageMap *)context->priv;
+    VADisplay va_display = image->va_display;
+    VAImage va_image = {};
+    VAImageFormat va_format = {};
+    void *surface_p = NULL;
+    Image image_sys = {};
+
+    assert(image->type == MEM_TYPE_VAAPI);
+
+    if (image->format == VA_FOURCC_RGBP) {
+        va_format = (VAImageFormat){.fourcc = (uint32_t)image->format,
+                                    .byte_order = VA_LSB_FIRST,
+                                    .bits_per_pixel = 24,
+                                    .depth = 24,
+                                    .red_mask = 0xff0000,
+                                    .green_mask = 0xff00,
+                                    .blue_mask = 0xff,
+                                    .alpha_mask = 0,
+                                    .va_reserved = {}};
+    }
+
+    VA_CALL(vaSyncSurface(va_display, image->surface_id));
+
+    if (va_format.fourcc &&
+        vaCreateImage(va_display, &va_format, image->width, image->height, &va_image) == VA_STATUS_SUCCESS) {
+        VA_CALL(vaGetImage(va_display, image->surface_id, 0, 0, image->width, image->height, va_image.image_id));
+    } else {
+        VA_CALL(vaDeriveImage(va_display, image->surface_id, &va_image));
+    }
+
+    VA_CALL(vaMapBuffer(va_display, va_image.buf, &surface_p));
+
+    image_sys.type = MEM_TYPE_SYSTEM;
+    image_sys.width = image->width;
+    image_sys.height = image->height;
+    image_sys.format = image->format;
+    for (uint32_t i = 0; i < va_image.num_planes; i++) {
+        image_sys.planes[i] = (uint8_t *)surface_p + va_image.offsets[i];
+        image_sys.stride[i] = va_image.pitches[i];
+    }
+
+    m->va_display = va_display;
+    m->va_image = va_image;
+    return image_sys;
+}
+
+static void VAAPIUnmap(ImageMapContext *context) {
+    VAAPIImageMap *m = (VAAPIImageMap *)context->priv;
+    if (m->va_display) {
+        VA_CALL(vaUnmapBuffer(m->va_display, m->va_image.buf));
+        VA_CALL(vaDestroyImage(m->va_display, m->va_image.image_id));
+    }
+}
+
+ImageMap image_map_vaapi = {
+    .name = "vaapi",
+    .priv_size = sizeof(VAAPIImageMap),
+    .Map = VAAPIMap,
+    .Unmap = VAAPIUnmap,
+};
+
+PreProc pre_proc_vaapi = {
+    .name = "vaapi",
+    .priv_size = sizeof(VAAPIPreProc),
+    .mem_type = MEM_TYPE_VAAPI,
+    .Init = VAAPIPreProcInit,
+    .Convert = VAAPIPreProcConvert,
+    .ReleaseImage = VAAPIPreProcReleaseImage,
+    .Destroy = VAAPIPreProcDestroy,
+};
+
+#endif // #if CONFIG_VAAPI
\ No newline at end of file
diff --git a/libavfilter/inference_backend/queue.c b/libavfilter/inference_backend/queue.c
new file mode 100644
index 0000000..263e76b
--- /dev/null
+++ b/libavfilter/inference_backend/queue.c
@@ -0,0 +1,171 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <assert.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+typedef struct _queue_entry queue_entry_t;
+typedef struct _queue queue_t;
+
+struct _queue_entry {
+    void *value;
+    queue_entry_t *prev;
+    queue_entry_t *next;
+    queue_t *queue;
+};
+
+struct _queue {
+    queue_entry_t *head;
+    queue_entry_t *tail;
+    size_t length;
+};
+
+static inline queue_entry_t *create_entry(queue_t *q) {
+    queue_entry_t *new_entry = (queue_entry_t *)calloc(1, sizeof(queue_entry_t));
+
+    assert(new_entry != NULL);
+
+    new_entry->queue = q;
+    return new_entry;
+}
+
+static queue_entry_t *queue_iterate(queue_t *q) {
+    queue_entry_t *it = q->head->next;
+    return it == q->tail ? NULL : it;
+}
+
+static queue_entry_t *queue_iterate_next(queue_t *q, queue_entry_t *it) {
+    queue_entry_t *next = it->next;
+    return next == q->tail ? NULL : next;
+}
+
+static void *queue_iterate_value(queue_entry_t *it) {
+    return it->value;
+}
+
+static queue_t *queue_create(void) {
+    queue_t *q = (queue_t *)malloc(sizeof(queue_t));
+    if (!q)
+        return NULL;
+
+    memset(q, 0, sizeof(queue_t));
+    q->head = create_entry(q);
+    q->tail = create_entry(q);
+    q->head->next = q->tail;
+    q->tail->prev = q->head;
+    q->head->prev = NULL;
+    q->tail->next = NULL;
+
+    return q;
+}
+
+static void queue_destroy(queue_t *q) {
+    queue_entry_t *entry;
+    if (!q)
+        return;
+
+    entry = q->head;
+    while (entry != NULL) {
+        queue_entry_t *temp = entry;
+        entry = entry->next;
+        free(temp);
+    }
+
+    q->head = NULL;
+    q->tail = NULL;
+    q->length = 0;
+    free(q);
+}
+
+static size_t queue_count(queue_t *q) {
+    return q ? q->length : 0;
+}
+
+static void queue_push_front(queue_t *q, void *val) {
+    queue_entry_t *new_node = create_entry(q);
+    queue_entry_t *original_next = q->head->next;
+
+    new_node->value = val;
+
+    q->head->next = new_node;
+    original_next->prev = new_node;
+    new_node->prev = q->head;
+    new_node->next = original_next;
+    q->length++;
+}
+
+static void queue_push_back(queue_t *q, void *val) {
+    queue_entry_t *new_node = create_entry(q);
+    queue_entry_t *original_prev = q->tail->prev;
+
+    new_node->value = val;
+
+    q->tail->prev = new_node;
+    original_prev->next = new_node;
+    new_node->next = q->tail;
+    new_node->prev = original_prev;
+    q->length++;
+}
+
+static void *queue_pop_front(queue_t *q) {
+    queue_entry_t *front = q->head->next;
+    queue_entry_t *new_head_next = front->next;
+    void *ret = front->value;
+
+    if (q->length == 0)
+        return NULL;
+
+    q->head->next = new_head_next;
+    new_head_next->prev = q->head;
+    free(front);
+    q->length--;
+    return ret;
+}
+
+static void *queue_pop_back(queue_t *q) {
+    queue_entry_t *back = q->tail->prev;
+    queue_entry_t *new_tail_prev = back->prev;
+    void *ret = back->value;
+
+    if (q->length == 0)
+        return NULL;
+
+    q->tail->prev = new_tail_prev;
+    new_tail_prev->next = q->tail;
+    free(back);
+    q->length--;
+    return ret;
+}
+
+static void *queue_peek_front(queue_t *q) {
+    if (!q || q->length == 0)
+        return NULL;
+
+    return q->head->next->value;
+}
+
+static void *queue_peek_back(queue_t *q) {
+    if (!q || q->length == 0)
+        return NULL;
+
+    return q->tail->prev->value;
+}
diff --git a/libavfilter/inference_backend/safe_queue.c b/libavfilter/inference_backend/safe_queue.c
new file mode 100644
index 0000000..cd6101d
--- /dev/null
+++ b/libavfilter/inference_backend/safe_queue.c
@@ -0,0 +1,167 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "safe_queue.h"
+#include <assert.h>
+#include <pthread.h>
+#include <stdlib.h>
+
+#include "queue.c"
+
+#define mutex_t pthread_mutex_t
+#define cond_t pthread_cond_t
+
+#define mutex_init(m) pthread_mutex_init((m), NULL)
+
+#define mutex_lock pthread_mutex_lock
+#define mutex_unlock pthread_mutex_unlock
+#define mutex_destroy pthread_mutex_destroy
+
+#define cond_init(c) pthread_cond_init((c), NULL)
+#define cond_signal pthread_cond_signal
+#define cond_broadcast pthread_cond_broadcast
+#define cond_wait pthread_cond_wait
+#define cond_destroy pthread_cond_destroy
+
+struct _SafeQueue {
+    queue_t *q;
+
+    mutex_t mutex;
+    cond_t cond;
+};
+
+SafeQueueT *SafeQueueCreate() {
+    SafeQueueT *sq = (SafeQueueT *)malloc(sizeof(SafeQueueT));
+    if (!sq)
+        return NULL;
+
+    sq->q = queue_create();
+    assert(sq->q);
+
+    mutex_init(&sq->mutex);
+    cond_init(&sq->cond);
+    return sq;
+}
+
+void SafeQueueDestroy(SafeQueueT *sq) {
+    if (!sq)
+        return;
+
+    mutex_lock(&sq->mutex);
+    queue_destroy(sq->q);
+    mutex_unlock(&sq->mutex);
+
+    mutex_destroy(&sq->mutex);
+    cond_destroy(&sq->cond);
+    free(sq);
+}
+
+void SafeQueuePush(SafeQueueT *sq, void *t) {
+    mutex_lock(&sq->mutex);
+    queue_push_back(sq->q, t);
+    cond_signal(&sq->cond);
+    mutex_unlock(&sq->mutex);
+}
+
+void SafeQueuePushFront(SafeQueueT *sq, void *t) {
+    mutex_lock(&sq->mutex);
+    queue_push_front(sq->q, t);
+    cond_signal(&sq->cond);
+    mutex_unlock(&sq->mutex);
+}
+
+void *SafeQueueFront(SafeQueueT *sq) {
+    void *value;
+    mutex_lock(&sq->mutex);
+    while (queue_count(sq->q) == 0) {
+        cond_wait(&sq->cond, &sq->mutex);
+    }
+    value = queue_peek_front(sq->q);
+    mutex_unlock(&sq->mutex);
+    return value;
+}
+
+void *SafeQueuePop(SafeQueueT *sq) {
+    void *value;
+    mutex_lock(&sq->mutex);
+    while (queue_count(sq->q) == 0) {
+        cond_wait(&sq->cond, &sq->mutex);
+    }
+    value = queue_pop_front(sq->q);
+    cond_signal(&sq->cond);
+    mutex_unlock(&sq->mutex);
+    return value;
+}
+
+int SafeQueueSize(SafeQueueT *sq) {
+    int size = 0;
+    mutex_lock(&sq->mutex);
+    size = queue_count(sq->q);
+    mutex_unlock(&sq->mutex);
+    return size;
+}
+
+int SafeQueueEmpty(SafeQueueT *sq) {
+    int empty = 0;
+    mutex_lock(&sq->mutex);
+    empty = (queue_count(sq->q) == 0);
+    mutex_unlock(&sq->mutex);
+    return empty;
+}
+
+void SafeQueueWaitEmpty(SafeQueueT *sq) {
+    mutex_lock(&sq->mutex);
+    while (queue_count(sq->q) != 0) {
+        cond_wait(&sq->cond, &sq->mutex);
+    }
+    mutex_unlock(&sq->mutex);
+}
+
+#if 0
+
+#include <stdio.h>
+
+int main(int argc, char *argv[])
+{
+    SafeQueueT *queue = SafeQueueCreate();
+
+    char TEST[] = {'A', 'B', 'C', 'D'};
+
+    SafeQueuePush(queue, (void *)&TEST[0]);
+    SafeQueuePush(queue, (void *)&TEST[1]);
+    SafeQueuePush(queue, (void *)&TEST[2]);
+    SafeQueuePush(queue, (void *)&TEST[3]);
+
+    SafeQueuePushFront(queue, (void *)&TEST[0]);
+    SafeQueuePushFront(queue, (void *)&TEST[1]);
+    SafeQueuePushFront(queue, (void *)&TEST[2]);
+    SafeQueuePushFront(queue, (void *)&TEST[3]);
+
+    while (!SafeQueueEmpty(queue))
+    {
+        char *c = (char *)SafeQueueFront(queue);
+        printf("%c\n", *c);
+        c = SafeQueuePop(queue);
+    }
+
+    SafeQueueDestroy(queue);
+    return 0;
+}
+#endif
diff --git a/libavfilter/inference_backend/safe_queue.h b/libavfilter/inference_backend/safe_queue.h
new file mode 100644
index 0000000..c6b8d93
--- /dev/null
+++ b/libavfilter/inference_backend/safe_queue.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2018-2019 Intel Corporation
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef __SAFE_QUEUE_H
+#define __SAFE_QUEUE_H
+
+typedef struct _SafeQueue SafeQueueT;
+
+SafeQueueT *SafeQueueCreate(void);
+
+void SafeQueueDestroy(SafeQueueT *sq);
+
+void SafeQueuePush(SafeQueueT *sq, void *t);
+
+void SafeQueuePushFront(SafeQueueT *sq, void *t);
+
+void *SafeQueueFront(SafeQueueT *sq);
+
+void *SafeQueuePop(SafeQueueT *sq);
+
+int SafeQueueEmpty(SafeQueueT *sq);
+
+void SafeQueueWaitEmpty(SafeQueueT *sq);
+
+// Debug only
+int SafeQueueSize(SafeQueueT *sq);
+
+#endif // __SAFE_QUEUE_H
diff --git a/libavfilter/vf_inference_classify.c b/libavfilter/vf_inference_classify.c
index f50db1e..a1b021a 100644
--- a/libavfilter/vf_inference_classify.c
+++ b/libavfilter/vf_inference_classify.c
@@ -18,8 +18,9 @@
 
 /**
  * @file
- * dnn inference classify filter
+ * image inference filter used for object classification
  */
+
 #include "libavutil/opt.h"
 #include "libavutil/mem.h"
 #include "libavutil/eval.h"
@@ -30,300 +31,38 @@
 #include "formats.h"
 #include "internal.h"
 #include "avfilter.h"
+#include "filters.h"
 #include "libavcodec/avcodec.h"
 #include "libavformat/avformat.h"
-#include "libswscale/swscale.h"
+#include "libavutil/time.h"
 
-#include "inference.h"
-#include "dnn_interface.h"
+#include "inference_backend/ff_base_inference.h"
 
-#define OFFSET(x) offsetof(InferenceClassifyContext, x)
+#define OFFSET(x) offsetof(IEClassifyContext, x)
 #define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
 
-#define MAX_MODEL_NUM 8
-
-typedef int (*ClassifyProcess)(AVFilterContext*, int, int, int,
-                               InferTensorMeta*, InferClassificationMeta*);
+static int flush_frame(AVFilterContext *ctx, AVFilterLink *outlink, int64_t pts, int64_t *out_pts);
 
-typedef struct InferenceClassifyContext {
+typedef struct IEClassifyContext {
     const AVClass *class;
 
-    InferenceBaseContext *infer_bases[MAX_MODEL_NUM];
+    FFBaseInference *base;
 
-    char  *model_file;
-    char  *model_proc;
-    char  *vpp_format;
+    FF_INFERENCE_OPTIONS
 
-    int    loaded_num;
+    int    async_preproc;
     int    backend_type;
-    int    device_type;
-
-    int    batch_size;
-    int    frame_number;
-    int    every_nth_frame;
-
-    ClassifyProcess post_process[MAX_MODEL_NUM];
-
-    void *proc_config[MAX_MODEL_NUM];
-    ModelInputPreproc   model_preproc[MAX_MODEL_NUM];
-    ModelOutputPostproc model_postproc[MAX_MODEL_NUM];
-} InferenceClassifyContext;
-
-static void infer_classify_metadata_buffer_free(void *opaque, uint8_t *data)
-{
-    int i;
-    InferClassificationMeta *meta = (InferClassificationMeta *)data;
-    ClassifyArray *classes        = meta->c_array;
-
-    if (classes) {
-        for (i = 0; i < classes->num; i++) {
-            InferClassification *c = classes->classifications[i];
-            av_buffer_unref(&c->label_buf);
-            av_buffer_unref(&c->tensor_buf);
-            av_freep(&c);
-        }
-        av_free(classes->classifications);
-        av_freep(&classes);
-    }
-
-    av_free(data);
-}
-
-static av_cold void dump_softmax(AVFilterContext *ctx, char *name, int label_id,
-                                 float conf, AVBufferRef *label_buf)
-{
-    LabelsArray *array = (LabelsArray *)label_buf->data;
-
-    av_log(ctx, AV_LOG_DEBUG, "CLASSIFY META - Label id:%d %s:%s Conf:%f\n",
-           label_id, name, array->label[label_id], conf);
-}
-
-static av_cold void dump_tensor_value(AVFilterContext *ctx, char *name, float value)
-{
-    av_log(ctx, AV_LOG_DEBUG, "CLASSIFY META - %s:%1.2f\n", name, value);
-}
-
-static void find_max_element_index(const float *array, int len,
-                                   int *index, float *value)
-{
-    int i;
-    *index = 0;
-    *value = array[0];
-    for (i = 1; i < len; i++) {
-        if (array[i] > *value) {
-            *index = i;
-            *value = array[i];
-        }
-    }
-}
-
-static int attributes_to_text(AVFilterContext *ctx,
-                              int detect_id,
-                              OutputPostproc *proc,
-                              InferTensorMeta *meta,
-                              InferClassificationMeta *c_meta)
-{
-    InferClassification *classify;
-    uint32_t method_max, method_compound, method_index;
-    const float *data = (const float *)meta->data;
-
-    method_max      = !strcmp(proc->method, "max");
-    method_compound = !strcmp(proc->method, "compound");
-    method_index    = !strcmp(proc->method, "index");
-
-    if (!data) return -1;
-
-    classify = av_mallocz(sizeof(*classify));
-    if (!classify)
-        return AVERROR(ENOMEM);
-
-    if (method_max) {
-        int    index;
-        float  confidence;
-        size_t n = meta->dims[1];
-
-        find_max_element_index(data, n, &index, &confidence);
-
-        classify->detect_id  = detect_id;
-        classify->name       = proc->attribute_name;
-        classify->label_id   = index;
-        classify->confidence = confidence;
-        classify->label_buf  = av_buffer_ref(proc->labels);
-
-        if (classify->label_buf) {
-            dump_softmax(ctx, classify->name, classify->label_id,
-                         classify->confidence,classify->label_buf);
-        }
-    } else if (method_compound) {
-        int i;
-        double threshold  = 0.5;
-        float  confidence = 0;
-        char attributes[4096] = {};
-        LabelsArray *array;
-
-        if (proc->threshold != 0)
-            threshold = proc->threshold;
-
-        array = (LabelsArray *)proc->labels->data;
-        for (i = 0; i < array->num; i++) {
-            if (data[i] >= threshold)
-                strncat(attributes, array->label[i], (strlen(array->label[i]) + 1));
-            if (data[i] > confidence)
-                confidence = data[i];
-        }
-
-        classify->name = proc->attribute_name;
-        classify->confidence = confidence;
-
-        av_log(ctx, AV_LOG_DEBUG, "Attributes: %s\n", attributes);
-        // TODO: to add into side data
-        av_free(classify);
-        return 0;
-    } else if (method_index) {
-        int i;
-        char attributes[1024] = {};
-        LabelsArray *array;
-
-        array = (LabelsArray *)proc->labels->data;
-        for (i = 0; i < array->num; i++) {
-            int value = data[i];
-            if (value < 0 || value >= array->num)
-                break;
-            strncat(attributes, array->label[value], (strlen(array->label[value]) + 1));
-        }
-
-        classify->name = proc->attribute_name;
-
-        av_log(ctx, AV_LOG_DEBUG, "Attributes: %s\n", attributes);
-        // TODO: to add into side data
-        av_free(classify);
-        return 0;
-    }
-
-    av_dynarray_add(&c_meta->c_array->classifications, &c_meta->c_array->num, classify);
-    return 0;
-}
-
-static int tensor_to_text(AVFilterContext *ctx,
-                          int detect_id,
-                          OutputPostproc *proc,
-                          InferTensorMeta *meta,
-                          InferClassificationMeta *c_meta)
-{
-    InferClassification *classify;
-    const float *data = (const float *)meta->data;
-    double scale = 1.0;
-
-    if (!data) return -1;
-
-    classify = av_mallocz(sizeof(*classify));
-    if (!classify)
-        return AVERROR(ENOMEM);
-
-    if (proc->tensor2text_scale != 0)
-        scale = proc->tensor2text_scale;
-
-    classify->detect_id = detect_id;
-    classify->name      = proc->attribute_name;
-    classify->value     = *data * scale;
-
-    dump_tensor_value(ctx, classify->name, classify->value);
-
-    av_dynarray_add(&c_meta->c_array->classifications, &c_meta->c_array->num, classify);
-    return 0;
-}
-
-static int default_postprocess(AVFilterContext *ctx,
-                               int detect_id,
-                               int result_id,
-                               int model_id,
-                               InferTensorMeta *meta,
-                               InferClassificationMeta *c_meta)
-{
-    InferenceClassifyContext *s = ctx->priv;
-    InferenceBaseContext *base  = s->infer_bases[model_id];
-    DNNModelInfo *info = ff_inference_base_get_output_info(base);
-    InferClassification *classify;
-
-    if (!meta->data) return -1;
-
-    classify = av_mallocz(sizeof(*classify));
-    if (!classify)
-        return AVERROR(ENOMEM);
-
-    classify->detect_id  = detect_id;
-    classify->layer_name = info->layer_name[result_id];
-    classify->model      = s->model_file;
-    classify->name       = (char *)"default";
-
-    classify->tensor_buf = av_buffer_alloc(meta->total_bytes);
-    if (!classify->tensor_buf) {
-        av_free(classify);
-        return AVERROR(ENOMEM);
-    }
-
-    if (meta->total_bytes > 0)
-        memcpy(classify->tensor_buf->data, meta->data, meta->total_bytes);
-
-    av_dynarray_add(&c_meta->c_array->classifications, &c_meta->c_array->num, classify);
-
-    av_log(ctx, AV_LOG_DEBUG, "default output[%s] size: %zu\n", classify->layer_name, meta->total_bytes);
-    return 0;
-}
-
-static int commmon_postprocess(AVFilterContext *ctx,
-                               int detect_id,
-                               int result_id,
-                               int model_id,
-                               InferTensorMeta *meta,
-                               InferClassificationMeta *c_meta)
-{
-    int proc_id;
-    InferenceClassifyContext *s = ctx->priv;
-    InferenceBaseContext *base  = s->infer_bases[model_id];
-
-    OutputPostproc *proc;
-    DNNModelInfo *info = ff_inference_base_get_output_info(base);
-
-    // search model postproc
-    for (proc_id = 0; proc_id < MAX_MODEL_OUTPUT; proc_id++) {
-        char *proc_layer_name = s->model_postproc[model_id].procs[proc_id].layer_name;
-
-        // skip this output process
-        if (!proc_layer_name)
-            continue;
-
-        if (!strcmp(info->layer_name[result_id], proc_layer_name))
-            break;
-    }
-
-    if (proc_id == MAX_MODEL_OUTPUT) {
-        av_log(ctx, AV_LOG_DEBUG, "Could not find proc:%s\n", info->layer_name[result_id]);
-        return 0;
-    }
-
-    proc = &s->model_postproc[model_id].procs[proc_id];
-
-    if (proc->converter == NULL)
-        return default_postprocess(ctx, detect_id, result_id, model_id, meta, c_meta);
-
-    if (!strcmp(proc->converter, "attributes"))
-        return attributes_to_text(ctx, detect_id, proc, meta, c_meta);
-
-    if (!strcmp(proc->converter, "tensor2text"))
-        return tensor_to_text(ctx, detect_id, proc, meta, c_meta);
-
-    return 0;
-}
+    int    already_flushed;
+} IEClassifyContext;
 
 static int query_formats(AVFilterContext *context)
 {
     AVFilterFormats *formats_list;
     const enum AVPixelFormat pixel_formats[] = {
-        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_YUV422P,  AV_PIX_FMT_YUV444P,
-        AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_YUVJ422P, AV_PIX_FMT_YUVJ444P,
-        AV_PIX_FMT_YUV410P,  AV_PIX_FMT_YUV411P,  AV_PIX_FMT_GRAY8,
-        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
+        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_NV12,
+        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,
+        AV_PIX_FMT_BGR0,     AV_PIX_FMT_RGBP,
+        AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
         AV_PIX_FMT_NONE};
 
     formats_list = ff_make_format_list(pixel_formats);
@@ -335,363 +74,261 @@ static int query_formats(AVFilterContext *context)
     return ff_set_common_formats(context, formats_list);
 }
 
-static av_cold int classify_init(AVFilterContext *ctx)
+static int config_input(AVFilterLink *inlink)
 {
-    InferenceClassifyContext *s = ctx->priv;
-    int i, ret;
-    int model_num = 0, model_proc_num = 0;
-    const int max_num = MAX_MODEL_NUM;
-    char *models[MAX_MODEL_NUM] = { };
-    char *models_proc[MAX_MODEL_NUM] = { };
-    InferenceParam p = {};
-
-    av_assert0(s->model_file);
-
-    av_split(s->model_file, "&", models, &model_num, max_num);
-    for (i = 0; i < model_num; i++)
-        av_log(ctx, AV_LOG_INFO, "model[%d]:%s\n", i, models[i]);
-
-    av_split(s->model_proc, "&", models_proc, &model_proc_num, max_num);
-    for (i = 0; i < model_proc_num; i++)
-        av_log(ctx, AV_LOG_INFO, "proc[%d]:%s\n", i, models_proc[i]);
-
-    av_assert0(s->backend_type == DNN_INTEL_IE);
-
-    p.backend_type    = s->backend_type;
-    p.device_type     = s->device_type;
-    p.batch_size      = s->batch_size;
-    p.input_precision = DNN_DATA_PRECISION_U8;
-    p.input_layout    = DNN_DATA_LAYOUT_NCHW;
-    p.input_is_image  = 1;
-
-    for (i = 0; i < model_num; i++) {
-        void *proc;
-        InferenceBaseContext *base = NULL;
-
-        p.model_file = models[i];
-        ret = ff_inference_base_create(ctx, &base, &p);
-        if (ret < 0) {
-            av_log(ctx, AV_LOG_ERROR, "Could not create inference\n");
-            return ret;
-        }
-
-        s->infer_bases[i] = base;
-
-        ff_load_default_model_proc(&s->model_preproc[i], &s->model_postproc[i]);
-
-        if (!models_proc[i])
-            continue;
+    int ret = 0;
+    AVFilterContext *ctx = inlink->dst;
+    IEClassifyContext *s = ctx->priv;
+    const AVPixFmtDescriptor *desc   = av_pix_fmt_desc_get(inlink->format);
+    if (desc == NULL)
+        return AVERROR(EINVAL);
 
-        proc = ff_read_model_proc(models_proc[i]);
-        if (!proc) {
-            av_log(ctx, AV_LOG_ERROR, "Could not read proc config file:"
-                    "%s\n", models_proc[i]);
-            ret = AVERROR(EIO);
-            goto fail;
-        }
+    FFInferenceParam param = { };
 
-        if (ff_parse_input_preproc(proc, &s->model_preproc[i]) < 0) {
-            av_log(ctx, AV_LOG_ERROR, "Parse input preproc error.\n");
-            ret = AVERROR(EIO);
-            goto fail;
-        }
+    av_assert0(s->model);
 
-        if (ff_parse_output_postproc(proc, &s->model_postproc[i]) < 0) {
-            av_log(ctx, AV_LOG_ERROR, "Parse output postproc error.\n");
-            ret = AVERROR(EIO);
-            goto fail;
-        }
+    param.model           = s->model;
+    param.device          = s->device;
+    param.nireq           = s->nireq;
+    param.batch_size      = s->batch_size;
+    param.every_nth_frame = s->every_nth_frame;
+    param.threshold       = s->threshold;
+    param.is_full_frame   = 0;
+    param.infer_config    = s->infer_config;
+    param.model_proc      = s->model_proc;
+    param.opaque          = s->async_preproc ? (void *)MOCKER_PRE_PROC_MAGIC : 0;
 
-        s->proc_config[i] = proc;
+    s->base = av_base_inference_create(ctx->filter->name);
+    if (!s->base) {
+        av_log(ctx, AV_LOG_ERROR, "Could not create inference.\n");
+        return AVERROR(EINVAL);
     }
-    s->loaded_num = model_num;
 
-    for (i = 0; i < model_num; i++) {
-        if (!models_proc[i])
-            s->post_process[i] = &default_postprocess;
-        else
-            s->post_process[i] = &commmon_postprocess;
+    if (desc->flags & AV_PIX_FMT_FLAG_HWACCEL) {
+        AVHWFramesContext *hw_frm_ctx = (AVHWFramesContext *)inlink->hw_frames_ctx->data;
+        AVHWDeviceContext *dev_ctx = (AVHWDeviceContext *)hw_frm_ctx->device_ref->data;
+#if CONFIG_VAAPI
+        param.vpp_device = VPP_DEVICE_HW;
+        param.opaque = (void *)((AVVAAPIDeviceContext *)dev_ctx->hwctx)->display;
+#endif
+        for (int i = 0; i < ctx->nb_outputs; i++) {
+            if (!ctx->outputs[i]->hw_frames_ctx)
+                ctx->outputs[i]->hw_frames_ctx = av_buffer_ref(inlink->hw_frames_ctx);
+        }
     }
 
-    return 0;
-
-fail:
-    for (i = 0; i < model_num; i++) {
-        ff_inference_base_free(&s->infer_bases[i]);
-    }
+    ret = av_base_inference_set_params(s->base, &param);
 
     return ret;
 }
 
-static av_cold void classify_uninit(AVFilterContext *ctx)
+static av_cold int classify_init(AVFilterContext *ctx)
 {
-    int i;
-    InferenceClassifyContext *s = ctx->priv;
-
-    for (i = 0; i < s->loaded_num; i++) {
-        ff_inference_base_free(&s->infer_bases[i]);
-        ff_release_model_proc(s->proc_config[i], &s->model_preproc[i], &s->model_postproc[i]);
-    }
+    /* moved to config_input */
+    return 0;
 }
 
-static int filter_frame(AVFilterLink *inlink, AVFrame *in)
+static av_cold void classify_uninit(AVFilterContext *ctx)
 {
-    int i, ret = 0;
-    AVFilterContext *ctx        = inlink->dst;
-    InferenceClassifyContext *s = ctx->priv;
-    AVFilterLink *outlink       = inlink->dst->outputs[0];
-    AVBufferRef             *ref;
-    AVFrameSideData         *sd, *new_sd;
-    BBoxesArray             *boxes;
-    InferDetectionMeta      *d_meta;
-    ClassifyArray           *c_array = NULL;
-    InferClassificationMeta *c_meta  = NULL;
-
-    if (s->frame_number % s->every_nth_frame != 0)
-        goto done;
-
-    sd = av_frame_get_side_data(in, AV_FRAME_DATA_INFERENCE_DETECTION);
-    if (!sd)
-        goto done;
-
-    d_meta = (InferDetectionMeta *)sd->data;
-    if (!d_meta)
-        goto done;
-
-    boxes = d_meta->bboxes;
-    if (!boxes || !boxes->num)
-        goto done;
-
-    c_meta = av_mallocz(sizeof(*c_meta));
-    c_array = av_mallocz(sizeof(*c_array));
-    if (!c_meta || !c_array) {
-        ret = AVERROR(ENOMEM);
-        goto fail;
-    }
+    IEClassifyContext *s = ctx->priv;
 
-    c_meta->c_array = c_array;
-
-    // handle according to detected metadata one by one
-    for (i = 0; i < boxes->num; i++) {
-        int j;
-        InferDetection *bbox = boxes->bbox[i];
-
-        // process for each model
-        for (j = 0; j < s->loaded_num; j++) {
-            int output;
-            InferenceBaseContext *base = s->infer_bases[j];
-            ModelInputPreproc *preproc = &s->model_preproc[j];
-
-            VideoPP *vpp        = ff_inference_base_get_vpp(base);
-            AVFrame *tmp        = vpp->frames[0];
-            DNNModelInfo *iinfo = ff_inference_base_get_input_info(base);
-            DNNModelInfo *oinfo = ff_inference_base_get_output_info(base);
-            int scale_width     = iinfo->dims[0][0];
-            int scale_height    = iinfo->dims[0][1];
-
-            Rect crop_rect = (Rect) {
-                .x0 = bbox->x_min * in->width,
-                .y0 = bbox->y_min * in->height,
-                .x1 = bbox->x_max * in->width,
-                .y1 = bbox->y_max * in->height,
-            };
-
-            // care interested object class only
-            if (preproc && preproc->object_class && bbox->label_buf) {
-                LabelsArray *array = (LabelsArray *)bbox->label_buf->data;
-                if (bbox->label_id >= array->num) {
-                    av_log(NULL, AV_LOG_ERROR, "The json file must match the input model\n");
-                    ret = AVERROR(ERANGE);
-                    goto fail;
-                }
-                if (0 != strcmp(preproc->object_class, array->label[bbox->label_id]))
-                    continue;
-            }
+    flush_frame(ctx, NULL, 0LL, NULL);
 
-            if (vpp->device == VPP_DEVICE_SW) {
-                ret = vpp->sw_vpp->crop_and_scale(in, &crop_rect,
-                        scale_width, scale_height,
-                        vpp->expect_format, tmp->data, tmp->linesize);
-            } else {
-#if CONFIG_VAAPI
-                ret = vpp->va_vpp->crop_and_scale(vpp->va_vpp, in, &crop_rect,
-                        scale_width, scale_height, tmp->data, tmp->linesize);
-#endif
-            }
-            if (ret != 0) {
-                ret = AVERROR(EINVAL);
-                goto fail;
-            }
-
-            // TODO: support dynamic batch for faces
-            ff_inference_base_submit_frame(base, tmp, 0, 0);
-            ff_inference_base_infer(base);
-
-            for (output = 0; output < oinfo->number; output++) {
-                InferTensorMeta tensor_meta = { };
-                ff_inference_base_get_infer_result(base, output, &tensor_meta);
+    av_base_inference_release(s->base);
+}
 
-                if (s->post_process[j])
-                    s->post_process[j](ctx, i, output, j, &tensor_meta, c_meta);
+static int flush_frame(AVFilterContext *ctx, AVFilterLink *outlink, int64_t pts, int64_t *out_pts)
+{
+    int ret = 0;
+    IEClassifyContext *s = ctx->priv;
+
+    if (s->already_flushed)
+        return ret;
+
+    while (!av_base_inference_frame_queue_empty(ctx, s->base)) {
+        AVFrame *output = NULL;
+        av_base_inference_get_frame(ctx, s->base, &output);
+        if (output) {
+            if (outlink) {
+                ret = ff_filter_frame(outlink, output);
+                if (out_pts)
+                    *out_pts = output->pts + pts;
+            } else {
+                av_frame_free(&output);
             }
         }
-    }
 
-    ref = av_buffer_create((uint8_t *)c_meta, sizeof(*c_meta),
-                           &infer_classify_metadata_buffer_free, NULL, 0);
-    if (!ref)
-        return AVERROR(ENOMEM);
-
-    // add meta data to side data
-    new_sd = av_frame_new_side_data_from_buf(in, AV_FRAME_DATA_INFERENCE_CLASSIFICATION, ref);
-    if (!new_sd) {
-        av_buffer_unref(&ref);
-        av_log(NULL, AV_LOG_ERROR, "Could not add new side data\n");
-        return AVERROR(ENOMEM);
+        av_base_inference_send_event(ctx, s->base, INFERENCE_EVENT_EOS);
+        av_usleep(5000);
     }
 
-done:
-    s->frame_number++;
-    return ff_filter_frame(outlink, in);
-fail:
-    if (c_array)
-        av_freep(&c_array);
-    if (c_meta)
-        av_freep(&c_meta);
-    av_frame_free(&in);
+    s->already_flushed = 1;
     return ret;
 }
 
-static av_cold int config_input(AVFilterLink *inlink)
-{
-    int i, ret;
-    AVFrame *frame;
-
-    AVFilterContext             *ctx = inlink->dst;
-    InferenceClassifyContext      *s = ctx->priv;
-    enum AVPixelFormat expect_format = AV_PIX_FMT_BGR24;
-    const AVPixFmtDescriptor   *desc = av_pix_fmt_desc_get(inlink->format);
-
-    if (!desc)
-        return AVERROR(EINVAL);
-
-    for (i = 0; i < s->loaded_num; i++) {
-        InferenceBaseContext *base = s->infer_bases[i];
-        DNNModelInfo         *info = ff_inference_base_get_input_info(base);
-        VideoPP               *vpp = ff_inference_base_get_vpp(base);
-
-        int input_width  = info->dims[0][0];
-        int input_height = info->dims[0][1];
+static int classify_num_of_roi(AVFrame *frame) {
+    int roi_num = 0;
+    BBoxesArray *bboxes = NULL;
+    InferDetectionMeta *detect_meta = NULL;
+    AVFrameSideData *side_data;
 
-        // right now, no model needs multiple inputs
-        // av_assert0(info->number == 1);
-
-        ff_inference_dump_model_info(ctx, info);
+    if (!frame)
+        return 0;
 
-        vpp->device = (desc->flags & AV_PIX_FMT_FLAG_HWACCEL) ?
-            VPP_DEVICE_HW : VPP_DEVICE_SW;
+    side_data = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_DETECTION);
+    if (side_data) {
+        detect_meta = (InferDetectionMeta *)(side_data->data);
+        av_assert0(detect_meta);
+        bboxes = detect_meta->bboxes;
+        roi_num = bboxes ? bboxes->num : 0;
+    }
 
-        // allocate avframes to save preprocessed data
-        frame = av_frame_alloc();
-        if (!frame)
-            return AVERROR(ENOMEM);
-        frame->width   = input_width;
-        frame->height  = input_height;
-        frame->format  = expect_format;
-        vpp->frames[0] = frame;
+    return roi_num;
+}
 
-        if (vpp->device == VPP_DEVICE_SW) {
-            ret = av_frame_get_buffer(frame, 0);
-            if (ret < 0)
-                goto fail;
-        } else {
-#if CONFIG_VAAPI
-            vpp->va_vpp = av_mallocz(sizeof(*vpp->va_vpp));
-            if (!vpp->va_vpp) {
-                ret = AVERROR(ENOMEM);
-                goto fail;
-            }
+static int load_balance(AVFilterContext *ctx)
+{
+    AVFilterLink *inlink = ctx->inputs[0];
+    AVFilterLink *outlink = ctx->outputs[0];
+    IEClassifyContext *s = ctx->priv;
+    AVFrame *in = NULL, *output = NULL;
+    int64_t pts;
+    int i, ret, status, idx;
+    int needed, resource, prepared, got_frames = 0;
+
+    FF_FILTER_FORWARD_STATUS_BACK(outlink, inlink);
+
+    while (av_base_inference_get_frame(ctx, s->base, &output) == 0) {
+        if (output) {
+            status = ff_filter_frame(outlink, output);
+            if (status < 0)
+                return status;
+            got_frames = 1;
+            output = NULL;
+        }
+    }
 
-            ret = va_vpp_device_create(vpp->va_vpp, inlink);
-            if (ret < 0) {
-                av_log(ctx, AV_LOG_ERROR, "Create va vpp device failed\n");
-                ret = AVERROR(EINVAL);
-                goto fail;
+    status = ff_outlink_get_status(inlink);
+    if (status)
+        needed = ff_inlink_queued_frames(inlink);
+    else {
+        needed = 0;
+        prepared = 0;
+        idx = ff_inlink_queued_frames(inlink);
+        resource = av_base_inference_resource_status(ctx, s->base);
+        for (i = 0; i < idx; i++) {
+            int num_roi;
+            in = ff_inlink_peek_frame(inlink, i);
+            num_roi = classify_num_of_roi(in);
+            if (num_roi > s->nireq * s->batch_size) {
+                needed = resource < s->nireq ? 0 : 1;
+                break;
             }
-
-            ret = va_vpp_surface_alloc(vpp->va_vpp,
-                    input_width, input_height, s->vpp_format);
-            if (ret < 0) {
-                av_log(ctx, AV_LOG_ERROR, "Create va surface failed\n");
-                ret = AVERROR(EINVAL);
-                goto fail;
+            else {
+                if (num_roi == 0 || (resource - prepared) >= num_roi) {
+                    needed++;
+                    prepared += num_roi;
+                    continue;
+                } else
+                    break;
             }
-
-            frame->format = vpp->va_vpp->av_format;
-#endif
         }
     }
 
-    return 0;
-fail:
-    for (i = 0; i < s->loaded_num; i++) {
-        VideoPP *vpp = ff_inference_base_get_vpp(s->infer_bases[i]);
+    while (needed > 0) {
+        ret = ff_inlink_consume_frame(inlink, &in);
+        if (ret < 0)
+            return ret;
+        if (ret > 0) {
+            av_base_inference_send_frame(ctx, s->base, in);
+        }
+        needed--;
+    }
 
-        frame = vpp->frames[0];
-        if (!frame)
-            continue;
+    if (!status && got_frames)
+        return 0;
 
-        av_frame_free(&frame);
+    if (ff_inlink_acknowledge_status(inlink, &status, &pts)) {
+        if (status == AVERROR_EOF) {
+            int64_t out_pts = pts;
 
-#if CONFIG_VAAPI
-        if (vpp->va_vpp) {
-            va_vpp_device_free(vpp->va_vpp);
-            av_freep(&vpp->va_vpp);
+            av_log(ctx, AV_LOG_INFO, "Get EOS.\n");
+            ret = flush_frame(ctx, outlink, pts, &out_pts);
+            ff_outlink_set_status(outlink, status, out_pts);
+            return ret;
         }
-#endif
     }
-    return ret;
+
+    FF_FILTER_FORWARD_WANTED(outlink, inlink);
+
+    return FFERROR_NOT_READY;
 }
 
-static av_cold int config_output(AVFilterLink *outlink)
+static int activate(AVFilterContext *ctx)
 {
-    int i;
-    AVFilterContext        *ctx = outlink->src;
-    InferenceClassifyContext *s = ctx->priv;
+    AVFilterLink *inlink = ctx->inputs[0];
+    AVFilterLink *outlink = ctx->outputs[0];
+    IEClassifyContext *s = ctx->priv;
+    AVFrame *in = NULL, *output = NULL;
+    int64_t pts;
+    int ret, status;
+    int got_frames = 0;
+
+    if (av_load_balance_get())
+        return load_balance(ctx);
+
+    FF_FILTER_FORWARD_STATUS_BACK(outlink, inlink);
+
+    // drain all frames from inlink
+    do {
+        ret = ff_inlink_consume_frame(inlink, &in);
+        if (ret < 0)
+            return ret;
+        if (ret > 0)
+            av_base_inference_send_frame(ctx, s->base, in);
+
+        while (av_base_inference_get_frame(ctx, s->base, &output) == 0) {
+            if (output) {
+                status = ff_filter_frame(outlink, output);
+                if (status < 0)
+                    return status;
+                got_frames = 1;
+                output = NULL;
+            }
+        }
+    } while (ret > 0);
 
-    for (i = 0; i < s->loaded_num; i++) {
-        InferenceBaseContext *base = s->infer_bases[i];
-        DNNModelInfo *info = ff_inference_base_get_output_info(base);
-        ff_inference_dump_model_info(ctx, info);
+    if (got_frames)
+        return 0;
 
-#if CONFIG_VAAPI
-        if (!outlink->hw_frames_ctx) {
-            VideoPP *vpp = ff_inference_base_get_vpp(base);
-            if (vpp->device == VPP_DEVICE_HW) {
-                if (!vpp->va_vpp || !vpp->va_vpp->hw_frames_ref) {
-                    av_log(ctx, AV_LOG_ERROR, "The input must have a hardware frame "
-                            "reference.\n");
-                    return AVERROR(EINVAL);
-                }
-                outlink->hw_frames_ctx = av_buffer_ref(vpp->va_vpp->hw_frames_ref);
-                if (!outlink->hw_frames_ctx)
-                    return AVERROR(ENOMEM);
-            }
+    if (ff_inlink_acknowledge_status(inlink, &status, &pts)) {
+        if (status == AVERROR_EOF) {
+            int64_t out_pts = pts;
+
+            av_log(ctx, AV_LOG_INFO, "Get EOS.\n");
+            ret = flush_frame(ctx, outlink, pts, &out_pts);
+            ff_outlink_set_status(outlink, status, out_pts);
+            return ret;
         }
-#endif
     }
 
-    return 0;
+    FF_FILTER_FORWARD_WANTED(outlink, inlink);
+
+    return FFERROR_NOT_READY;
 }
 
 static const AVOption inference_classify_options[] = {
-    { "dnn_backend",    "DNN backend for model execution", OFFSET(backend_type),    AV_OPT_TYPE_FLAGS,  { .i64 = DNN_INTEL_IE },          0, 2,    FLAGS, "engine" },
-    { "model",          "path to model files for network", OFFSET(model_file),      AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,    FLAGS },
-    { "model_proc",     "model preproc and postproc",      OFFSET(model_proc),      AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,    FLAGS },
-    { "vpp_format",     "specify vpp output format",       OFFSET(vpp_format),      AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,    FLAGS },
-    { "device",         "running on device type",          OFFSET(device_type),     AV_OPT_TYPE_FLAGS,  { .i64 = DNN_TARGET_DEVICE_CPU }, 0, 12,   FLAGS },
-    { "interval",       "do infer every Nth frame",        OFFSET(every_nth_frame), AV_OPT_TYPE_INT,    { .i64 = 1 },                     1, 1024, FLAGS },
-    { "batch_size",     "batch size per infer",            OFFSET(batch_size),      AV_OPT_TYPE_INT,    { .i64 = 1 },                     1, 1024, FLAGS },
+    { "dnn_backend",  "DNN backend for model execution", OFFSET(backend_type),    AV_OPT_TYPE_FLAGS,  { .i64 = 1},          0, 2,  FLAGS },
+    { "model",        "path to model file for network",  OFFSET(model),           AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "model_proc",   "model preproc and postproc",      OFFSET(model_proc),      AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "object_class", "objective class",                 OFFSET(object_class),    AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "device",       "running on device name",          OFFSET(device),          AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "configs",      "configurations to backend",       OFFSET(infer_config),    AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "interval",     "detect every Nth frame",          OFFSET(every_nth_frame), AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 1024, FLAGS},
+    { "nireq",        "inference request number",        OFFSET(nireq),           AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 128,  FLAGS},
+    { "batch_size",   "batch size per infer",            OFFSET(batch_size),      AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 1000, FLAGS},
+    { "threshold",    "threshod to filter output data",  OFFSET(threshold),       AV_OPT_TYPE_FLOAT,  { .dbl = 0.5}, 0, 1,    FLAGS},
+    { "async_preproc", "do asynchronous preproc in inference backend", OFFSET(async_preproc), AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, FLAGS },
     { NULL }
 };
 
@@ -702,7 +339,6 @@ static const AVFilterPad classify_inputs[] = {
         .name          = "default",
         .type          = AVMEDIA_TYPE_VIDEO,
         .config_props  = config_input,
-        .filter_frame  = filter_frame,
     },
     { NULL }
 };
@@ -711,16 +347,16 @@ static const AVFilterPad classify_outputs[] = {
     {
         .name          = "default",
         .type          = AVMEDIA_TYPE_VIDEO,
-        .config_props  = config_output,
     },
     { NULL }
 };
 
 AVFilter ff_vf_inference_classify = {
     .name          = "classify",
-    .description   = NULL_IF_CONFIG_SMALL("DNN Inference classification."),
-    .priv_size     = sizeof(InferenceClassifyContext),
+    .description   = NULL_IF_CONFIG_SMALL("Image Inference classify filter."),
+    .priv_size     = sizeof(IEClassifyContext),
     .query_formats = query_formats,
+    .activate      = activate,
     .init          = classify_init,
     .uninit        = classify_uninit,
     .inputs        = classify_inputs,
diff --git a/libavfilter/vf_inference_detect.c b/libavfilter/vf_inference_detect.c
index 8e03595..140c891 100644
--- a/libavfilter/vf_inference_detect.c
+++ b/libavfilter/vf_inference_detect.c
@@ -18,7 +18,7 @@
 
 /**
  * @file
- * dnn inference detection filter
+ * image inference filter used for object detection
  */
 
 #include "libavutil/opt.h"
@@ -31,183 +31,40 @@
 #include "formats.h"
 #include "internal.h"
 #include "avfilter.h"
+#include "filters.h"
 #include "libavcodec/avcodec.h"
 #include "libavformat/avformat.h"
-#include "libswscale/swscale.h"
+#include "libavutil/time.h"
 
-#include "inference.h"
-#include "dnn_interface.h"
+#include "inference_backend/ff_base_inference.h"
 
-#define OFFSET(x) offsetof(InferenceDetectContext, x)
+#define OFFSET(x) offsetof(IEDetectContext, x)
 #define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
 
-#define FUNC_ENTRY() printf("enter >>> %s\n", __FUNCTION__);
-#define FUNC_EXIT()  printf("exit  <<< %s\n", __FUNCTION__);
+static int flush_frame(AVFilterContext *ctx, AVFilterLink *outlink, int64_t pts, int64_t *out_pts);
 
-typedef struct InferenceDetectContext {
-    const AVClass *class;
-
-    InferenceBaseContext *base;
-
-    char  *model_file;
-    char  *vpp_format;
-    char  *model_proc;
-    int    backend_type;
-    int    device_type;
-
-    int    batch_size;
-    int    frame_number;
-    int    every_nth_frame;
-    int    max_count;
-    float  threshold;
-
-    int    input_layout;
-    int    input_precision;
-    int    input_is_image;
-
-    void  *proc_config;
-
-    ModelInputPreproc   model_preproc;
-    ModelOutputPostproc model_postproc;
-} InferenceDetectContext;
-
-static void infer_detect_metadata_buffer_free(void *opaque, uint8_t *data)
-{
-    BBoxesArray *bboxes = ((InferDetectionMeta *)data)->bboxes;
-
-    if (bboxes) {
-        int i;
-        for (i = 0; i < bboxes->num; i++) {
-            InferDetection *p = bboxes->bbox[i];
-            if (p->label_buf)
-                av_buffer_unref(&p->label_buf);
-            av_freep(&p);
-        }
-        av_free(bboxes->bbox);
-        av_freep(&bboxes);
-    }
-
-    av_free(data);
-}
-
-static int detect_postprocess(AVFilterContext *ctx, InferTensorMeta *meta, AVFrame *frame)
-{
-    int i;
-    InferenceDetectContext *s = ctx->priv;
-    int object_size           = meta->dims[3];
-    int max_proposal_count    = meta->dims[2];
-    const float *detection    = (float *)meta->data;
-    AVBufferRef *ref;
-    AVFrameSideData *sd;
-    InferDetectionMeta *detect_meta = NULL;
-
-    BBoxesArray *boxes        = av_mallocz(sizeof(*boxes));
-    if (!boxes)
-        return AVERROR(ENOMEM);
-
-    detect_meta = av_malloc(sizeof(*detect_meta));
-    if (!detect_meta) {
-        av_free(boxes);
-        return AVERROR(ENOMEM);
-    }
-
-    // FIXME: output object size standard??
-    av_assert0(object_size == 7);
-
-    av_assert0(meta->precision == DNN_DATA_PRECISION_FP32);
-
-    av_assert0(meta->total_bytes >= max_proposal_count * object_size * sizeof(float));
-
-    for (i = 0; i < max_proposal_count; i++) {
-        InferDetection *new_bbox = av_mallocz(sizeof(*new_bbox));
-        if (!new_bbox) {
-            av_log(ctx, AV_LOG_ERROR, "Could not alloc bbox!\n");
-            break;
-        }
-
-        new_bbox->label_id   = (int)detection[i * object_size + 1];
-        new_bbox->confidence = detection[i * object_size + 2];
-        new_bbox->x_min      = detection[i * object_size + 3];
-        new_bbox->y_min      = detection[i * object_size + 4];
-        new_bbox->x_max      = detection[i * object_size + 5];
-        new_bbox->y_max      = detection[i * object_size + 6];
-
-        if (new_bbox->confidence < s->threshold) {
-            av_freep(&new_bbox);
-            continue;
-        }
-
-        // TODO: use layer name to get proc
-        if (s->model_postproc.procs[0].labels)
-            new_bbox->label_buf = av_buffer_ref(s->model_postproc.procs[0].labels);
 
-        av_dynarray_add(&boxes->bbox, &boxes->num, new_bbox);
-
-        if (boxes->num >= s->max_count)
-            break;
-    }
-
-    // dump face detected meta
-    for (i = 0; i < boxes->num; i++) {
-        InferDetection *p = boxes->bbox[i];
-        av_log(ctx, AV_LOG_DEBUG,
-               "DETECT META - label:%d confi:%f coord:%f %f %f %f\n",
-               p->label_id, p->confidence,p->x_min, p->y_min, p->x_max, p->y_max);
-    }
-
-    detect_meta->bboxes = boxes;
-
-    ref = av_buffer_create((uint8_t *)detect_meta, sizeof(*detect_meta),
-                           &infer_detect_metadata_buffer_free, NULL, 0);
-    if (!ref) {
-        infer_detect_metadata_buffer_free(NULL, (uint8_t *)detect_meta);
-        return AVERROR(ENOMEM);
-    }
+typedef struct IEDetectContext {
+    const AVClass *class;
 
-    // add meta data to side data
-    sd = av_frame_new_side_data_from_buf(frame, AV_FRAME_DATA_INFERENCE_DETECTION, ref);
-    if (!sd) {
-        av_buffer_unref(&ref);
-        av_log(ctx, AV_LOG_ERROR, "Could not add new side data\n");
-        return AVERROR(ENOMEM);
-    }
+    FFBaseInference *base;
 
-    return 0;
-}
+    FF_INFERENCE_OPTIONS
 
-static int detect_preprocess(InferenceBaseContext *base, int index, AVFrame *in, AVFrame **out)
-{
-    int ret;
-    VideoPP *vpp = ff_inference_base_get_vpp(base);
-    AVFrame *tmp = vpp->frames[index];
-
-    if (vpp->device == VPP_DEVICE_SW) {
-        if (!vpp->sw_vpp->scale_contexts[index]) {
-            *out = in;
-            return 0;
-        }
-
-        ret = vpp->sw_vpp->scale(vpp->sw_vpp->scale_contexts[index],
-                (const uint8_t * const*)in->data,
-                in->linesize, 0, in->height, tmp->data, tmp->linesize);
-    } else {
-#if CONFIG_VAAPI
-        ret = vpp->va_vpp->scale(vpp->va_vpp, in,
-                tmp->width, tmp->height, tmp->data, tmp->linesize);
-#endif
-    }
-    *out = tmp;
-    return ret;
-}
+    int    async_preproc;
+    int    backend_type;
+    int    already_flushed;
+    char  *crop_params;
+} IEDetectContext;
 
 static int query_formats(AVFilterContext *context)
 {
     AVFilterFormats *formats_list;
     const enum AVPixelFormat pixel_formats[] = {
-        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_YUV422P,  AV_PIX_FMT_YUV444P,
-        AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_YUVJ422P, AV_PIX_FMT_YUVJ444P,
-        AV_PIX_FMT_YUV410P,  AV_PIX_FMT_YUV411P,  AV_PIX_FMT_GRAY8,
-        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
+        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_NV12,
+        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,
+        AV_PIX_FMT_BGR0,     AV_PIX_FMT_RGBP,
+        AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
         AV_PIX_FMT_NONE};
 
     formats_list = ff_make_format_list(pixel_formats);
@@ -221,221 +78,242 @@ static int query_formats(AVFilterContext *context)
 
 static int config_input(AVFilterLink *inlink)
 {
-    int ret;
-    AVFrame *frame;
-    AVFilterContext      *ctx        = inlink->dst;
-    InferenceDetectContext *s        = ctx->priv;
-    enum AVPixelFormat expect_format = AV_PIX_FMT_BGR24;
-
+    int ret = 0;
+    AVFilterContext *ctx = inlink->dst;
+    IEDetectContext *s = ctx->priv;
     const AVPixFmtDescriptor *desc   = av_pix_fmt_desc_get(inlink->format);
-    DNNModelInfo *info               = ff_inference_base_get_input_info(s->base);
-    VideoPP *vpp                     = ff_inference_base_get_vpp(s->base);
-
-    int width = info->dims[0][0], height = info->dims[0][1];
-
-    ff_inference_dump_model_info(ctx, info);
-
-    // right now, no model needs multiple inputs
-    av_assert0(info->number == 1);
-
-    if (!desc)
+    if (desc == NULL)
         return AVERROR(EINVAL);
-    vpp->device = (desc->flags & AV_PIX_FMT_FLAG_HWACCEL) ? VPP_DEVICE_HW : VPP_DEVICE_SW;
 
-    // allocate frame to save scaled output
-    frame = av_frame_alloc();
-    if (!frame)
-        return AVERROR(ENOMEM);
-    frame->width   = width;
-    frame->height  = height;
-    frame->format  = expect_format;
-    vpp->frames[0] = frame;
-
-    if (vpp->device == VPP_DEVICE_SW) {
-        int need_scale = expect_format != inlink->format ||
-                         width         != inlink->w      ||
-                         height        != inlink->h;
-
-        if (need_scale) {
-            if (av_frame_get_buffer(frame, 0) < 0) {
-                av_frame_free(&frame);
-                return AVERROR(ENOMEM);
-            }
-
-            vpp->sw_vpp->scale_contexts[0] = sws_getContext(
-                    inlink->w, inlink->h, inlink->format,
-                    width, height, expect_format,
-                    SWS_BILINEAR, NULL, NULL, NULL);
-
-            if (!vpp->sw_vpp->scale_contexts[0]) {
-                av_log(ctx, AV_LOG_ERROR, "Impossible to create scale context\n");
-                av_frame_free(&frame);
-                return AVERROR(EINVAL);
-            }
-        }
-    } else {
-#if CONFIG_VAAPI
-        vpp->va_vpp = av_mallocz(sizeof(*vpp->va_vpp));
-        if (!vpp->va_vpp) {
-            ret = AVERROR(ENOMEM);
-            goto fail;
-        }
-
-        ret = va_vpp_device_create(vpp->va_vpp, inlink);
-        if (ret < 0) {
-            av_log(ctx, AV_LOG_ERROR, "Create va vpp device failed\n");
-            ret = AVERROR(EINVAL);
-            goto fail;
-        }
-
-        ret = va_vpp_surface_alloc(vpp->va_vpp, width, height, s->vpp_format);
-        if (ret < 0) {
-            av_log(ctx, AV_LOG_ERROR, "Create va surface failed\n");
-            ret = AVERROR(EINVAL);
-            goto fail;
+    FFInferenceParam param = { };
+
+    av_assert0(s->model);
+
+    param.model           = s->model;
+    param.device          = s->device;
+    param.nireq           = s->nireq;
+    param.batch_size      = s->batch_size;
+    param.every_nth_frame = s->every_nth_frame;
+    param.threshold       = s->threshold;
+    param.is_full_frame   = 1;
+    param.infer_config    = s->infer_config;
+    param.model_proc      = s->model_proc;
+    param.opaque          = s->async_preproc ? (void *)MOCKER_PRE_PROC_MAGIC : 0;
+
+    if (s->crop_params) {
+        sscanf(s->crop_params, "%d|%d|%d|%d",
+                &param.crop_rect.x, &param.crop_rect.y, &param.crop_rect.width, &param.crop_rect.height);
+        if (param.crop_rect.x < 0         || param.crop_rect.y < 0      ||
+                param.crop_rect.width < 0 || param.crop_rect.height < 0 ||
+                param.crop_rect.width  + param.crop_rect.x > inlink->w  ||
+                param.crop_rect.height + param.crop_rect.y > inlink->h) {
+            av_log(ctx, AV_LOG_ERROR, "Invalid cropping parameters.\n");
+            return AVERROR(EINVAL);
         }
+    }
 
-        frame->format = vpp->va_vpp->av_format;
-#endif
+    s->base = av_base_inference_create(ctx->filter->name);
+    if (!s->base) {
+        av_log(ctx, AV_LOG_ERROR, "Could not create inference.\n");
+        return AVERROR(EINVAL);
     }
 
-    return 0;
-fail:
-    av_frame_free(&frame);
+    if (desc->flags & AV_PIX_FMT_FLAG_HWACCEL) {
+        AVHWFramesContext *hw_frm_ctx = (AVHWFramesContext *)inlink->hw_frames_ctx->data;
+        AVHWDeviceContext *dev_ctx = (AVHWDeviceContext *)hw_frm_ctx->device_ref->data;
 #if CONFIG_VAAPI
-    if (vpp->va_vpp) {
-        va_vpp_device_free(vpp->va_vpp);
-        av_freep(&vpp->va_vpp);
-    }
+        param.vpp_device = VPP_DEVICE_HW;
+        param.opaque = (void *)((AVVAAPIDeviceContext *)dev_ctx->hwctx)->display;
 #endif
+        for (int i = 0; i < ctx->nb_outputs; i++) {
+            if (!ctx->outputs[i]->hw_frames_ctx)
+                ctx->outputs[i]->hw_frames_ctx = av_buffer_ref(inlink->hw_frames_ctx);
+        }
+    }
+
+    ret = av_base_inference_set_params(s->base, &param);
+
     return ret;
 }
 
-static int config_output(AVFilterLink *outlink)
+static av_cold int detect_init(AVFilterContext *ctx)
 {
-    AVFilterContext      *ctx = outlink->src;
-    InferenceDetectContext *s = ctx->priv;
-    VideoPP *vpp              = ff_inference_base_get_vpp(s->base);
-
-    DNNModelInfo *info = ff_inference_base_get_output_info(s->base);
+    /* moved to config_input */
+    return 0;
+}
 
-    ff_inference_dump_model_info(ctx, info);
+static av_cold void detect_uninit(AVFilterContext *ctx)
+{
+    IEDetectContext *s = ctx->priv;
 
-#if CONFIG_VAAPI
-    if (vpp->device == VPP_DEVICE_HW) {
-        if (!vpp->va_vpp || !vpp->va_vpp->hw_frames_ref) {
-            av_log(ctx, AV_LOG_ERROR, "The input must have a hardware frame "
-                    "reference.\n");
-            return AVERROR(EINVAL);
-        }
-        outlink->hw_frames_ctx = av_buffer_ref(vpp->va_vpp->hw_frames_ref);
-        if (!outlink->hw_frames_ctx)
-            return AVERROR(ENOMEM);
-    }
-#endif
+    flush_frame(ctx, NULL, 0LL, NULL);
 
-    return 0;
+    av_base_inference_release(s->base);
 }
 
-static av_cold int detect_init(AVFilterContext *ctx)
+static int flush_frame(AVFilterContext *ctx, AVFilterLink *outlink, int64_t pts, int64_t *out_pts)
 {
-    int ret;
-    InferenceDetectContext *s = ctx->priv;
-    InferenceParam p = {};
+    int ret = 0;
+    IEDetectContext *s = ctx->priv;
 
-    av_assert0(s->model_file);
+    if (s->already_flushed)
+        return ret;
 
-    av_assert0(s->backend_type == DNN_INTEL_IE);
+    while (!av_base_inference_frame_queue_empty(ctx, s->base)) {
+        AVFrame *output = NULL;
+        av_base_inference_get_frame(ctx, s->base, &output);
+        if (output) {
+            if (outlink) {
+                ret = ff_filter_frame(outlink, output);
+                if (out_pts)
+                    *out_pts = output->pts + pts;
+            } else {
+                av_frame_free(&output);
+            }
+        }
 
-    ff_load_default_model_proc(&s->model_preproc, &s->model_postproc);
+        av_base_inference_send_event(ctx, s->base, INFERENCE_EVENT_EOS);
+        av_usleep(5000);
+    }
 
-    if (s->model_proc) {
-        void *proc = ff_read_model_proc(s->model_proc);
-        if (!proc) {
-            av_log(ctx, AV_LOG_ERROR, "Could not read proc config file:"
-                    "%s\n", s->model_proc);
-            return AVERROR(EIO);
-        }
+    s->already_flushed = 1;
+    return ret;
+}
 
-        if (ff_parse_input_preproc(proc, &s->model_preproc) < 0) {
-            av_log(ctx, AV_LOG_ERROR, "Parse input preproc error.\n");
-            return AVERROR(EIO);
+static int load_balance(AVFilterContext *ctx)
+{
+    AVFilterLink *inlink = ctx->inputs[0];
+    AVFilterLink *outlink = ctx->outputs[0];
+    IEDetectContext *s = ctx->priv;
+    AVFrame *in = NULL, *output = NULL;
+    int64_t pts;
+    int ret, status;
+    int resource, got_frames = 0;
+    int get_frame_status;
+
+    FF_FILTER_FORWARD_STATUS_BACK(outlink, inlink);
+
+    // drain all processed frames
+    do {
+        get_frame_status = av_base_inference_get_frame(ctx, s->base, &output);
+        if (output) {
+            int ret_val = ff_filter_frame(outlink, output);
+            if (ret_val < 0)
+                return ret_val;
+
+            got_frames = 1;
+            output = NULL;
         }
-
-        if (ff_parse_output_postproc(proc, &s->model_postproc) < 0) {
-            av_log(ctx, AV_LOG_ERROR, "Parse output postproc error.\n");
-            return AVERROR(EIO);
+    } while (get_frame_status == 0);
+
+    status = ff_outlink_get_status(inlink);
+    if (status)
+        resource = ff_inlink_queued_frames(inlink);
+    else
+        resource = av_base_inference_resource_status(ctx, s->base);
+
+    while (resource > 0) {
+        ret = ff_inlink_consume_frame(inlink, &in);
+        if (ret < 0)
+            return ret;
+        if (ret == 0)
+            break;
+        if (ret > 0) {
+            av_base_inference_send_frame(ctx, s->base, in);
         }
-
-        s->proc_config = proc;
+        resource--;
     }
 
-    p.model_file      = s->model_file;
-    p.backend_type    = s->backend_type;
-    p.device_type     = s->device_type;
-    p.batch_size      = s->batch_size;
-    p.input_precision = DNN_DATA_PRECISION_U8;
-    p.input_layout    = DNN_DATA_LAYOUT_NCHW;
-    p.input_is_image  = 1;
-    p.preprocess      = &detect_preprocess;
-
-    ret = ff_inference_base_create(ctx, &s->base, &p);
-    if (ret < 0) {
-        av_log(ctx, AV_LOG_ERROR, "Could not create inference\n");
-        return ret;
+    if (!status && got_frames)
+        return 0;
+
+    if (ff_inlink_acknowledge_status(inlink, &status, &pts)) {
+        if (status == AVERROR_EOF) {
+            int64_t out_pts = pts;
+
+            av_log(ctx, AV_LOG_INFO, "Get EOS.\n");
+            ret = flush_frame(ctx, outlink, pts, &out_pts);
+            ff_outlink_set_status(outlink, status, out_pts);
+            return ret;
+        }
     }
 
-    return 0;
+    FF_FILTER_FORWARD_WANTED(outlink, inlink);
+
+    return FFERROR_NOT_READY;
 }
 
-static av_cold void detect_uninit(AVFilterContext *ctx)
+static int activate(AVFilterContext *ctx)
 {
-    InferenceDetectContext *s = ctx->priv;
+    AVFilterLink *inlink = ctx->inputs[0];
+    AVFilterLink *outlink = ctx->outputs[0];
+    IEDetectContext *s = ctx->priv;
+    AVFrame *in = NULL, *output = NULL;
+    int64_t pts;
+    int ret, status;
+    int got_frame = 0;
+
+    if (av_load_balance_get())
+        return load_balance(ctx);
+
+    FF_FILTER_FORWARD_STATUS_BACK(outlink, inlink);
+
+    do {
+        int get_frame_status;
+        // drain all input frames
+        ret = ff_inlink_consume_frame(inlink, &in);
+        if (ret < 0)
+            return ret;
+        if (ret > 0)
+            av_base_inference_send_frame(ctx, s->base, in);
+
+        // drain all processed frames
+        do {
+            get_frame_status = av_base_inference_get_frame(ctx, s->base, &output);
+            if (output) {
+                int ret_val = ff_filter_frame(outlink, output);
+                if (ret_val < 0)
+                    return ret_val;
+
+                got_frame = 1;
+                output = NULL;
+            }
+        } while (get_frame_status == 0);
+    } while (ret > 0);
 
-    ff_inference_base_free(&s->base);
+    // if frame got, schedule to next filter
+    if (got_frame)
+        return 0;
 
-    ff_release_model_proc(s->proc_config, &s->model_preproc, &s->model_postproc);
-}
+    if (ff_inlink_acknowledge_status(inlink, &status, &pts)) {
+        if (status == AVERROR_EOF) {
+            int64_t out_pts = pts;
 
-static int filter_frame(AVFilterLink *inlink, AVFrame *in)
-{
-    int ret;
-    AVFilterContext *ctx      = inlink->dst;
-    InferenceDetectContext *s = ctx->priv;
-    AVFilterLink *outlink     = inlink->dst->outputs[0];
-    InferTensorMeta tensor_meta = { };
-
-    if (s->frame_number % s->every_nth_frame != 0)
-        goto done;
-
-    ret = ff_inference_base_filter_frame(s->base, in);
-    if (ret < 0)
-        goto fail;
-
-    ret = ff_inference_base_get_infer_result(s->base, 0, &tensor_meta);
-    if (ret < 0)
-        goto fail;
-
-    detect_postprocess(ctx, &tensor_meta, in);
-
-done:
-    s->frame_number++;
-    return ff_filter_frame(outlink, in);
-fail:
-    av_frame_free(&in);
-    return AVERROR(EIO);
+            av_log(ctx, AV_LOG_INFO, "Get EOS.\n");
+            ret = flush_frame(ctx, outlink, pts, &out_pts);
+            ff_outlink_set_status(outlink, status, out_pts);
+            return ret;
+        }
+    }
+
+    FF_FILTER_FORWARD_WANTED(outlink, inlink);
+
+    return FFERROR_NOT_READY;
 }
 
 static const AVOption inference_detect_options[] = {
-    { "dnn_backend", "DNN backend for model execution", OFFSET(backend_type),    AV_OPT_TYPE_FLAGS,  { .i64 = DNN_INTEL_IE },          0, 2,  FLAGS, "engine" },
-    { "model",       "path to model file for network",  OFFSET(model_file),      AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,  FLAGS },
-    { "model_proc",  "model preproc and postproc",      OFFSET(model_proc),      AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,  FLAGS },
-    { "device",      "running on device type",          OFFSET(device_type),     AV_OPT_TYPE_FLAGS,  { .i64 = DNN_TARGET_DEVICE_CPU }, 0, 12, FLAGS },
-    { "vpp_format",  "specify vpp output format",       OFFSET(vpp_format),      AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,  FLAGS },
-    { "interval",    "detect every Nth frame",          OFFSET(every_nth_frame), AV_OPT_TYPE_INT,    { .i64 = 1 },   1, 1024,    FLAGS},
-    { "batch_size",  "batch size per infer",            OFFSET(batch_size),      AV_OPT_TYPE_INT,    { .i64 = 1 },   1, 1024,    FLAGS},
-    { "max_count",   "max count of output result",      OFFSET(max_count),       AV_OPT_TYPE_INT,    { .i64 = 1000}, 1, INT_MAX, FLAGS},
-    { "threshold",   "threshod to filter output data",  OFFSET(threshold),       AV_OPT_TYPE_FLOAT,  { .dbl = 0.5},  0, 1,       FLAGS},
+    { "dnn_backend",  "DNN backend for model execution", OFFSET(backend_type),    AV_OPT_TYPE_FLAGS,  { .i64 = 1},          0, 2,  FLAGS },
+    { "model",        "path to model file for network",  OFFSET(model),           AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "model_proc",   "model preproc and postproc",      OFFSET(model_proc),      AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "object_class", "objective class",                 OFFSET(object_class),    AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "device",       "running on device name",          OFFSET(device),          AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "configs",      "configurations to backend",       OFFSET(infer_config),    AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "interval",     "detect every Nth frame",          OFFSET(every_nth_frame), AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 1024, FLAGS},
+    { "nireq",        "inference request number",        OFFSET(nireq),           AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 128,  FLAGS},
+    { "batch_size",   "batch size per infer",            OFFSET(batch_size),      AV_OPT_TYPE_INT,    { .i64 = 1 },  1, 1000, FLAGS},
+    { "threshold",    "threshod to filter output data",  OFFSET(threshold),       AV_OPT_TYPE_FLOAT,  { .dbl = 0.5}, 0, 1,    FLAGS},
+    { "crop_params",  "cropping rectangle format x|y|w|h", OFFSET(crop_params),   AV_OPT_TYPE_STRING, { .str = NULL},       0, 0,  FLAGS },
+    { "async_preproc", "do asynchronous preproc in inference backend", OFFSET(async_preproc), AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, FLAGS },
 
     { NULL }
 };
@@ -447,7 +325,6 @@ static const AVFilterPad detect_inputs[] = {
         .name          = "default",
         .type          = AVMEDIA_TYPE_VIDEO,
         .config_props  = config_input,
-        .filter_frame  = filter_frame,
     },
     { NULL }
 };
@@ -456,16 +333,16 @@ static const AVFilterPad detect_outputs[] = {
     {
         .name          = "default",
         .type          = AVMEDIA_TYPE_VIDEO,
-        .config_props  = config_output,
     },
     { NULL }
 };
 
 AVFilter ff_vf_inference_detect = {
     .name          = "detect",
-    .description   = NULL_IF_CONFIG_SMALL("DNN Inference detection."),
-    .priv_size     = sizeof(InferenceDetectContext),
+    .description   = NULL_IF_CONFIG_SMALL("Image Inference Detect Filter."),
+    .priv_size     = sizeof(IEDetectContext),
     .query_formats = query_formats,
+    .activate      = activate,
     .init          = detect_init,
     .uninit        = detect_uninit,
     .inputs        = detect_inputs,
diff --git a/libavfilter/vf_inference_identify.c b/libavfilter/vf_inference_identify.c
index 6f9aaaa..7fbb5e6 100644
--- a/libavfilter/vf_inference_identify.c
+++ b/libavfilter/vf_inference_identify.c
@@ -32,13 +32,16 @@
 #include "internal.h"
 #include "avfilter.h"
 
-#include "inference.h"
+#include "inference_backend/ff_base_inference.h"
+#include "inference_backend/model_proc.h"
 
 #include <json-c/json.h>
 
 #define OFFSET(x) offsetof(InferenceIdentifyContext, x)
 #define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
 
+#define UNUSED(x) (void)(x)
+
 #define PI 3.1415926
 #define FACE_FEATURE_VECTOR_LEN 256
 
@@ -68,24 +71,33 @@ static const char *get_filename_ext(const char *filename) {
 
 const char *gallery_file_suffix = "json";
 
-static void infer_labels_buffer_free(void *opaque, uint8_t *data)
-{
-    int i;
-    LabelsArray *labels = (LabelsArray *)data;
+static double av_norm(float vec[], size_t num) {
+    size_t i;
+    double result = 0.0;
+
+    for (i = 0; i < num; i++)
+        result += vec[i] * vec[i];
+
+    return sqrt(result);
+}
+
+static double av_dot(float vec1[], float vec2[], size_t num) {
+    size_t i;
+    double result = 0.0;
 
-    for (i = 0; i < labels->num; i++)
-        av_freep(&labels->label[i]);
+    for (i = 0; i < num; i++)
+        result += vec1[i] * vec2[i];
 
-    av_free(data);
+    return result;
 }
 
-static int query_formats(AVFilterContext *context)
-{
+static int query_formats(AVFilterContext *context) {
     AVFilterFormats *formats_list;
     const enum AVPixelFormat pixel_formats[] = {
         AV_PIX_FMT_YUV420P,  AV_PIX_FMT_YUV422P,  AV_PIX_FMT_YUV444P,
         AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_YUVJ422P, AV_PIX_FMT_YUVJ444P,
         AV_PIX_FMT_YUV410P,  AV_PIX_FMT_YUV411P,  AV_PIX_FMT_GRAY8,
+        AV_PIX_FMT_RGBP,
         AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
         AV_PIX_FMT_NONE};
 
@@ -98,11 +110,10 @@ static int query_formats(AVFilterContext *context)
     return ff_set_common_formats(context, formats_list);
 }
 
-static av_cold int identify_init(AVFilterContext *ctx)
-{
+static av_cold int identify_init(AVFilterContext *ctx) {
     size_t i, index = 1;
     char *dup, *unknown;
-    const char *dirname;
+    const char *dirname, *suffix;
     json_object *entry;
     LabelsArray *larray = NULL;
     AVBufferRef *ref    = NULL;
@@ -112,12 +123,18 @@ static av_cold int identify_init(AVFilterContext *ctx)
 
     av_assert0(s->gallery);
 
-    if (strcmp(get_filename_ext(s->gallery), gallery_file_suffix)) {
+    suffix = get_filename_ext(s->gallery);
+    if (!suffix) {
+        av_log(ctx, AV_LOG_ERROR, "Unrecognized gallery file '%s' \n", s->gallery);
+        return AVERROR(EINVAL);
+    }
+
+    if (0 != strcmp(suffix, gallery_file_suffix)) {
         av_log(ctx, AV_LOG_ERROR, "Face gallery '%s' is not a json file\n", s->gallery);
         return AVERROR(EINVAL);
     }
 
-    entry = ff_read_model_proc(s->gallery);
+    entry = model_proc_read_config_file(s->gallery);
     if (!entry) {
         av_log(ctx, AV_LOG_ERROR, "Could not open gallery file:%s\n", s->gallery);
         return AVERROR(EIO);
@@ -144,18 +161,17 @@ static av_cold int identify_init(AVFilterContext *ctx)
         if (ret) {
             size_t features_num = json_object_array_length(features);
 
-            for(int i = 0; i < features_num; i++){
+            for(int i = 0; i < features_num; i++) {
                 FILE *vec_fp;
                 FeatureLabelPair *pair;
                 char path[4096];
 
-                memset(path, 0, sizeof(path));
-
                 feature = json_object_array_get_idx(features, i);
                 if (json_object_get_string(feature) == NULL)
                     continue;
 
-                strncpy(path, dirname, strlen(dirname));
+                av_assert0((strlen(dirname) + strlen(json_object_get_string(feature)) + 1) < sizeof(path));
+                strncpy(path, dirname, sizeof(path));
                 strncat(path, "/", 1);
                 strncat(path, json_object_get_string(feature), strlen(json_object_get_string(feature)));
 
@@ -168,19 +184,22 @@ static av_cold int identify_init(AVFilterContext *ctx)
                 pair = av_mallocz(sizeof(FeatureLabelPair));
                 if (!pair){
                     fclose(vec_fp);
-                    return AVERROR(ENOMEM);
+                    goto error;
                 }
 
                 pair->feature = av_malloc(vec_size_in_bytes);
                 if (!pair->feature){
                     fclose(vec_fp);
-                    return AVERROR(ENOMEM);
+                    av_free(pair);
+                    goto error;
                 }
 
                 if (fread(pair->feature, vec_size_in_bytes, 1, vec_fp) != 1) {
                     av_log(ctx, AV_LOG_ERROR, "Feature vector size mismatch:%s\n", path);
                     fclose(vec_fp);
-                    return AVERROR(EINVAL);
+                    av_free(pair->feature);
+                    av_free(pair);
+                    goto error;
                 }
 
                 fclose(vec_fp);
@@ -194,7 +213,7 @@ static av_cold int identify_init(AVFilterContext *ctx)
 
     s->norm_std = av_mallocz(sizeof(double) * s->features_num);
     if (!s->norm_std)
-        return AVERROR(ENOMEM);
+        goto error;
 
     for (i = 0; i < s->features_num; i++)
         s->norm_std[i] = av_norm(s->features[i]->feature, FACE_FEATURE_VECTOR_LEN);
@@ -204,12 +223,16 @@ static av_cold int identify_init(AVFilterContext *ctx)
 
     s->labels = ref;
     av_free(dup);
+    json_object_put(entry);
 
     return 0;
+error:
+    if (larray)
+        av_free(larray);
+    return AVERROR(ENOMEM);
 }
 
-static av_cold void identify_uninit(AVFilterContext *ctx)
-{
+static av_cold void identify_uninit(AVFilterContext *ctx) {
     int i;
     InferenceIdentifyContext *s = ctx->priv;
 
@@ -227,16 +250,14 @@ static av_cold void identify_uninit(AVFilterContext *ctx)
 }
 
 static av_cold void dump_face_id(AVFilterContext *ctx, int label_id,
-                                 float conf, AVBufferRef *label_buf)
-{
+                                 float conf, AVBufferRef *label_buf) {
     LabelsArray *array = (LabelsArray *)label_buf->data;
 
     av_log(ctx, AV_LOG_DEBUG,"CLASSIFY META - Face_id:%d Name:%s Conf:%1.2f\n",
            label_id, array->label[label_id], conf);
 }
 
-static int face_identify(AVFilterContext *ctx, AVFrame *frame)
-{
+static int face_identify(AVFilterContext *ctx, AVFrame *frame) {
     int i;
     InferenceIdentifyContext *s = ctx->priv;
     AVFrameSideData *side_data;
@@ -289,8 +310,7 @@ static int face_identify(AVFilterContext *ctx, AVFrame *frame)
     return 0;
 }
 
-static int filter_frame(AVFilterLink *inlink, AVFrame *in)
-{
+static int filter_frame(AVFilterLink *inlink, AVFrame *in) {
     AVFilterContext *ctx  = inlink->dst;
     AVFilterLink *outlink = inlink->dst->outputs[0];
 
diff --git a/libavfilter/vf_inference_metaconvert.c b/libavfilter/vf_inference_metaconvert.c
index 89178a8..ca74d08 100644
--- a/libavfilter/vf_inference_metaconvert.c
+++ b/libavfilter/vf_inference_metaconvert.c
@@ -23,10 +23,7 @@
 
 #include "libavutil/opt.h"
 #include "libavutil/mem.h"
-#include "libavutil/eval.h"
 #include "libavutil/avassert.h"
-#include "libavutil/pixdesc.h"
-#include "libavutil/mathematics.h"
 
 #include "formats.h"
 #include "internal.h"
@@ -34,27 +31,10 @@
 #include "libavcodec/avcodec.h"
 #include "libavformat/avformat.h"
 
-#include "inference.h"
-#include "dnn_interface.h"
+#include "inference_backend/ff_base_inference.h"
+#include "inference_backend/metaconverter.h"
 
-#define OFFSET(x) offsetof(MetaConvertContext, x)
-#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
-
-typedef struct MetaConvertContext {
-    const AVClass *class;
-
-    char *model;
-    char *converter;
-    char *method;
-    char *location;
-    char *layer;
-
-    void (*convert_func)(AVFilterContext *ctx, AVFrame *frame);
-
-} MetaConvertContext;
-
-static int query_formats(AVFilterContext *ctx)
-{
+static int query_formats(AVFilterContext *ctx) {
     AVFilterFormats *formats_list;
     const enum AVPixelFormat pixel_formats[] = {
         AV_PIX_FMT_YUV420P,  AV_PIX_FMT_YUV422P,  AV_PIX_FMT_YUV444P,
@@ -72,85 +52,85 @@ static int query_formats(AVFilterContext *ctx)
     return ff_set_common_formats(ctx, formats_list);
 }
 
-static av_cold void tensors_to_file(AVFilterContext *ctx, AVFrame *frame)
-{
-    AVFrameSideData *sd;
-    MetaConvertContext *s = ctx->priv;
-    InferClassificationMeta *c_meta;
-
-    static uint32_t frame_num = 0;
-
-    if (!(sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION)))
-        return;
-
-    c_meta = (InferClassificationMeta *)sd->data;
-
-    if (c_meta) {
-        int i;
-        uint32_t index = 0;
-        char filename[1024] = {0};
-        const int meta_num = c_meta->c_array->num;
-        for (i = 0; i < meta_num; i++) {
-            FILE *f = NULL;
-            InferClassification *c = c_meta->c_array->classifications[i];
-            //TODO:check model and layer
-            if (!c->tensor_buf || !c->tensor_buf->data)
-                continue;
-
-            snprintf(filename, sizeof(filename), "%s/%s_frame_%u_idx_%u.tensor", s->location,
-                    s->method, frame_num, index);
-            f = fopen(filename, "wb");
-            if (!f) {
-                av_log(ctx, AV_LOG_WARNING, "Failed to open/create file: %s\n", filename);
-            } else {
-                fwrite(c->tensor_buf->data, sizeof(float), c->tensor_buf->size / sizeof(float), f);
-                fclose(f);
-            }
-            index++;
-        }
-    }
-
-    frame_num++;
-}
-
-static av_cold int metaconvert_init(AVFilterContext *ctx)
-{
+static av_cold int metaconvert_init(AVFilterContext *ctx) {
     MetaConvertContext *s = ctx->priv;
+    char filename[1024] = {0};
 
-    if (!s->model || !s->converter || !s->method) {
+    if (!s->converter || !s->method || !s->location) {
         av_log(ctx, AV_LOG_ERROR, "Missing key parameters!!\n");
         return AVERROR(EINVAL);
     }
 
-    av_log(ctx, AV_LOG_INFO, "\nmodel:%s\nconverter:%s\nmethod:%s\nlocation:%s\n",
-           s->model, s->converter, s->method, s->location);
+    snprintf(filename, sizeof(filename), "%s/%s.json", s->location, s->method);
+    s->f = fopen(filename, "wb");
+    if (!s->f) {
+        av_log(ctx, AV_LOG_ERROR, "Failed to open/create file: %s\n", filename);
+        return AVERROR(EINVAL);
+    }
 
-    if (!strcmp(s->converter, "tensors-to-file")) {
-        if (!s->location) {
-            av_log(ctx, AV_LOG_ERROR, "Missing parameters location!");
-            return AVERROR(EINVAL);
-        }
+    av_log(ctx, AV_LOG_INFO, "\nconverter:%s\nmethod:%s\nlocation:%s\n", s->converter, s->method, s->location);
+
+    if (!strcmp(s->converter, "classification-to-json")) {
+        s->convert_func = &classification_to_json;
+    } else if (!strcmp(s->converter, "detection-to-json")) {
+        s->convert_func = &detection_to_json;
+    } else if (!strcmp(s->converter, "all-to-json")) {
+        s->convert_func = &all_to_json;
+    } else if (!strcmp(s->converter, "tensors-to-file")) {
         s->convert_func = &tensors_to_file;
+    } else {
+        av_log(ctx, AV_LOG_ERROR, "Please check your converter!");
+        return AVERROR(ERANGE);
     }
 
     return 0;
 }
 
-static int filter_frame(AVFilterLink *inlink, AVFrame *in)
-{
+static av_cold void metaconvert_uninit(AVFilterContext *ctx) {
+    MetaConvertContext *s = ctx->priv;
+    const char *tail = "\n}";
+
+    fwrite(tail, sizeof(char), strlen(tail) / sizeof(char), s->f);
+    fclose(s->f);
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in) {
     AVFilterContext *ctx  = inlink->dst;
     MetaConvertContext *s = ctx->priv;
     AVFilterLink *outlink = inlink->dst->outputs[0];
+    const char *proc_json = NULL;
+    const char *str_insert = ",\n\"Metadata\":";
+    const char *head = "{\n\"Metadata\":";
+    json_object *info_object = json_object_new_object();
+    int ret;
+
+    ret = s->convert_func(ctx, in, info_object);
+    if (ret) {
+        int64_t nano_ts = 1000000000;
+        char timestamp[1024] = {0};
+
+        nano_ts = in->pts * (nano_ts * inlink->time_base.num / inlink->time_base.den);
+        snprintf(timestamp, sizeof(timestamp), "%"PRIu64"", nano_ts);
+
+        json_object_object_add(info_object, "timestamp", json_object_new_string(timestamp));
+
+        if(s->frame_number == 0) {
+            fwrite(head, sizeof(char), strlen(head) / sizeof(char), s->f);
+        } else {
+            fwrite(str_insert, sizeof(char), strlen(str_insert) / sizeof(char), s->f);
+        }
 
-    if (s->convert_func)
-        s->convert_func(ctx, in);
+        proc_json = json_object_to_json_string_ext(info_object, JSON_C_TO_STRING_PRETTY);
+        fwrite(proc_json, sizeof(char), strlen(proc_json) / sizeof(char), s->f);
+
+        s->frame_number++;
+    }
+    json_object_put(info_object);
 
     return ff_filter_frame(outlink, in);
 }
 
 static const AVOption inference_metaconvert_options[] = {
-    { "model",     "select tensor by model name", OFFSET(model),     AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
-    { "layer",     "select tensor by layer name", OFFSET(layer),     AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
     { "converter", "metadata conversion group",   OFFSET(converter), AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
     { "method",    "metadata conversion method",  OFFSET(method),    AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
     { "location",  "location for output files",   OFFSET(location),  AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
@@ -183,6 +163,7 @@ AVFilter ff_vf_inference_metaconvert = {
     .priv_size     = sizeof(MetaConvertContext),
     .query_formats = query_formats,
     .init          = metaconvert_init,
+    .uninit        = metaconvert_uninit,
     .inputs        = metaconvert_inputs,
     .outputs       = metaconvert_outputs,
     .priv_class    = &inference_metaconvert_class,
diff --git a/libavfilter/vf_ocv_overlay.c b/libavfilter/vf_ocv_overlay.c
new file mode 100644
index 0000000..668ce65
--- /dev/null
+++ b/libavfilter/vf_ocv_overlay.c
@@ -0,0 +1,215 @@
+/*
+ * Copyright (c) 2018 Pengfei Qu
+ * Copyright (c) 2019 Lin Xie
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * libopencv wrapper functions to overlay
+ */
+
+#include "config.h"
+#if HAVE_OPENCV2_CORE_CORE_C_H
+#include <opencv2/core/core_c.h>
+#include <opencv2/imgproc/imgproc_c.h>
+#else
+#include <opencv/cv.h>
+#include <opencv/cxcore.h>
+#endif
+#include "libavutil/avstring.h"
+#include "libavutil/common.h"
+#include "libavutil/file.h"
+#include "libavutil/opt.h"
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+#include "inference_backend/ff_base_inference.h"
+
+static void fill_iplimage_from_frame(IplImage *img, const AVFrame *frame, enum AVPixelFormat pixfmt)
+{
+    IplImage *tmpimg;
+    int depth, channels_nb;
+
+    if      (pixfmt == AV_PIX_FMT_GRAY8) { depth = IPL_DEPTH_8U;  channels_nb = 1; }
+    else if (pixfmt == AV_PIX_FMT_BGRA)  { depth = IPL_DEPTH_8U;  channels_nb = 4; }
+    else if (pixfmt == AV_PIX_FMT_BGR24) { depth = IPL_DEPTH_8U;  channels_nb = 3; }
+    else return;
+
+    tmpimg = cvCreateImageHeader((CvSize){frame->width, frame->height}, depth, channels_nb);
+    *img = *tmpimg;
+    img->imageData = img->imageDataOrigin = frame->data[0];
+    img->dataOrder = IPL_DATA_ORDER_PIXEL;
+    img->origin    = IPL_ORIGIN_TL;
+    img->widthStep = frame->linesize[0];
+}
+
+static void fill_frame_from_iplimage(AVFrame *frame, const IplImage *img, enum AVPixelFormat pixfmt)
+{
+    frame->linesize[0] = img->widthStep;
+    frame->data[0]     = img->imageData;
+}
+
+static int query_formats(AVFilterContext *ctx)
+{
+    const enum AVPixelFormat pix_fmts[] = {
+        AV_PIX_FMT_BGR24, AV_PIX_FMT_BGRA, AV_PIX_FMT_GRAY8, AV_PIX_FMT_NONE
+    };
+    AVFilterFormats *fmts_list = ff_make_format_list(pix_fmts);
+    if (!fmts_list)
+        return AVERROR(ENOMEM);
+    return ff_set_common_formats(ctx, fmts_list);
+}
+
+typedef struct OCVOverlayContext {
+    const AVClass *class;
+
+    char *color_line_str;
+    char *color_text_str;
+    int x, y, width, height;
+
+    CvPoint lt,rd;
+    CvScalar color_line;
+    CvScalar color_text;
+    CvFont font;
+    int thickness;
+    int line_type;
+    int shift;
+} OCVOverlayContext;
+
+static av_cold int init(AVFilterContext *ctx)
+{
+    OCVOverlayContext *s = ctx->priv;
+
+    if (strcmp(s->color_line_str, "red") == 0)
+        s->color_line = cvScalar(255, 0, 0, 255);
+    else if (strcmp(s->color_line_str, "green") == 0)
+        s->color_line = cvScalar(0, 255, 0, 255);
+    else if (strcmp(s->color_line_str, "blue") == 0)
+        s->color_line = cvScalar(0, 0, 255, 255);
+    else
+        s->color_line = cvScalar(255, 0, 0, 255);
+
+    if (strcmp(s->color_text_str, "red") == 0)
+        s->color_text = cvScalar(255, 0, 0, 255);
+    else if (strcmp(s->color_text_str, "green") == 0)
+        s->color_text = cvScalar(0, 255, 0, 255);
+    else if (strcmp(s->color_text_str, "blue") == 0)
+        s->color_text = cvScalar(0, 0, 255, 255);
+    else
+        s->color_text = cvScalar(0, 0, 255, 255);
+
+    s->line_type = 8;
+    s->shift     = 0;
+    cvInitFont(&s->font, CV_FONT_HERSHEY_SIMPLEX, 0.5, 0.5, 0, 1, s->line_type);
+
+    return 0;
+}
+
+static void rectangle(AVFilterContext *ctx, IplImage *img, CvPoint pt1, CvPoint pt2)
+{
+    OCVOverlayContext *s = ctx->priv;
+    cvRectangle(img, pt1, pt2, s->color_line, s->thickness, s->line_type, s->shift);
+}
+
+static void put_text(AVFilterContext *ctx, IplImage *img, CvPoint pt1, const char * text)
+{
+    OCVOverlayContext *s = ctx->priv;
+    cvPutText(img, text, pt1, &s->font, s->color_text);
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in)
+{
+    AVFilterContext *ctx  = inlink->dst;
+    AVFilterLink *outlink = inlink->dst->outputs[0];
+    AVFrameSideData *sd;
+    IplImage img;
+    CvPoint pt1, pt2;
+
+    if (in->nb_side_data == 0)
+        return ff_filter_frame(outlink, in);
+
+    fill_iplimage_from_frame(&img, in , inlink->format);
+
+    sd = av_frame_get_side_data(in, AV_FRAME_DATA_INFERENCE_DETECTION);
+    if (sd) {
+        InferDetectionMeta *meta = (InferDetectionMeta *)sd->data;
+
+        BBoxesArray *boxes = meta->bboxes;
+        if (boxes && boxes->num > 0) {
+            int i;
+            for (i = 0; i < boxes->num; i++) {
+                InferDetection *box = boxes->bbox[i];
+
+                pt1.x = box->x_min;
+                pt1.y = box->y_min;
+                pt2.x = box->x_max;
+                pt2.y = box->y_max;
+
+                rectangle(ctx, &img, pt1, pt2);
+
+                // TODO: label
+            }
+        }
+    }
+
+    fill_frame_from_iplimage(in, &img, inlink->format);
+
+    return ff_filter_frame(outlink, in);
+}
+
+#define OFFSET(x) offsetof(OCVOverlayContext, x)
+#define FLAGS AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM
+static const AVOption ocv_overlay_options[] = {
+    { "color_line",     "set color of the line(red/gree/blue)",  OFFSET(color_line_str), AV_OPT_TYPE_STRING,{ .str = "red" }, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "color_text",     "set color of the line(red/gree/blue)",  OFFSET(color_text_str), AV_OPT_TYPE_STRING,{ .str = "green" }, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "thickness", "set the box thickness value", OFFSET(thickness), AV_OPT_TYPE_INT,{ .i64 = 1 }, 1, 5 , FLAGS },
+    { "line_type", "set the line_type value", OFFSET(line_type), AV_OPT_TYPE_INT,{ .i64 = 4 }, 1, 5 , FLAGS },
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(ocv_overlay);
+
+static const AVFilterPad avfilter_vf_ocv_overlay_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = filter_frame,
+    },
+    { NULL }
+};
+
+static const AVFilterPad avfilter_vf_ocv_overlay_outputs[] = {
+    {
+        .name = "default",
+        .type = AVMEDIA_TYPE_VIDEO,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_ocv_overlay = {
+    .name          = "ocv_overlay",
+    .description   = NULL_IF_CONFIG_SMALL("Draw rectangle and text using libopencv."),
+    .priv_size     = sizeof(OCVOverlayContext),
+    .priv_class    = &ocv_overlay_class,
+    .query_formats = query_formats,
+    .init          = init,
+    .inputs        = avfilter_vf_ocv_overlay_inputs,
+    .outputs       = avfilter_vf_ocv_overlay_outputs,
+};
diff --git a/libavformat/iemetadataenc.c b/libavformat/iemetadataenc.c
index c382964..bcb2fdf 100644
--- a/libavformat/iemetadataenc.c
+++ b/libavformat/iemetadataenc.c
@@ -23,7 +23,7 @@
 #include "internal.h"
 #include "libavutil/dict.h"
 #include "libavfilter/avfilter.h"
-#include "libavfilter/inference.h"
+#include "libavfilter/inference_backend/ff_base_inference.h"
 #include "libavutil/opt.h"
 #include <float.h>
 
diff --git a/libavutil/buffer.c b/libavutil/buffer.c
index 8d1aa5f..fdb0b9d 100644
--- a/libavutil/buffer.c
+++ b/libavutil/buffer.c
@@ -331,6 +331,17 @@ static AVBufferRef *pool_alloc_buffer(AVBufferPool *pool)
     return ret;
 }
 
+int av_buffer_pool_is_empty(AVBufferPool *pool)
+{
+    int empty = 0;
+
+    ff_mutex_lock(&pool->mutex);
+    empty = pool->pool ? 0 : 1;
+    ff_mutex_unlock(&pool->mutex);
+
+    return empty;
+}
+
 AVBufferRef *av_buffer_pool_get(AVBufferPool *pool)
 {
     AVBufferRef *ret;
diff --git a/libavutil/buffer.h b/libavutil/buffer.h
index 73b6bd0..9d74cd2 100644
--- a/libavutil/buffer.h
+++ b/libavutil/buffer.h
@@ -285,6 +285,12 @@ void av_buffer_pool_uninit(AVBufferPool **pool);
 AVBufferRef *av_buffer_pool_get(AVBufferPool *pool);
 
 /**
+ * Check if the buffer pool is empty.
+ *
+ * @return 1: empty 0: not empty
+ */
+int av_buffer_pool_is_empty(AVBufferPool *pool);
+/**
  * @}
  */
 
diff --git a/libavutil/log.c b/libavutil/log.c
index 84d0282..18feee0 100644
--- a/libavutil/log.c
+++ b/libavutil/log.c
@@ -53,6 +53,7 @@ static AVMutex mutex = AV_MUTEX_INITIALIZER;
 
 static int av_log_level = AV_LOG_INFO;
 static int av_profiling = 0;
+static int av_load_balance = 0;
 static int flags;
 
 #define NB_LEVELS 8
@@ -413,6 +414,16 @@ void av_profiling_set(int arg)
     av_profiling = arg;
 }
 
+void av_load_balance_set(int arg)
+{
+    av_load_balance = arg;
+}
+
+int av_load_balance_get(void)
+{
+    return av_load_balance;
+}
+
 static void missing_feature_sample(int sample, void *avc, const char *msg,
                                    va_list argument_list)
 {
diff --git a/libavutil/log.h b/libavutil/log.h
index 0fb29dd..6f24ec8 100644
--- a/libavutil/log.h
+++ b/libavutil/log.h
@@ -300,6 +300,9 @@ void av_log_default_callback(void *avcl, int level, const char *fmt,
 int av_profiling_get(void);
 void av_profiling_set(int arg);
 
+int av_load_balance_get(void);
+void av_load_balance_set(int arg);
+
 /**
  * Return the context name
  *
diff --git a/libswscale/utils.c b/libswscale/utils.c
index d5913ed..304ab80 100644
--- a/libswscale/utils.c
+++ b/libswscale/utils.c
@@ -221,6 +221,7 @@ static const FormatEntry format_entries[AV_PIX_FMT_NB] = {
     [AV_PIX_FMT_YUV444P12LE] = { 1, 1 },
     [AV_PIX_FMT_YUV444P14BE] = { 1, 1 },
     [AV_PIX_FMT_YUV444P14LE] = { 1, 1 },
+    [AV_PIX_FMT_RGBP]        = { 1, 1 },
     [AV_PIX_FMT_GBRP]        = { 1, 1 },
     [AV_PIX_FMT_GBRP9LE]     = { 1, 1 },
     [AV_PIX_FMT_GBRP9BE]     = { 1, 1 },
-- 
2.7.4

